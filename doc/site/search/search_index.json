{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 Welcome to the Pachyderm documentation portal! Here you can find information for beginners and experienced Pachyderm users, as well as the Pachyderm API reference docs. If you cannot find what you are looking for or have an issue that is not mentioned here, we'd love to hear from you either on GitHub , in our Users Slack channel , or by email at support@pachyderm.io . Getting Started Get started by deploying Pachyderm locally and completing our beginner's tutorial. Install Pachyderm on Your Computer Beginner Tutorial Deploy and Manage Deploy Pachyderm on a cloud platform of your choice and manage its lifecycle. Deploy Pachyderm Manage Operations Concepts Learn the basics. Versioned Data Concepts Pipeline Concepts Examples Try Pachyderm examples. Distributed Hyperparameter Tuning Variant Discovery with GATK More Examples Data Operations Learn the most common data operations in Pachyderm. Load Your Data Into Pachyderm Export Your Data From Pachyderm Individual Developer Workflow Advanced Operations Try data operations for experienced Pachyderm users. Splitting Data for Distributed Processing Process Time-Windowed Data Deferred Processing of Data","title":"Welcome"},{"location":"#welcome","text":"Welcome to the Pachyderm documentation portal! Here you can find information for beginners and experienced Pachyderm users, as well as the Pachyderm API reference docs. If you cannot find what you are looking for or have an issue that is not mentioned here, we'd love to hear from you either on GitHub , in our Users Slack channel , or by email at support@pachyderm.io .","title":"Welcome"},{"location":"concepts/data-concepts/","text":"Pachyderm Versioned Data Concepts \u00b6 Pachyderm data concepts describe version-control primitives that you interact with when you use Pachyderm. These ideas are conceptually similar to the Git version-control system with a few notable exceptions. Because Pachyderm deals not only with plain text but also with binary files and large datasets, it does not process the data in the same way as Git. When you use Git, you store a copy of the repository on your local machine. You work with that copy, apply your changes, and then send the changes to the upstream master copy of the repository where it gets merged. The Pachyderm version control works slightly differently. In Pachyderm, only a centralized repository exists and you do not store any local copies of that repository. Therefore, the merge, in the traditional Git meaning, does not occur. Instead, your data can be continuously updated in the master branch of your repo, while you can experiment with specific data commits in a separate branch or branches. Because of this behavior, you cannot run into a merge conflict with Pachyderm. The Pachyderm data versioning system has the following main concepts: Repository A Pachyderm repository is the highest level data object. Typically, each dataset in Pachyderm is its own repository. Commit A commit is an immutable snapshot of a repo at a particular point in time. Branch A branch is an alias to a specific commit, or a pointer, that automatically moves as new data is submitted. File Files and directories are actual data in your repository. Pachyderm supports any type, size, and number of files. Provenance Provenance expresses the relationship between various commits, branches, and repositories. It helps you to track the origin of each commit.","title":"Overview"},{"location":"concepts/data-concepts/#pachyderm-versioned-data-concepts","text":"Pachyderm data concepts describe version-control primitives that you interact with when you use Pachyderm. These ideas are conceptually similar to the Git version-control system with a few notable exceptions. Because Pachyderm deals not only with plain text but also with binary files and large datasets, it does not process the data in the same way as Git. When you use Git, you store a copy of the repository on your local machine. You work with that copy, apply your changes, and then send the changes to the upstream master copy of the repository where it gets merged. The Pachyderm version control works slightly differently. In Pachyderm, only a centralized repository exists and you do not store any local copies of that repository. Therefore, the merge, in the traditional Git meaning, does not occur. Instead, your data can be continuously updated in the master branch of your repo, while you can experiment with specific data commits in a separate branch or branches. Because of this behavior, you cannot run into a merge conflict with Pachyderm. The Pachyderm data versioning system has the following main concepts: Repository A Pachyderm repository is the highest level data object. Typically, each dataset in Pachyderm is its own repository. Commit A commit is an immutable snapshot of a repo at a particular point in time. Branch A branch is an alias to a specific commit, or a pointer, that automatically moves as new data is submitted. File Files and directories are actual data in your repository. Pachyderm supports any type, size, and number of files. Provenance Provenance expresses the relationship between various commits, branches, and repositories. It helps you to track the origin of each commit.","title":"Pachyderm Versioned Data Concepts"},{"location":"concepts/data-concepts/branch/","text":"Branch \u00b6 A Pachyderm branch is a pointer, or an alias, to a commit that moves along with new commits as they are submitted. By default, when you create a repository, Pachyderm does not create any branches. Most users prefer to create a master branch by initiating the first commit and specifying the master branch in the put file command. Also, you can create additional branches to experiment with the data. Branches enable collaboration between teams of data scientists. However, many users find it sufficient to use the master branch for all their work. Although the concept of branch is similar to Git branches, in most cases brances are not used as extensively as in source code version-control systems. Each branch has a HEAD which references the latest commit in the branch. Pachyderm pipelines look at the HEAD of the branch for changes and, if they detect new changes, trigger a job. When you commit a new change, the HEAD of the branch moves to the latest commit. repository. By default, Pachyderm pipelines look at the HEAD of the branch for changes and, if they detect new changes, launch. When you commit a new change, the HEAD of the branch moves to the latest commit. To view a list of branches in a repo, run the pachctl list branch command. Example $ pachctl list branch images BRANCH HEAD master bb41c5fb83a14b69966a21c78a3c3b24","title":"Branch"},{"location":"concepts/data-concepts/branch/#branch","text":"A Pachyderm branch is a pointer, or an alias, to a commit that moves along with new commits as they are submitted. By default, when you create a repository, Pachyderm does not create any branches. Most users prefer to create a master branch by initiating the first commit and specifying the master branch in the put file command. Also, you can create additional branches to experiment with the data. Branches enable collaboration between teams of data scientists. However, many users find it sufficient to use the master branch for all their work. Although the concept of branch is similar to Git branches, in most cases brances are not used as extensively as in source code version-control systems. Each branch has a HEAD which references the latest commit in the branch. Pachyderm pipelines look at the HEAD of the branch for changes and, if they detect new changes, trigger a job. When you commit a new change, the HEAD of the branch moves to the latest commit. repository. By default, Pachyderm pipelines look at the HEAD of the branch for changes and, if they detect new changes, launch. When you commit a new change, the HEAD of the branch moves to the latest commit. To view a list of branches in a repo, run the pachctl list branch command. Example $ pachctl list branch images BRANCH HEAD master bb41c5fb83a14b69966a21c78a3c3b24","title":"Branch"},{"location":"concepts/data-concepts/commit/","text":"Commit \u00b6 A commit is a snapshot that preserves the state of your data at a point in time. It represents a single set of changes to files or directories in your Pachyderm repository. Commit is a user-defined operation, which means that you can start a commit, make changes, and then close the commit after you are done. Each commit has a unique identifier (ID) that you can reference in the <repo>@<commitID> format. When you create a new commit, the previous commit on which the new commit is based becomes the parent of the new commit. Your pipeline history consists of those parent-child relationships between your data commits. You can obtain information about commits in a repository by running list commit <repo> or inspect commit <commitID> . In Pachyderm, commits are atomic operations that capture a state of the files and directories in a repository. Unlike Git commits, Pachyderm commits are centralized and transactional. You can start a commit by running the pachctl start commit command, make changes to the repository, and close the commit by running the pachctl finish commit command. After the commit is finished, Pachyderm saves the new state of the repository. When you start , or open , a commit, it means that you can make changes by using put file , delete file , or other commands. You can finish , or close a commit which means the commit is immutable and cannot be changed. The pachctl list commit repo@branch command. This command returns a timestamp, size, parent, and other information about the commit. The initial commit has <none> as a parent. Example $ pachctl list commit images@master REPO BRANCH COMMIT PARENT STARTED DURATION SIZE raw_data master 8248d97632874103823c7603fb8c851c 22cdb5ae05cb40868566586140ea5ed5 6 seconds ago Less than a second 5 .121MiB raw_data master 22cdb5ae05cb40868566586140ea5ed5 <none> 33 minutes ago Less than a second 2 .561MiB The list commit <repo> command displays all commits in all branches in the specified repository. The pachctl inspect commit command enables you to view detailed information about a commit, such as the size, parent, and the original branch of the commit, as well as how long ago the commit was started and finished. The --full-timestamps flag, enables you to see the exact date and time of when the commit was opened and when it was finished. If you specify a branch instead of a specific commit, Pachyderm displays the information about the HEAD of the branch. Example $ pachctl inspect commit raw_data@master --full-timestamps Commit: raw_data@8248d97632874103823c7603fb8c851c Original Branch: master Parent: 22cdb5ae05cb40868566586140ea5ed5 Started: 2019 -07-29T18:09:51.397535516Z Finished: 2019 -07-29T18:09:51.500669562Z Size: 5 .121MiB The delete commit command enables you to delete opened and closed commits, which results in permanent loss of all the data introduced in those commits. You can think about the delete commit command as an equivalent of the rm -rf command in Linux. It is an irreversible operation that should be used with caution. An alternative and a much safer way to revert incorrect data changes is to move the HEAD of the branch or create a new commit that removes the incorrect data. Example $ pachctl delete commit raw_data@8248d97632874103823c7603fb8c851c","title":"Commit"},{"location":"concepts/data-concepts/commit/#commit","text":"A commit is a snapshot that preserves the state of your data at a point in time. It represents a single set of changes to files or directories in your Pachyderm repository. Commit is a user-defined operation, which means that you can start a commit, make changes, and then close the commit after you are done. Each commit has a unique identifier (ID) that you can reference in the <repo>@<commitID> format. When you create a new commit, the previous commit on which the new commit is based becomes the parent of the new commit. Your pipeline history consists of those parent-child relationships between your data commits. You can obtain information about commits in a repository by running list commit <repo> or inspect commit <commitID> . In Pachyderm, commits are atomic operations that capture a state of the files and directories in a repository. Unlike Git commits, Pachyderm commits are centralized and transactional. You can start a commit by running the pachctl start commit command, make changes to the repository, and close the commit by running the pachctl finish commit command. After the commit is finished, Pachyderm saves the new state of the repository. When you start , or open , a commit, it means that you can make changes by using put file , delete file , or other commands. You can finish , or close a commit which means the commit is immutable and cannot be changed. The pachctl list commit repo@branch command. This command returns a timestamp, size, parent, and other information about the commit. The initial commit has <none> as a parent. Example $ pachctl list commit images@master REPO BRANCH COMMIT PARENT STARTED DURATION SIZE raw_data master 8248d97632874103823c7603fb8c851c 22cdb5ae05cb40868566586140ea5ed5 6 seconds ago Less than a second 5 .121MiB raw_data master 22cdb5ae05cb40868566586140ea5ed5 <none> 33 minutes ago Less than a second 2 .561MiB The list commit <repo> command displays all commits in all branches in the specified repository. The pachctl inspect commit command enables you to view detailed information about a commit, such as the size, parent, and the original branch of the commit, as well as how long ago the commit was started and finished. The --full-timestamps flag, enables you to see the exact date and time of when the commit was opened and when it was finished. If you specify a branch instead of a specific commit, Pachyderm displays the information about the HEAD of the branch. Example $ pachctl inspect commit raw_data@master --full-timestamps Commit: raw_data@8248d97632874103823c7603fb8c851c Original Branch: master Parent: 22cdb5ae05cb40868566586140ea5ed5 Started: 2019 -07-29T18:09:51.397535516Z Finished: 2019 -07-29T18:09:51.500669562Z Size: 5 .121MiB The delete commit command enables you to delete opened and closed commits, which results in permanent loss of all the data introduced in those commits. You can think about the delete commit command as an equivalent of the rm -rf command in Linux. It is an irreversible operation that should be used with caution. An alternative and a much safer way to revert incorrect data changes is to move the HEAD of the branch or create a new commit that removes the incorrect data. Example $ pachctl delete commit raw_data@8248d97632874103823c7603fb8c851c","title":"Commit"},{"location":"concepts/data-concepts/file/","text":"File \u00b6 A file is a Unix filesystem object, which is a directory or file, that stores data. Unlike source code version-control systems that are most suitable for storing plain text files, you can store any type of file in Pachyderm, including binary files. Often, data scientists operate with comma-separated values (CSV), JavaScript Object Notation (JSON), images, and other plain text and binary file formats. Pachyderm supports all file sizes and formats and applies storage optimization techniques, such as deduplication, in the background. To upload your files to a Pachyderm repository, run the pachctl put file command. By using the pachctl put file command, you can put both files and directories into a Pachyderm repository. File Processing Strategies \u00b6 Pachyderm provides the following file processing strategies: Appending files By default, when you put a file into a Pachyderm repository and a file by the same name already exists in the repo, Pachyderm appends the new data to the existing file. For example, you have an A.csv file in a repository. If you upload the same file to that repository, Pachyderm appends the data to the existing file, which results in the A.csv file having twice the data from its original size. Example View the list of files: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 258B Add the A.csv file once again: $ pachctl put file images@master -f A.csv Verify that the file has doubled in size: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 516B Overwriting files When you enable the overwrite mode by using the --overwrite flag or -o , the file replaces the existing file instead of appending to it. For example, you have an A.csv file in the images repository. If you upload the same file to that repository with the --overwrite flag, Pachyderm overwrites the whole file. Example View the list of files: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 258B Add the A.csv file once again: $ pachctl put file --overwrite images@master -f A.csv Check the file size: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 258B","title":"File"},{"location":"concepts/data-concepts/file/#file","text":"A file is a Unix filesystem object, which is a directory or file, that stores data. Unlike source code version-control systems that are most suitable for storing plain text files, you can store any type of file in Pachyderm, including binary files. Often, data scientists operate with comma-separated values (CSV), JavaScript Object Notation (JSON), images, and other plain text and binary file formats. Pachyderm supports all file sizes and formats and applies storage optimization techniques, such as deduplication, in the background. To upload your files to a Pachyderm repository, run the pachctl put file command. By using the pachctl put file command, you can put both files and directories into a Pachyderm repository.","title":"File"},{"location":"concepts/data-concepts/file/#file-processing-strategies","text":"Pachyderm provides the following file processing strategies: Appending files By default, when you put a file into a Pachyderm repository and a file by the same name already exists in the repo, Pachyderm appends the new data to the existing file. For example, you have an A.csv file in a repository. If you upload the same file to that repository, Pachyderm appends the data to the existing file, which results in the A.csv file having twice the data from its original size. Example View the list of files: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 258B Add the A.csv file once again: $ pachctl put file images@master -f A.csv Verify that the file has doubled in size: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 516B Overwriting files When you enable the overwrite mode by using the --overwrite flag or -o , the file replaces the existing file instead of appending to it. For example, you have an A.csv file in the images repository. If you upload the same file to that repository with the --overwrite flag, Pachyderm overwrites the whole file. Example View the list of files: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 258B Add the A.csv file once again: $ pachctl put file --overwrite images@master -f A.csv Check the file size: $ pachctl list file images@master NAME TYPE SIZE /A.csv file 258B","title":"File Processing Strategies"},{"location":"concepts/data-concepts/history/","text":"History \u00b6 Pachyderm implements rich version-control and history semantics. This section describes the core concepts and architecture of Pachyderm's version control and the various ways to use the system to access historical data. The following abstractions store the history of your data: Commits In Pachyderm, commits are the core version-control primitive that is similar to Git commits. Commits represent an immutable snapshot of a filesystem and can be accessed with an ID. Commits have a parentage structure, where new commits inherit content from their parents. You can think of this parentage structure as of a linked list or a chain of commits . Commit IDs are useful if you want to have a static pointer to a snapshot of a filesystem. However, because they are static, their use is limited. Instead, you mostly work with branches. Branches Branches are pointers to commits that are similar to Git branches. Typically, Branches have semantically meaningful names such as master and staging . Branches are mutable, and they move along a growing chain of commits as you commit to the branch, and can even be reassigned to any commit within the repo by using the pachctl create branch command. The commit that a branch points to is referred to as the branches head , and the head's ancestors are referred to as on the branch . Branches can be substituted for commits in Pachyderm's API and behave as if the head of the branch were passed. This allows you to deal with semantically meaningful names for commits that can be updated, rather than static opaque identifiers. Ancestry Syntax \u00b6 Pachyderm's commits and branches support a familiar Git syntax for referencing their history. A commit or branch parent can be referenced by adding a ^ to the end of the commit or branch. Similar to how master resolves to the head commit of master , master^ resolves to the parent of the head commit. You can add multiple ^ s. For example, master^^ resolves to the parent of the parent of the head commit of master , and so on. Similarly, master^3 has the same meaning as master^^^ . Git supports two characters for ancestor references\u2014 ^ and ~ \u2014 with slightly different meanings. Pachyderm supports both characters as well, but their meaning is identical. Also, Pachyderm also supports a type of ancestor reference that Git does not\u2014 forward references, these use a different special character . and resolve to commits on the beginning of commit chains. For example, master.1 is the first (oldest) commit on the master branch, master.2 is the second commit, and so on. Resolving ancestry syntax requires traversing chains of commits high numbers passed to ^ and low numbers passed to . . These operations require traversing a large number of commits which might take a long time. If you plan to repeatedly access an ancestor, you might want to resolve that ancestor to a static commit ID with pachctl inspect commit and use that ID for future accesses. View the Filesystem Object History \u00b6 Pachyderm enables you to view the history of filesystem objects by using the --history flag with the pachctl list file command. This flag takes a single argument, an integer, which indicates how many historical versions you want to display. For example, you can get the two most recent versions of a file with the following command: $ pachctl list file repo@master:/file --history 2 COMMIT NAME TYPE COMMITTED SIZE 73ba56144be94f5bad1ce64e6b96eade /file file 16 seconds ago 8B c5026f053a7f482fbd719dadecec8f89 /file file 21 seconds ago 4B This command might return a different result from if you run pachctl list file repo@master:/file followed by pachctl list file repo@master^:/file . The history flag looks for changes to the file, and the file might not be changed with every commit. Similar to the ancestry syntax above, because the history flag requires traversing through a linked list of commits, this operation can be expensive. You can get back the full history of a file by passing all to the history flag. Example: $ pachctl list file edges@master:liberty.png --history all COMMIT NAME TYPE COMMITTED SIZE ff479f3a639344daa9474e729619d258 /liberty.png file 23 hours ago 22 .22KiB View the Pipeline History \u00b6 Pipelines are the main processing primitive in Pachyderm. However, they expose version-control and history semantics similar to filesystem objects. This is largely because, under the hood, they are implemented in terms of filesystem objects. You can access previous versions of a pipeline by using the same ancestry syntax that works for commits and branches. For example, pachctl inspect pipeline foo^ gives you the previous version of the pipeline foo . The pachctl inspect pipeline foo.1 command returns the first ever version of that same pipeline. You can use this syntax in all operations and scripts that accept pipeline names. To view historical versions of a pipeline use the --history flag with the pachctl list pipeline command: $ pachctl list pipeline --history all NAME VERSION INPUT CREATED STATE / LAST JOB Pipeline2 1 input2:/* 4 hours ago running / success Pipeline1 3 input1:/* 4 hours ago running / success Pipeline1 2 input1:/* 4 hours ago running / success Pipeline1 1 input1:/* 4 hours ago running / success A common operation with pipelines is reverting a pipeline to a previous. To revert a pipeline to a previous version, run the following command: $ pachctl extract pipeline pipeline^ | pachctl create pipeline View the Job History \u00b6 Jobs do not have versioning semantics associated with them. However, they are strongly associated with the pipelines that created them. Therefore, they inherit some of their versioning semantics. You can use the -p <pipeline> flag with the pachctl list job command to list all the jobs that were run for the latest version of the pipeline. To view a previous version of a pipeline you can add the caret symbol to the end of the pipeline name. For example -p edges^ . Furthermore you can get jobs from multiple versions of pipelines by passing the --history flag. For example, pachctl list job --history all returns all jobs from all versions of all pipelines. To view job history, run the following command: By using the -p flag: $ pachctl list job -p <pipeline^> By using the history flag: $ pachctl list job --history all","title":"History"},{"location":"concepts/data-concepts/history/#history","text":"Pachyderm implements rich version-control and history semantics. This section describes the core concepts and architecture of Pachyderm's version control and the various ways to use the system to access historical data. The following abstractions store the history of your data: Commits In Pachyderm, commits are the core version-control primitive that is similar to Git commits. Commits represent an immutable snapshot of a filesystem and can be accessed with an ID. Commits have a parentage structure, where new commits inherit content from their parents. You can think of this parentage structure as of a linked list or a chain of commits . Commit IDs are useful if you want to have a static pointer to a snapshot of a filesystem. However, because they are static, their use is limited. Instead, you mostly work with branches. Branches Branches are pointers to commits that are similar to Git branches. Typically, Branches have semantically meaningful names such as master and staging . Branches are mutable, and they move along a growing chain of commits as you commit to the branch, and can even be reassigned to any commit within the repo by using the pachctl create branch command. The commit that a branch points to is referred to as the branches head , and the head's ancestors are referred to as on the branch . Branches can be substituted for commits in Pachyderm's API and behave as if the head of the branch were passed. This allows you to deal with semantically meaningful names for commits that can be updated, rather than static opaque identifiers.","title":"History"},{"location":"concepts/data-concepts/history/#ancestry-syntax","text":"Pachyderm's commits and branches support a familiar Git syntax for referencing their history. A commit or branch parent can be referenced by adding a ^ to the end of the commit or branch. Similar to how master resolves to the head commit of master , master^ resolves to the parent of the head commit. You can add multiple ^ s. For example, master^^ resolves to the parent of the parent of the head commit of master , and so on. Similarly, master^3 has the same meaning as master^^^ . Git supports two characters for ancestor references\u2014 ^ and ~ \u2014 with slightly different meanings. Pachyderm supports both characters as well, but their meaning is identical. Also, Pachyderm also supports a type of ancestor reference that Git does not\u2014 forward references, these use a different special character . and resolve to commits on the beginning of commit chains. For example, master.1 is the first (oldest) commit on the master branch, master.2 is the second commit, and so on. Resolving ancestry syntax requires traversing chains of commits high numbers passed to ^ and low numbers passed to . . These operations require traversing a large number of commits which might take a long time. If you plan to repeatedly access an ancestor, you might want to resolve that ancestor to a static commit ID with pachctl inspect commit and use that ID for future accesses.","title":"Ancestry Syntax"},{"location":"concepts/data-concepts/history/#view-the-filesystem-object-history","text":"Pachyderm enables you to view the history of filesystem objects by using the --history flag with the pachctl list file command. This flag takes a single argument, an integer, which indicates how many historical versions you want to display. For example, you can get the two most recent versions of a file with the following command: $ pachctl list file repo@master:/file --history 2 COMMIT NAME TYPE COMMITTED SIZE 73ba56144be94f5bad1ce64e6b96eade /file file 16 seconds ago 8B c5026f053a7f482fbd719dadecec8f89 /file file 21 seconds ago 4B This command might return a different result from if you run pachctl list file repo@master:/file followed by pachctl list file repo@master^:/file . The history flag looks for changes to the file, and the file might not be changed with every commit. Similar to the ancestry syntax above, because the history flag requires traversing through a linked list of commits, this operation can be expensive. You can get back the full history of a file by passing all to the history flag. Example: $ pachctl list file edges@master:liberty.png --history all COMMIT NAME TYPE COMMITTED SIZE ff479f3a639344daa9474e729619d258 /liberty.png file 23 hours ago 22 .22KiB","title":"View the Filesystem Object History"},{"location":"concepts/data-concepts/history/#view-the-pipeline-history","text":"Pipelines are the main processing primitive in Pachyderm. However, they expose version-control and history semantics similar to filesystem objects. This is largely because, under the hood, they are implemented in terms of filesystem objects. You can access previous versions of a pipeline by using the same ancestry syntax that works for commits and branches. For example, pachctl inspect pipeline foo^ gives you the previous version of the pipeline foo . The pachctl inspect pipeline foo.1 command returns the first ever version of that same pipeline. You can use this syntax in all operations and scripts that accept pipeline names. To view historical versions of a pipeline use the --history flag with the pachctl list pipeline command: $ pachctl list pipeline --history all NAME VERSION INPUT CREATED STATE / LAST JOB Pipeline2 1 input2:/* 4 hours ago running / success Pipeline1 3 input1:/* 4 hours ago running / success Pipeline1 2 input1:/* 4 hours ago running / success Pipeline1 1 input1:/* 4 hours ago running / success A common operation with pipelines is reverting a pipeline to a previous. To revert a pipeline to a previous version, run the following command: $ pachctl extract pipeline pipeline^ | pachctl create pipeline","title":"View the Pipeline History"},{"location":"concepts/data-concepts/history/#view-the-job-history","text":"Jobs do not have versioning semantics associated with them. However, they are strongly associated with the pipelines that created them. Therefore, they inherit some of their versioning semantics. You can use the -p <pipeline> flag with the pachctl list job command to list all the jobs that were run for the latest version of the pipeline. To view a previous version of a pipeline you can add the caret symbol to the end of the pipeline name. For example -p edges^ . Furthermore you can get jobs from multiple versions of pipelines by passing the --history flag. For example, pachctl list job --history all returns all jobs from all versions of all pipelines. To view job history, run the following command: By using the -p flag: $ pachctl list job -p <pipeline^> By using the history flag: $ pachctl list job --history all","title":"View the Job History"},{"location":"concepts/data-concepts/provenance/","text":"Provenance \u00b6 Data versioning enables Pachyderm users to go back in time and see the state of a dataset or repository at a particular moment in time. Data provenance (from the French provenir which means the place of origin ), also known as data lineage, tracks the dependencies and relationships between datasets. Provenance answers not only the question of where the data comes from, but also how the data was transformed along the way. Data scientists use provenance in root cause analysis to improve their code, workflows, and understanding of the data and its implications on final results. Data scientists need to have confidence in the information with which they operate. They need to be able to reproduce the results and sometimes go through the whole data transformation process from scratch multiple times, which makes data provenance one of the most critical aspects of data analysis. If your computations result in unexpected numbers, the first place to look is the historical data that gives insights into possible flaws in the transformation chain or the data itself. For example, when a bank makes a decision about a mortgage application, many factors are taken into consideration, including the credit history, annual income, and loan size. This data goes through multiple automated steps of analysis with numerous dependencies and decisions made along the way. If the final decision does not satisfy the applicant, the historical data is the first place to look for proof of authenticity, as well as for possible prejudice or model bias against the applicant. Data provenance creates a complete audit trail that enables data scientists to track the data from its origin through to the final decision and make appropriate changes that address issues. With the adoption of General Data Protection Regulation (GDPR) compliance requirements, monitoring data lineage is becoming a necessity for many organizations that work with sensitive data. Pachyderm implements provenance for both commits and repositories. You can track revisions of the data and understand the connection between the data stored in one repository and the results in the other repository. Collaboration takes data provenance even further. Provenance enables teams of data scientists across the globe to build on each other work, share, transform, and update datasets while automatically maintaining a complete audit trail so that all results are reproducible. The following diagram demonstrates how provenance works: In the diagram above, you can see two input repositories called parameters and training-data . The training-data repository continuously collects data from an outside source. The training model pipeline combines the data from these two repositories, trains many models, and runs tests to select the best one. Provenance helps you to understand how and why the best model was selected and enables you to track the origin of the best model. In the diagram above, the best model is represented with a purple circle. By using provenance, you can find that the best model was created from the commit 2 in the training-data repository and the commit 1 in the parameters repository. Tracking Provenance in Pachyderm \u00b6 Pachyderm provides the pachctl inspect command that enables you to track provenance of your commits and learn where the data in the repository originates in. Example $ pachctl inspect commit split@master Commit: split@f71e42704b734598a89c02026c8f7d13 Original Branch: master Started: 4 minutes ago Finished: 3 minutes ago Size: 0B Provenance: __spec__@8c6440f52a2d4aa3980163e25557b4a1 ( split ) raw_data@ccf82debb4b94ca3bfe165aca8d517c3 ( master ) In the example above, you can see that the latest commit in the master branch of the split repository tracks back to the master branch in the raw_data repository. Tracking Provenance Downstream \u00b6 Pachyderm provides the flush commit command that enables you to track provenance downstream. Tracking downstream means that instead of tracking the origin of a commit, you can learn in which output repository a certain input has resulted. For example, you have the ccf82debb4b94ca3bfe165aca8d517c3 commit in the raw_data repository. If you run the pachctl flush commit command for this commit. Example $ pachctl flush commit raw_data@ccf82debb4b94ca3bfe165aca8d517c3 REPO BRANCH COMMIT PARENT STARTED DURATION SIZE split master f71e42704b734598a89c02026c8f7d13 <none> 52 minutes ago About a minute 0B split stats 9b46d7abf9a74bf7bf66c77f2a0da4b1 <none> 52 minutes ago About a minute 15 .39MiB pre_process master a99ab362dc944b108fb33544b2b24a8c <none> 48 minutes ago About a minute 0B","title":"Provenance"},{"location":"concepts/data-concepts/provenance/#provenance","text":"Data versioning enables Pachyderm users to go back in time and see the state of a dataset or repository at a particular moment in time. Data provenance (from the French provenir which means the place of origin ), also known as data lineage, tracks the dependencies and relationships between datasets. Provenance answers not only the question of where the data comes from, but also how the data was transformed along the way. Data scientists use provenance in root cause analysis to improve their code, workflows, and understanding of the data and its implications on final results. Data scientists need to have confidence in the information with which they operate. They need to be able to reproduce the results and sometimes go through the whole data transformation process from scratch multiple times, which makes data provenance one of the most critical aspects of data analysis. If your computations result in unexpected numbers, the first place to look is the historical data that gives insights into possible flaws in the transformation chain or the data itself. For example, when a bank makes a decision about a mortgage application, many factors are taken into consideration, including the credit history, annual income, and loan size. This data goes through multiple automated steps of analysis with numerous dependencies and decisions made along the way. If the final decision does not satisfy the applicant, the historical data is the first place to look for proof of authenticity, as well as for possible prejudice or model bias against the applicant. Data provenance creates a complete audit trail that enables data scientists to track the data from its origin through to the final decision and make appropriate changes that address issues. With the adoption of General Data Protection Regulation (GDPR) compliance requirements, monitoring data lineage is becoming a necessity for many organizations that work with sensitive data. Pachyderm implements provenance for both commits and repositories. You can track revisions of the data and understand the connection between the data stored in one repository and the results in the other repository. Collaboration takes data provenance even further. Provenance enables teams of data scientists across the globe to build on each other work, share, transform, and update datasets while automatically maintaining a complete audit trail so that all results are reproducible. The following diagram demonstrates how provenance works: In the diagram above, you can see two input repositories called parameters and training-data . The training-data repository continuously collects data from an outside source. The training model pipeline combines the data from these two repositories, trains many models, and runs tests to select the best one. Provenance helps you to understand how and why the best model was selected and enables you to track the origin of the best model. In the diagram above, the best model is represented with a purple circle. By using provenance, you can find that the best model was created from the commit 2 in the training-data repository and the commit 1 in the parameters repository.","title":"Provenance"},{"location":"concepts/data-concepts/provenance/#tracking-provenance-in-pachyderm","text":"Pachyderm provides the pachctl inspect command that enables you to track provenance of your commits and learn where the data in the repository originates in. Example $ pachctl inspect commit split@master Commit: split@f71e42704b734598a89c02026c8f7d13 Original Branch: master Started: 4 minutes ago Finished: 3 minutes ago Size: 0B Provenance: __spec__@8c6440f52a2d4aa3980163e25557b4a1 ( split ) raw_data@ccf82debb4b94ca3bfe165aca8d517c3 ( master ) In the example above, you can see that the latest commit in the master branch of the split repository tracks back to the master branch in the raw_data repository.","title":"Tracking Provenance in Pachyderm"},{"location":"concepts/data-concepts/provenance/#tracking-provenance-downstream","text":"Pachyderm provides the flush commit command that enables you to track provenance downstream. Tracking downstream means that instead of tracking the origin of a commit, you can learn in which output repository a certain input has resulted. For example, you have the ccf82debb4b94ca3bfe165aca8d517c3 commit in the raw_data repository. If you run the pachctl flush commit command for this commit. Example $ pachctl flush commit raw_data@ccf82debb4b94ca3bfe165aca8d517c3 REPO BRANCH COMMIT PARENT STARTED DURATION SIZE split master f71e42704b734598a89c02026c8f7d13 <none> 52 minutes ago About a minute 0B split stats 9b46d7abf9a74bf7bf66c77f2a0da4b1 <none> 52 minutes ago About a minute 15 .39MiB pre_process master a99ab362dc944b108fb33544b2b24a8c <none> 48 minutes ago About a minute 0B","title":"Tracking Provenance Downstream"},{"location":"concepts/data-concepts/repo/","text":"Repository \u00b6 A Pachyderm repository is a location where you store your data inside Pachyderm. A Pachyderm repository is a top-level data object that contains files and folders. Similar to Git, a Pachyderm repository tracks all changes to the data and creates a history of data modifications that you can access and review. You can store any type of file in a Pachyderm repo, including binary and plain text files. Unlike a Git repository that stores history in a .git file in your copy of a Git repo, Pachyderm stores the history of your commits in a centralized location. Because of that, you do not run into merge conflicts as you often do with Git commits when you try to merge your .git history with the master copy of the repo. With large datatsets resolving a merge conflict might not be possible. A Pachyderm repository is the first entity that you configure to create A Pachyderm repository is the first entity that you configure when you want to add data to Pachyderm pachctl create repo command or by using the Pachyderm UI. After creating the repository, you can add your data by using the pachctl put file command. The following types of repositories exist in Pachyderm: Input repositories Users or external applications outside of Pachyderm can add data to the input repositories for further processing. Output repositories Pachyderm automatically creates output repositories pipelines write results of computations into these repositories. You can view the list of repositories in your Pachyderm cluster by running the pachctl list repo command. Example $ pachctl list repo NAME CREATED SIZE ( MASTER ) raw_data 6 hours ago 0B The pachctl inspect repo command provides a more detailed overview of a specified repository. Example $ pachctl inspect repo raw_data Name: raw_data Description: A raw data repository Created: 6 hours ago Size of HEAD on master: 5 .121MiB If you need to delete a repository, you can run the pachctl delete command . This command deletes all data and the information about the specified repository, such as commit history. The delete operation is irreversible and results in a complete cleanup of your Pachyderm cluster. If you run the delete command with the --all flag, all repositories will be deleted. See also: Pipeline","title":"Repository"},{"location":"concepts/data-concepts/repo/#repository","text":"A Pachyderm repository is a location where you store your data inside Pachyderm. A Pachyderm repository is a top-level data object that contains files and folders. Similar to Git, a Pachyderm repository tracks all changes to the data and creates a history of data modifications that you can access and review. You can store any type of file in a Pachyderm repo, including binary and plain text files. Unlike a Git repository that stores history in a .git file in your copy of a Git repo, Pachyderm stores the history of your commits in a centralized location. Because of that, you do not run into merge conflicts as you often do with Git commits when you try to merge your .git history with the master copy of the repo. With large datatsets resolving a merge conflict might not be possible. A Pachyderm repository is the first entity that you configure to create A Pachyderm repository is the first entity that you configure when you want to add data to Pachyderm pachctl create repo command or by using the Pachyderm UI. After creating the repository, you can add your data by using the pachctl put file command. The following types of repositories exist in Pachyderm: Input repositories Users or external applications outside of Pachyderm can add data to the input repositories for further processing. Output repositories Pachyderm automatically creates output repositories pipelines write results of computations into these repositories. You can view the list of repositories in your Pachyderm cluster by running the pachctl list repo command. Example $ pachctl list repo NAME CREATED SIZE ( MASTER ) raw_data 6 hours ago 0B The pachctl inspect repo command provides a more detailed overview of a specified repository. Example $ pachctl inspect repo raw_data Name: raw_data Description: A raw data repository Created: 6 hours ago Size of HEAD on master: 5 .121MiB If you need to delete a repository, you can run the pachctl delete command . This command deletes all data and the information about the specified repository, such as commit history. The delete operation is irreversible and results in a complete cleanup of your Pachyderm cluster. If you run the delete command with the --all flag, all repositories will be deleted. See also: Pipeline","title":"Repository"},{"location":"concepts/pipeline-concepts/","text":"Pipeline Concepts \u00b6 Pachyderm Pipeline System (PPS) is the computational component of the Pachyderm platform that enables you to perform various transformations on your data. Pachyderm pipelines have the following main concepts: Pipeline A pipeline is a job-spawner that waits for certain conditions to be met. Most commonly, this means watching one or more Pachyderm repositories for new data. When a new data arrives, a pipeline executes a user-defined piece of code to perform an operation and process the data. Each of these executions is called a job. Job A job is an individual execution of a pipeline. A job can succeed or fail. Within a job, data and processing can be broken up into individual units of work called datums. Datum A datum is the smallest indivisible unit of work within a job. Different datums can be processed in parallel within a job. Service A service is a special type of pipeline that instead of executing jobs and then waiting, permanently runs a serving data through an endpoint. For example, you can be serving an ML model or a REST API that can be queried. A service reads data from Pachyderm but does not have an output repo. Spout A spout is a special type of pipeline for ingesting data from a data stream. A spout can subscribe to a message stream, such as Kafka or Amazon SQS, and ingest data when it receives a message. A spout does not have an input repo.","title":"Overview"},{"location":"concepts/pipeline-concepts/#pipeline-concepts","text":"Pachyderm Pipeline System (PPS) is the computational component of the Pachyderm platform that enables you to perform various transformations on your data. Pachyderm pipelines have the following main concepts: Pipeline A pipeline is a job-spawner that waits for certain conditions to be met. Most commonly, this means watching one or more Pachyderm repositories for new data. When a new data arrives, a pipeline executes a user-defined piece of code to perform an operation and process the data. Each of these executions is called a job. Job A job is an individual execution of a pipeline. A job can succeed or fail. Within a job, data and processing can be broken up into individual units of work called datums. Datum A datum is the smallest indivisible unit of work within a job. Different datums can be processed in parallel within a job. Service A service is a special type of pipeline that instead of executing jobs and then waiting, permanently runs a serving data through an endpoint. For example, you can be serving an ML model or a REST API that can be queried. A service reads data from Pachyderm but does not have an output repo. Spout A spout is a special type of pipeline for ingesting data from a data stream. A spout can subscribe to a message stream, such as Kafka or Amazon SQS, and ingest data when it receives a message. A spout does not have an input repo.","title":"Pipeline Concepts"},{"location":"concepts/pipeline-concepts/distributed_computing/","text":"Distributed Computing \u00b6 Distributing your computations across multiple workers is a fundamental part of any big data processing. When you build production-scale pipelines, you need to adjust the number of workers and resources that are allocated to each job to optimize throughput. Pachyderm Workers \u00b6 A Pachyderm worker is an identical Kubernetes pod that runs the Docker image that you specified in the pipeline spec . Your analysis code does not affect how Pachyderm distributes the workload among workers. Instead, Pachyderm spreads out the data that needs to be processed across the various workers and makes that data available for your code. When you create a pipeline, Pachyderm spins up worker pods that continuously run in the cluster waiting for new data to be available for processing. You can change this behavior by setting \"standby\" :true . Therefore, you do not need to recreate and schedule workers for every new job. For each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance. Controlling the Number of Workers \u00b6 You can control the number of worker pods that Pachyderm runs in a pipeline by defining the parallelism parameter in the pipeline specification . \"parallelism_spec\": { // Exactly one of these two fields should be set \"constant\": int \"coefficient\": double Pachyderm has the following parallelism strategies that you can set in the pipeline spec: Strategy Description constant Pachyderm starts the specified number of workers. For example, if you set \"constant\":10 , Pachyderm spreads the computation workload among ten workers. coefficient Pachyderm starts a number of workers that is a multiple of your Kubernetes cluster size. For example, if your Kubernetes cluster has ten nodes, and you set \"coefficient\": 0.5 , Pachyderm starts five workers. If you set parallelism to \"coefficient\": 2.0 , Pachyderm starts twenty workers. By default, Pachyderm sets parallelism to \u201ccoefficient\": 1 , which means that it spawns one worker per Kubernetes node for this pipeline. See also: Glob Pattern Pipeline Specification","title":"Distributed Computing"},{"location":"concepts/pipeline-concepts/distributed_computing/#distributed-computing","text":"Distributing your computations across multiple workers is a fundamental part of any big data processing. When you build production-scale pipelines, you need to adjust the number of workers and resources that are allocated to each job to optimize throughput.","title":"Distributed Computing"},{"location":"concepts/pipeline-concepts/distributed_computing/#pachyderm-workers","text":"A Pachyderm worker is an identical Kubernetes pod that runs the Docker image that you specified in the pipeline spec . Your analysis code does not affect how Pachyderm distributes the workload among workers. Instead, Pachyderm spreads out the data that needs to be processed across the various workers and makes that data available for your code. When you create a pipeline, Pachyderm spins up worker pods that continuously run in the cluster waiting for new data to be available for processing. You can change this behavior by setting \"standby\" :true . Therefore, you do not need to recreate and schedule workers for every new job. For each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance.","title":"Pachyderm Workers"},{"location":"concepts/pipeline-concepts/distributed_computing/#controlling-the-number-of-workers","text":"You can control the number of worker pods that Pachyderm runs in a pipeline by defining the parallelism parameter in the pipeline specification . \"parallelism_spec\": { // Exactly one of these two fields should be set \"constant\": int \"coefficient\": double Pachyderm has the following parallelism strategies that you can set in the pipeline spec: Strategy Description constant Pachyderm starts the specified number of workers. For example, if you set \"constant\":10 , Pachyderm spreads the computation workload among ten workers. coefficient Pachyderm starts a number of workers that is a multiple of your Kubernetes cluster size. For example, if your Kubernetes cluster has ten nodes, and you set \"coefficient\": 0.5 , Pachyderm starts five workers. If you set parallelism to \"coefficient\": 2.0 , Pachyderm starts twenty workers. By default, Pachyderm sets parallelism to \u201ccoefficient\": 1 , which means that it spawns one worker per Kubernetes node for this pipeline. See also: Glob Pattern Pipeline Specification","title":"Controlling the Number of Workers"},{"location":"concepts/pipeline-concepts/job/","text":"Job \u00b6 A Pachyderm job is an execution of a pipeline that triggers when new data is detected in an input repository. Each job runs your code against the current commit and then submits the results to the output repository and creates a single output commit. A pipeline triggers a new job every time you submit new changes, a commit, into your input source. Each job has the following stages: Stage Description Starting Pachyderm starts the job when it detects new data in the input repository. The new data appears as a commit in the input repository, and Pachyderm automatically launches the job. Pachyderm spins the number of Pachyderm worker pods specified in the pipeline spec and spreads the workload among them. Running Pachyderm runs the transformation code that is specified in the pipeline specification against the data in the input commit. Merging Pachyderm concatenates the results of the processed data into one or more files, uploads them to the output repository, completes the final output commits, and creates/persists all the versioning metadata","title":"Job"},{"location":"concepts/pipeline-concepts/job/#job","text":"A Pachyderm job is an execution of a pipeline that triggers when new data is detected in an input repository. Each job runs your code against the current commit and then submits the results to the output repository and creates a single output commit. A pipeline triggers a new job every time you submit new changes, a commit, into your input source. Each job has the following stages: Stage Description Starting Pachyderm starts the job when it detects new data in the input repository. The new data appears as a commit in the input repository, and Pachyderm automatically launches the job. Pachyderm spins the number of Pachyderm worker pods specified in the pipeline spec and spreads the workload among them. Running Pachyderm runs the transformation code that is specified in the pipeline specification against the data in the input commit. Merging Pachyderm concatenates the results of the processed data into one or more files, uploads them to the output repository, completes the final output commits, and creates/persists all the versioning metadata","title":"Job"},{"location":"concepts/pipeline-concepts/datum/","text":"Datum \u00b6 A datum is the smallest indivisible unit of computation within a A job can have one, many or no datums. Each datum is processed independently with a single execution of the user code and then the results of all the datums are merged together to create the final output commit. The number of datums for a job is defined by the [glob pattern] (glob-pattern.md) which you specify for each input. Think of datums as if you were telling Pachyderm how to divide your input data to efficiently distribute computation and only process the new data. You can configure a whole input repository to be one datum, each top-level filesystem object to be a separate datum, specific paths can be datums, and so on. Datums affect how Pachyderm distributes processing workloads and are instrumental in optimizing your configuration for best performance. Pachyderm takes each datum and processes it in isolation on one of the pipeline worker nodes. You can define datums, workers, and other performance parameters can all be configured through the corresponding fields in the pipeline specification . To understand how datums affect data processing in Pachyderm, you need to understand the following subconcepts: Glob Pattern Datum Processing","title":"Overview"},{"location":"concepts/pipeline-concepts/datum/#datum","text":"A datum is the smallest indivisible unit of computation within a A job can have one, many or no datums. Each datum is processed independently with a single execution of the user code and then the results of all the datums are merged together to create the final output commit. The number of datums for a job is defined by the [glob pattern] (glob-pattern.md) which you specify for each input. Think of datums as if you were telling Pachyderm how to divide your input data to efficiently distribute computation and only process the new data. You can configure a whole input repository to be one datum, each top-level filesystem object to be a separate datum, specific paths can be datums, and so on. Datums affect how Pachyderm distributes processing workloads and are instrumental in optimizing your configuration for best performance. Pachyderm takes each datum and processes it in isolation on one of the pipeline worker nodes. You can define datums, workers, and other performance parameters can all be configured through the corresponding fields in the pipeline specification . To understand how datums affect data processing in Pachyderm, you need to understand the following subconcepts: Glob Pattern Datum Processing","title":"Datum"},{"location":"concepts/pipeline-concepts/datum/cross-union/","text":"Cross and Union Inputs \u00b6 Pachyderm enables you to combine multiple input repositories in a single pipeline by using the union and cross operators in the pipeline specification. If you are familiar with Set theory , you can think of union as a disjoint union binary operator and cross as a cartesian product binary operator . However, if you are unfamiliar with these concepts, it is still easy to understand how cross and union work in Pachyderm. This section describes how to use cross and union in your pipelines and how you can optimize your code when you work with them. Union Input \u00b6 The union input combines each of the datums in the input repos as one set of datums. The number of datums that are processed is the sum of all the datums in each repo. For example, you have two input repos, A and B . Each of these repositories contain three files with the following names. Repository A has the following structure: A \u251c\u2500\u2500 1 .txt \u251c\u2500\u2500 2 .txt \u2514\u2500\u2500 3 .txt Repository B has the following structure: B \u251c\u2500\u2500 4 .txt \u251c\u2500\u2500 5 .txt \u2514\u2500\u2500 6 .txt If you want your pipeline to process each file independently as a separate datum, use a glob pattern of /* . Each glob is applied to each input independently. The input section in the pipeline spec might have the following structure: \"input\" : { \"union\" : [ { \"pfs\" : { \"glob\" : \"/*\" , \"repo\" : \"A\" } } , { \"pfs\" : { \"glob\" : \"/*\" , \"repo\" : \"B\" } } ] } In this example, each Pachyderm repository has those three files in the root directory, so three datums from each input. Therefore, the union of A and B has six datums in total. Your pipeline processes the following datums without any specific order: /pfs/A/1.txt /pfs/A/2.txt /pfs/A/3.txt /pfs/B/4.txt /pfs/B/5.txt /pfs/B/6.txt Note: Each datum in a pipeline is processed independently by a single execution of your code. In this example, your code runs six times, and each datum is available to it one at a time. For example, your code processes pfs/A/1.txt in one of the runs and pfs/B/5.txt in a different run, and so on. In a union, two or more datums are never available to your code at the same time. You can simplify your union code by using the name property as described below. Simplifying the Union Pipelines Code \u00b6 In the example above, your code needs to read into the pfs/A or pfs/B directory because only one of them is present in any given datum. To simplify your code, you can add the name field to the pfs object and give the same name to each of the input repos. For example, you can add, the name field with the value C to the input repositories A and B : \"input\": { \"union\": [ { \"pfs\": { \"name\": \"C\", \"glob\": \"/*\", \"repo\": \"A\" } }, { \"pfs\": { \"name\": \"C\", \"glob\": \"/*\", \"repo\": \"B\" } } ] } Then, in the pipeline, all datums appear in the same directory. /pfs/C/1.txt # from A /pfs/C/2.txt # from A /pfs/C/3.txt # from A /pfs/C/4.txt # from B /pfs/C/5.txt # from B /pfs/C/6.txt # from B Cross Input \u00b6 In a cross input, Pachyderm exposes every combination of datums, or a cross-product, from each of your input repositories to your code in a single run. In other words, a cross input pairs every datum in one repository with each datum in another, creating sets of datums. Your transformation code is provided one of these sets at the time to process. For example, you have repositories A and B with three datums, each with the following structure: Note: For this example, the glob pattern is set to /* . Repository A has three files at the top level: A \u251c\u2500\u2500 1 .txt \u251c\u2500\u2500 2 .txt \u2514\u2500\u2500 3 .txt Repository B has three files at the top level: B \u251c\u2500\u2500 4 .txt \u251c\u2500\u2500 5 .txt \u2514\u2500\u2500 6 .txt Because you have three datums in each repo, Pachyderm exposes a total of nine combinations of datums to your code. Important: In cross pipelines, both pfs/A and pfs/B directories are visible during each code run. Run 1 : /pfs/A/1.txt /pfs/B/1.txt Run 2 : /pfs/A/1.txt /pfs/B/2.txt ... Run 9 : /pfs/A/3.txt /pfs/B/3.txt Note: In cross inputs, if you use the name field, your two inputs cannot have the same name. This could cause file system collisions. See Also: Cross Input Union Input Combining/Merging/Joining Data Distributed hyperparameter tuning","title":"Cross and Union Inputs"},{"location":"concepts/pipeline-concepts/datum/cross-union/#cross-and-union-inputs","text":"Pachyderm enables you to combine multiple input repositories in a single pipeline by using the union and cross operators in the pipeline specification. If you are familiar with Set theory , you can think of union as a disjoint union binary operator and cross as a cartesian product binary operator . However, if you are unfamiliar with these concepts, it is still easy to understand how cross and union work in Pachyderm. This section describes how to use cross and union in your pipelines and how you can optimize your code when you work with them.","title":"Cross and Union Inputs"},{"location":"concepts/pipeline-concepts/datum/cross-union/#union-input","text":"The union input combines each of the datums in the input repos as one set of datums. The number of datums that are processed is the sum of all the datums in each repo. For example, you have two input repos, A and B . Each of these repositories contain three files with the following names. Repository A has the following structure: A \u251c\u2500\u2500 1 .txt \u251c\u2500\u2500 2 .txt \u2514\u2500\u2500 3 .txt Repository B has the following structure: B \u251c\u2500\u2500 4 .txt \u251c\u2500\u2500 5 .txt \u2514\u2500\u2500 6 .txt If you want your pipeline to process each file independently as a separate datum, use a glob pattern of /* . Each glob is applied to each input independently. The input section in the pipeline spec might have the following structure: \"input\" : { \"union\" : [ { \"pfs\" : { \"glob\" : \"/*\" , \"repo\" : \"A\" } } , { \"pfs\" : { \"glob\" : \"/*\" , \"repo\" : \"B\" } } ] } In this example, each Pachyderm repository has those three files in the root directory, so three datums from each input. Therefore, the union of A and B has six datums in total. Your pipeline processes the following datums without any specific order: /pfs/A/1.txt /pfs/A/2.txt /pfs/A/3.txt /pfs/B/4.txt /pfs/B/5.txt /pfs/B/6.txt Note: Each datum in a pipeline is processed independently by a single execution of your code. In this example, your code runs six times, and each datum is available to it one at a time. For example, your code processes pfs/A/1.txt in one of the runs and pfs/B/5.txt in a different run, and so on. In a union, two or more datums are never available to your code at the same time. You can simplify your union code by using the name property as described below.","title":"Union Input"},{"location":"concepts/pipeline-concepts/datum/cross-union/#simplifying-the-union-pipelines-code","text":"In the example above, your code needs to read into the pfs/A or pfs/B directory because only one of them is present in any given datum. To simplify your code, you can add the name field to the pfs object and give the same name to each of the input repos. For example, you can add, the name field with the value C to the input repositories A and B : \"input\": { \"union\": [ { \"pfs\": { \"name\": \"C\", \"glob\": \"/*\", \"repo\": \"A\" } }, { \"pfs\": { \"name\": \"C\", \"glob\": \"/*\", \"repo\": \"B\" } } ] } Then, in the pipeline, all datums appear in the same directory. /pfs/C/1.txt # from A /pfs/C/2.txt # from A /pfs/C/3.txt # from A /pfs/C/4.txt # from B /pfs/C/5.txt # from B /pfs/C/6.txt # from B","title":"Simplifying the Union Pipelines Code"},{"location":"concepts/pipeline-concepts/datum/cross-union/#cross-input","text":"In a cross input, Pachyderm exposes every combination of datums, or a cross-product, from each of your input repositories to your code in a single run. In other words, a cross input pairs every datum in one repository with each datum in another, creating sets of datums. Your transformation code is provided one of these sets at the time to process. For example, you have repositories A and B with three datums, each with the following structure: Note: For this example, the glob pattern is set to /* . Repository A has three files at the top level: A \u251c\u2500\u2500 1 .txt \u251c\u2500\u2500 2 .txt \u2514\u2500\u2500 3 .txt Repository B has three files at the top level: B \u251c\u2500\u2500 4 .txt \u251c\u2500\u2500 5 .txt \u2514\u2500\u2500 6 .txt Because you have three datums in each repo, Pachyderm exposes a total of nine combinations of datums to your code. Important: In cross pipelines, both pfs/A and pfs/B directories are visible during each code run. Run 1 : /pfs/A/1.txt /pfs/B/1.txt Run 2 : /pfs/A/1.txt /pfs/B/2.txt ... Run 9 : /pfs/A/3.txt /pfs/B/3.txt Note: In cross inputs, if you use the name field, your two inputs cannot have the same name. This could cause file system collisions. See Also: Cross Input Union Input Combining/Merging/Joining Data Distributed hyperparameter tuning","title":"Cross Input"},{"location":"concepts/pipeline-concepts/datum/glob-pattern/","text":"Glob Pattern \u00b6 Defining how your data is spread among workers is one of the most important aspects of distributed computation and is the fundamental idea around concepts such as Map and Reduce. Instead of confining users to data-distribution patterns, such as Map, that splits everything as much as possible, and Reduce, that groups all the data, Pachyderm uses glob patterns to provide incredible flexibility to define data distribution. You can configure a glob pattern for each PFS input in the input field of a pipeline specification. Pachyderm detects this parameter and divides the input data into individual datums . You can think of each input repository as a filesystem where the glob pattern is applied to the root of the filesystem. The files and directories that match the glob pattern are considered datums. The Pachyderm's concept of glob patterns is similar to the Unix glob patterns. For example, the ls *.md command matches all files with the .md file extension. In Pachyderm, the / and * indicators are most commonly used globs. The following are examples of glob patterns that you can define: / \u2014 Pachyderm denotes the whole repository as a single datum and sends all of the input data to a single worker node to be processed together. /* \u2014 Pachyderm defines each top-level filesystem object, that is a file or a directory, in the input repo as a separate datum. For example, if you have a repository with ten files in it and no directory structure, Pachyderm identifies each file as a single datum and processes them independently. /*/* \u2014 Pachyderm processes each filesystem object in each subdirectory as a separate datum. If you have more than one input repo in your pipeline, you can define a different glob pattern for each input repo. You can combine the datums from each input repo by using either the cross or union operator to create the final datums that your code processes. For more information, see Cross and Union . Example of Defining Datums \u00b6 For example, you have the following directory: Example /California /San-Francisco.json /Los-Angeles.json ... /Colorado /Denver.json /Boulder.json ... ... Each top-level directory represents a US state with a json file for each city in that state. If you set glob pattern to / , every time you change anything in any of the files and directories or add a new file to the repository, Pachyderm processes the contents of the whole repository from scratch as a single datum. For example, if you add Sacramento.json to the California/ directory, Pachyderm processes all files and folders in the repo as a single datum. If you set /* as a glob pattern, Pachyderm processes the data for each state individually. It defines one datum per state, which means that all the cities for a given state are processed together by a single worker, but each state is processed independently. For example, if you add a new file Sacramento.json to the California/ directory, Pachyderm processes the California/ datum only. If you set /*/* , Pachyderm processes each city as a single datum on a separate worker. For example, if you add the Sacramento.json file, Pachyderm processes the Sacramento.json file only. Glob patterns also let you take only a particular directory or subset of directories as an input instead of the whole repo. For example, you can set /California/* to process only the data for the state of California. Therefore, if you add a new city in the Colorado/ directory, Pachyderm ignore this change and does not start the pipeline. However, if you add Sacramento.json to the California/ directory, Pachyderm processes the California/ datum. Test a Glob pattern \u00b6 You can use the pachctl glob file command to preview which filesystem objects a pipeline defines as datums. This command helps you to test various glob patterns before you use them in a pipeline. If you set the glob property to / , Pachyderm detects all top-level filesystem objects in the train repository as one datum: Example $ pachctl glob file train@master:/ NAME TYPE SIZE / dir 15 .11KiB If you set the glob property to / , Pachyderm detects each top-level filesystem object in the train repository as a separate datum: Example $ pachctl glob file train@master:/* NAME TYPE SIZE /IssueSummarization.py file 1 .224KiB /requirements.txt file 74B /seq2seq_utils.py file 13 .81KiB","title":"Glob Patterns"},{"location":"concepts/pipeline-concepts/datum/glob-pattern/#glob-pattern","text":"Defining how your data is spread among workers is one of the most important aspects of distributed computation and is the fundamental idea around concepts such as Map and Reduce. Instead of confining users to data-distribution patterns, such as Map, that splits everything as much as possible, and Reduce, that groups all the data, Pachyderm uses glob patterns to provide incredible flexibility to define data distribution. You can configure a glob pattern for each PFS input in the input field of a pipeline specification. Pachyderm detects this parameter and divides the input data into individual datums . You can think of each input repository as a filesystem where the glob pattern is applied to the root of the filesystem. The files and directories that match the glob pattern are considered datums. The Pachyderm's concept of glob patterns is similar to the Unix glob patterns. For example, the ls *.md command matches all files with the .md file extension. In Pachyderm, the / and * indicators are most commonly used globs. The following are examples of glob patterns that you can define: / \u2014 Pachyderm denotes the whole repository as a single datum and sends all of the input data to a single worker node to be processed together. /* \u2014 Pachyderm defines each top-level filesystem object, that is a file or a directory, in the input repo as a separate datum. For example, if you have a repository with ten files in it and no directory structure, Pachyderm identifies each file as a single datum and processes them independently. /*/* \u2014 Pachyderm processes each filesystem object in each subdirectory as a separate datum. If you have more than one input repo in your pipeline, you can define a different glob pattern for each input repo. You can combine the datums from each input repo by using either the cross or union operator to create the final datums that your code processes. For more information, see Cross and Union .","title":"Glob Pattern"},{"location":"concepts/pipeline-concepts/datum/glob-pattern/#example-of-defining-datums","text":"For example, you have the following directory: Example /California /San-Francisco.json /Los-Angeles.json ... /Colorado /Denver.json /Boulder.json ... ... Each top-level directory represents a US state with a json file for each city in that state. If you set glob pattern to / , every time you change anything in any of the files and directories or add a new file to the repository, Pachyderm processes the contents of the whole repository from scratch as a single datum. For example, if you add Sacramento.json to the California/ directory, Pachyderm processes all files and folders in the repo as a single datum. If you set /* as a glob pattern, Pachyderm processes the data for each state individually. It defines one datum per state, which means that all the cities for a given state are processed together by a single worker, but each state is processed independently. For example, if you add a new file Sacramento.json to the California/ directory, Pachyderm processes the California/ datum only. If you set /*/* , Pachyderm processes each city as a single datum on a separate worker. For example, if you add the Sacramento.json file, Pachyderm processes the Sacramento.json file only. Glob patterns also let you take only a particular directory or subset of directories as an input instead of the whole repo. For example, you can set /California/* to process only the data for the state of California. Therefore, if you add a new city in the Colorado/ directory, Pachyderm ignore this change and does not start the pipeline. However, if you add Sacramento.json to the California/ directory, Pachyderm processes the California/ datum.","title":"Example of Defining Datums"},{"location":"concepts/pipeline-concepts/datum/glob-pattern/#test-a-glob-pattern","text":"You can use the pachctl glob file command to preview which filesystem objects a pipeline defines as datums. This command helps you to test various glob patterns before you use them in a pipeline. If you set the glob property to / , Pachyderm detects all top-level filesystem objects in the train repository as one datum: Example $ pachctl glob file train@master:/ NAME TYPE SIZE / dir 15 .11KiB If you set the glob property to / , Pachyderm detects each top-level filesystem object in the train repository as a separate datum: Example $ pachctl glob file train@master:/* NAME TYPE SIZE /IssueSummarization.py file 1 .224KiB /requirements.txt file 74B /seq2seq_utils.py file 13 .81KiB","title":"Test a Glob pattern"},{"location":"concepts/pipeline-concepts/datum/relationship-between-datums/","text":"Datum Processing \u00b6 This section helps you to understand the following concepts: Pachyderm job stages Multiple datums processing Incremental processing A datum is a Pachyderm abstraction that helps in optimizing pipeline processing. Because datums exist only as a pipeline processing property and are not filesystem objects, you can never list or copy a datum. Instead, a datum, as a representation of a unit of work, helps you to run your pipelines much faster by avoiding repeated processing of unchanged datums. For example, if you have multiple datums, and only one datum was modified, Pachyderm processes only that datum and skips processing other datums. This incremental behavior ensures efficient resource utilization. Each Pachyderm job can process multiple datums, which can consist of one or multiple files. While each input datum results in one output datum, the number of files in the output datum might differ from the number of files in the input datum. When you create a pipeline specification, one of the most important fields that you need to configure is pfs/ , or PFS input. The PFS input field is where you define a data source from which the pipeline pulls data for further processing. The glob parameter defines the number of datums in the pfs/ source repository. Thus, you can define everything in the source repository to be processed as a single datum or break it down to multiple datums. The way you break your source repository into datums directly affects incremental processing and your pipeline processing speed. You know your data better and can decide how to optimize your pipeline based on the repository structure and data generation workflows. For more information about glob patterns, see Glob Pattern . Disregarding of how many datums you define and how many filesystem objects a datum has, Pachyderm always matches the number of input datums with the number of output datums. For example, if you have three input datums in pfs/ , you will have three output datums in pfs/out . pfs/out is the output repository that Pachyderm creates automatically for each pipeline. You can add your changes in any order and submit them in one or multiple commits, the result of your pipeline processing remains the same. Another aspect of Pachyderm data processing is appending and overwriting files. By default, Pachyderm appends new data to the existing data. For example, if you have a file foo that is 100 KB in size in the repository A and add the same file foo to that repository again by using the pachctl put file command, Pachyderm does not overwrite that file but appends it to the file foo in the repo. Therefore, the size of the file foo doubles and becomes 200 KB. Pachyderm enables you to overwrite files as well by using the --overwrite flag. The order of processing is not guaranteed, and all datums are processed randomly. For more information, see File . When new data comes in, a Pachyderm pipeline automatically starts a new job. Each Pachyderm job consists of the following stages: Creation of input datums. In this stage, Pachyderm breaks input files into datums according to the glob pattern setting in the pipeline specification. Transformation. The pipeline uses your code to processes the datums. Creation of output datums. Pachyderm creates file or files from the processed data and combines them into output datums. Merge. Pachyderm combines all files with the same file path by appending, overwriting, or deleting them to create the final commit. If you think about this process in terms of filesystem objects and processing abstractions, the following transformation happens: input files = > input datums => output datums => output files This section provides examples that help you understand such fundamental Pachyderm concepts as the datum, incremental processing, and phases of data processing. Example 1: One file in the input datum, one file in the output datum \u00b6 The simplest example of datum processing is when you have one file in the input datum that results in one file in the output datum. In the diagram below, you can see three input datums, each of which includes one file, that result in three output datums. Whether you have submitted all these datums in a single or multiple commits, the final result remains the same\u2014three datums, each of which has one file. In the diagram below, you can see the following datums: datum 1 has one file and results in one file in one output datum. datum 2 has one file and results in one file in one output datum. datum 3 has one file and results in one file in one output datum. If you decide to overwrite a single line in the file in datum 3 and add datum 4 , Pachyderm sees the four datums and checks them for changes one-by-one. Pachyderm verifies that there are no changes in datum 1 and datum 2 and skips these datums. Pachyderm detects changes in the datum 3 and the --overwrite flag and replaces the datum 3 with the new datum 3' . When it detects datum 4 as a completely new datum, it processes the whole datum as new. Although only two datums were processed, the output commit of this change contains all four files. Example 2: One file in the input datum, multiple files in the output datum \u00b6 Some pipelines ingest one file in one input datum and create multiple files in the output datum. The files in the output datums might need to be appended or overwritten with other files to create the final commit. If you apply changes to that datum, Pachyderm does not detect which particular part of the datum has changed and processes the whole datum. In the diagram below, you have the following datums: datum 1 has one file and results in files 1 and 2 . datum 2 has one file and results in files 2 and 3 . datum 3 has one file and results in files 1 and 3 . Pachyderm processes all these datums independently, and in the end, it needs to create a commit by combining the results of processing these datums. A commit is a filesystem that has specific constraints, such as duplicate files with the same file path. Pachyderm merges results from different output datums with the same file path into single files. For example, datum 1 produces pfs/out/1 and datum 3 produces pfs/out/1 . Pachyderm merges these two files by appending them one to another without any particular order. Therefore, the file 1 in the final commit has parts from datum1 and datum2 . If you decide to create a new commit and overwrite the file in datum 2 , Pachyderm detects three datums. Because datum 1 and datum 3 are unchanged, it skips processing these datums. Then, Pachyderm detects that something has changed in datum 2 . Pachyderm is unaware of any details of the change. Therefore, it processes the whole datum 2 and outputs the files 1 , 3 , and 4 . Then, Pachyderm merges these datums to create the following final result: In the diagram above, Pachyderm appends the file 1 from the datum 2 to the file 1 in the final commit, deletes the file 2 from datum 2 , overwrites the old part from datum 2 in file 3 with a new version, and creates a new output file 4 . Similarly, if you have multiple files in your input datum, Pachyderm might write them into multiple files in output datums that are later merged into files with the same file path.","title":"Datum Processing"},{"location":"concepts/pipeline-concepts/datum/relationship-between-datums/#datum-processing","text":"This section helps you to understand the following concepts: Pachyderm job stages Multiple datums processing Incremental processing A datum is a Pachyderm abstraction that helps in optimizing pipeline processing. Because datums exist only as a pipeline processing property and are not filesystem objects, you can never list or copy a datum. Instead, a datum, as a representation of a unit of work, helps you to run your pipelines much faster by avoiding repeated processing of unchanged datums. For example, if you have multiple datums, and only one datum was modified, Pachyderm processes only that datum and skips processing other datums. This incremental behavior ensures efficient resource utilization. Each Pachyderm job can process multiple datums, which can consist of one or multiple files. While each input datum results in one output datum, the number of files in the output datum might differ from the number of files in the input datum. When you create a pipeline specification, one of the most important fields that you need to configure is pfs/ , or PFS input. The PFS input field is where you define a data source from which the pipeline pulls data for further processing. The glob parameter defines the number of datums in the pfs/ source repository. Thus, you can define everything in the source repository to be processed as a single datum or break it down to multiple datums. The way you break your source repository into datums directly affects incremental processing and your pipeline processing speed. You know your data better and can decide how to optimize your pipeline based on the repository structure and data generation workflows. For more information about glob patterns, see Glob Pattern . Disregarding of how many datums you define and how many filesystem objects a datum has, Pachyderm always matches the number of input datums with the number of output datums. For example, if you have three input datums in pfs/ , you will have three output datums in pfs/out . pfs/out is the output repository that Pachyderm creates automatically for each pipeline. You can add your changes in any order and submit them in one or multiple commits, the result of your pipeline processing remains the same. Another aspect of Pachyderm data processing is appending and overwriting files. By default, Pachyderm appends new data to the existing data. For example, if you have a file foo that is 100 KB in size in the repository A and add the same file foo to that repository again by using the pachctl put file command, Pachyderm does not overwrite that file but appends it to the file foo in the repo. Therefore, the size of the file foo doubles and becomes 200 KB. Pachyderm enables you to overwrite files as well by using the --overwrite flag. The order of processing is not guaranteed, and all datums are processed randomly. For more information, see File . When new data comes in, a Pachyderm pipeline automatically starts a new job. Each Pachyderm job consists of the following stages: Creation of input datums. In this stage, Pachyderm breaks input files into datums according to the glob pattern setting in the pipeline specification. Transformation. The pipeline uses your code to processes the datums. Creation of output datums. Pachyderm creates file or files from the processed data and combines them into output datums. Merge. Pachyderm combines all files with the same file path by appending, overwriting, or deleting them to create the final commit. If you think about this process in terms of filesystem objects and processing abstractions, the following transformation happens: input files = > input datums => output datums => output files This section provides examples that help you understand such fundamental Pachyderm concepts as the datum, incremental processing, and phases of data processing.","title":"Datum Processing"},{"location":"concepts/pipeline-concepts/datum/relationship-between-datums/#example-1-one-file-in-the-input-datum-one-file-in-the-output-datum","text":"The simplest example of datum processing is when you have one file in the input datum that results in one file in the output datum. In the diagram below, you can see three input datums, each of which includes one file, that result in three output datums. Whether you have submitted all these datums in a single or multiple commits, the final result remains the same\u2014three datums, each of which has one file. In the diagram below, you can see the following datums: datum 1 has one file and results in one file in one output datum. datum 2 has one file and results in one file in one output datum. datum 3 has one file and results in one file in one output datum. If you decide to overwrite a single line in the file in datum 3 and add datum 4 , Pachyderm sees the four datums and checks them for changes one-by-one. Pachyderm verifies that there are no changes in datum 1 and datum 2 and skips these datums. Pachyderm detects changes in the datum 3 and the --overwrite flag and replaces the datum 3 with the new datum 3' . When it detects datum 4 as a completely new datum, it processes the whole datum as new. Although only two datums were processed, the output commit of this change contains all four files.","title":"Example 1: One file in the input datum, one file in the output datum"},{"location":"concepts/pipeline-concepts/datum/relationship-between-datums/#example-2-one-file-in-the-input-datum-multiple-files-in-the-output-datum","text":"Some pipelines ingest one file in one input datum and create multiple files in the output datum. The files in the output datums might need to be appended or overwritten with other files to create the final commit. If you apply changes to that datum, Pachyderm does not detect which particular part of the datum has changed and processes the whole datum. In the diagram below, you have the following datums: datum 1 has one file and results in files 1 and 2 . datum 2 has one file and results in files 2 and 3 . datum 3 has one file and results in files 1 and 3 . Pachyderm processes all these datums independently, and in the end, it needs to create a commit by combining the results of processing these datums. A commit is a filesystem that has specific constraints, such as duplicate files with the same file path. Pachyderm merges results from different output datums with the same file path into single files. For example, datum 1 produces pfs/out/1 and datum 3 produces pfs/out/1 . Pachyderm merges these two files by appending them one to another without any particular order. Therefore, the file 1 in the final commit has parts from datum1 and datum2 . If you decide to create a new commit and overwrite the file in datum 2 , Pachyderm detects three datums. Because datum 1 and datum 3 are unchanged, it skips processing these datums. Then, Pachyderm detects that something has changed in datum 2 . Pachyderm is unaware of any details of the change. Therefore, it processes the whole datum 2 and outputs the files 1 , 3 , and 4 . Then, Pachyderm merges these datums to create the following final result: In the diagram above, Pachyderm appends the file 1 from the datum 2 to the file 1 in the final commit, deletes the file 2 from datum 2 , overwrites the old part from datum 2 in file 3 with a new version, and creates a new output file 4 . Similarly, if you have multiple files in your input datum, Pachyderm might write them into multiple files in output datums that are later merged into files with the same file path.","title":"Example 2: One file in the input datum, multiple files in the output datum"},{"location":"concepts/pipeline-concepts/pipeline/","text":"Pipeline \u00b6 A pipeline is a Pachyderm primitive that is responsible for reading data from a specified source, such as a Pachyderm repo, transforming it according to the pipeline configuration, and writing the result to an output repo. A pipeline subscribes to a branch in one or more input repositories. Every time the branch has a new commit, the pipeline executes a job that runs your code to completion and writes the results to a commit in the output repository. Every pipeline automatically creates an output repository by the same name as the pipeline. For example, a pipeline named model writes all results to the model output repo. In Pachyderm, a Pipeline is an individual execution step. You can chain multiple pipelines together to create a directed acyclic graph (DAG). A minimum pipeline specification must include the following parameters: name \u2014 The name of your data pipeline. Set a meaningful name for your pipeline, such as the name of the transformation that the pipeline performs. For example, split or edges . Pachyderm automatically creates an output repository with the same name. A pipeline name must be an alphanumeric string that is less than 63 characters long and can include dashes and underscores. No other special characters allowed. input \u2014 A location of the data that you want to process, such as a Pachyderm repository. You can specify multiple input repositories and set up the data to be combined in various ways. For more information, see Cross and Union . One very important property that is defined in the input field is the glob pattern that defines how Pachyderm breaks the data into individual processing units, called Datums. For more information, see Datum . transform \u2014 Specifies the code that you want to run against your data. The transform section must include an image field that defines the Docker image that you want to run, as well as a cmd field for the specific code within the container that you want to execute, such as a Python script. Example: { \"pipeline\" : { \"name\" : \"wordcount\" } , \"transform\" : { \"image\" : \"wordcount-image\" , \"cmd\" : [ \"python3\" , \"/my_python_code.py\" ] } , \"input\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } } } See also: Pipeline Specification","title":"Overview"},{"location":"concepts/pipeline-concepts/pipeline/#pipeline","text":"A pipeline is a Pachyderm primitive that is responsible for reading data from a specified source, such as a Pachyderm repo, transforming it according to the pipeline configuration, and writing the result to an output repo. A pipeline subscribes to a branch in one or more input repositories. Every time the branch has a new commit, the pipeline executes a job that runs your code to completion and writes the results to a commit in the output repository. Every pipeline automatically creates an output repository by the same name as the pipeline. For example, a pipeline named model writes all results to the model output repo. In Pachyderm, a Pipeline is an individual execution step. You can chain multiple pipelines together to create a directed acyclic graph (DAG). A minimum pipeline specification must include the following parameters: name \u2014 The name of your data pipeline. Set a meaningful name for your pipeline, such as the name of the transformation that the pipeline performs. For example, split or edges . Pachyderm automatically creates an output repository with the same name. A pipeline name must be an alphanumeric string that is less than 63 characters long and can include dashes and underscores. No other special characters allowed. input \u2014 A location of the data that you want to process, such as a Pachyderm repository. You can specify multiple input repositories and set up the data to be combined in various ways. For more information, see Cross and Union . One very important property that is defined in the input field is the glob pattern that defines how Pachyderm breaks the data into individual processing units, called Datums. For more information, see Datum . transform \u2014 Specifies the code that you want to run against your data. The transform section must include an image field that defines the Docker image that you want to run, as well as a cmd field for the specific code within the container that you want to execute, such as a Python script. Example: { \"pipeline\" : { \"name\" : \"wordcount\" } , \"transform\" : { \"image\" : \"wordcount-image\" , \"cmd\" : [ \"python3\" , \"/my_python_code.py\" ] } , \"input\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } } } See also: Pipeline Specification","title":"Pipeline"},{"location":"concepts/pipeline-concepts/pipeline/cron/","text":"Cron Pipeline \u00b6 Pachyderm triggers pipelines when new changes appear in the input repository. However, if you want to trigger a pipeline based on time instead of upon arrival of input data, you can schedule such pipelines to run periodically by using the Pachyderm built-in cron input type. A standard pipeline with a PFS input might not satisfy the requirements of the following tasks: Scrape websites Make API calls Query a database Retrieve a file from a location accessible through an S3 protocol or a File Transfer Protocol (FTP). A minimum cron pipeline must include the following parameters: Parameter Description \"name\" A descriptive name of the cron pipeline. \"spec\" An interval between scheduled cron jobs. You can specify any value that is formatted according to RFC 3339 . For example, if you set */10 * * * * , the pipeline runs every ten minutes. Example of a Cron Pipeline \u00b6 For example, you want to query a database every ten seconds and update your dataset with the new data every time the pipeline is triggered. The following pipeline extract illustrates how you can specify this configuration. Example \"input\" : { \"cron\" : { \"name\" : \"tick\" , \"spec\" : \"@every 10s\" } } When you create this pipeline, Pachyderm creates a new input data repository that corresponds to the cron input. Then, Pachyderm automatically commits a timestamp file to the cron input repository every ten seconds, which triggers the pipeline. The pipeline runs every ten seconds, queries the database and updates its output. By default, each cron trigger adds a new tick file to the cron input repository, accumulating more datums over time. This behavior works for some pipelines. For others you might want each tick file to overwrite the previous one. You can set the overwrite flag to true to overwrite the timestamp file on each tick. To learn more about overwriting commits in Pachyderm, see Datum processing . Example \"input\" : { \"cron\" : { \"name\" : \"tick\" , \"spec\" : \"@every 10s\" , \"overwrite\" : true } } See also: Periodic Ingress from MongoDB","title":"Cron"},{"location":"concepts/pipeline-concepts/pipeline/cron/#cron-pipeline","text":"Pachyderm triggers pipelines when new changes appear in the input repository. However, if you want to trigger a pipeline based on time instead of upon arrival of input data, you can schedule such pipelines to run periodically by using the Pachyderm built-in cron input type. A standard pipeline with a PFS input might not satisfy the requirements of the following tasks: Scrape websites Make API calls Query a database Retrieve a file from a location accessible through an S3 protocol or a File Transfer Protocol (FTP). A minimum cron pipeline must include the following parameters: Parameter Description \"name\" A descriptive name of the cron pipeline. \"spec\" An interval between scheduled cron jobs. You can specify any value that is formatted according to RFC 3339 . For example, if you set */10 * * * * , the pipeline runs every ten minutes.","title":"Cron Pipeline"},{"location":"concepts/pipeline-concepts/pipeline/cron/#example-of-a-cron-pipeline","text":"For example, you want to query a database every ten seconds and update your dataset with the new data every time the pipeline is triggered. The following pipeline extract illustrates how you can specify this configuration. Example \"input\" : { \"cron\" : { \"name\" : \"tick\" , \"spec\" : \"@every 10s\" } } When you create this pipeline, Pachyderm creates a new input data repository that corresponds to the cron input. Then, Pachyderm automatically commits a timestamp file to the cron input repository every ten seconds, which triggers the pipeline. The pipeline runs every ten seconds, queries the database and updates its output. By default, each cron trigger adds a new tick file to the cron input repository, accumulating more datums over time. This behavior works for some pipelines. For others you might want each tick file to overwrite the previous one. You can set the overwrite flag to true to overwrite the timestamp file on each tick. To learn more about overwriting commits in Pachyderm, see Datum processing . Example \"input\" : { \"cron\" : { \"name\" : \"tick\" , \"spec\" : \"@every 10s\" , \"overwrite\" : true } } See also: Periodic Ingress from MongoDB","title":"Example of a Cron Pipeline"},{"location":"concepts/pipeline-concepts/pipeline/service/","text":"Service \u00b6 Service is a special type of pipeline that does not process data but provides a capability to expose it to the outside world. For example, you can use a service to serve a machine learning model as an API that has the most up-to-date version of your data. The following pipeline spec extract is an example of how you can expose your Jupyter notebook as a service by adding a service field: { \"input\" : { \"pfs\" : { \"glob\" : \"/\" , \"repo\" : \"input\" } }, \"service\" : { \"external_port\" : 30888 , \"internal_port\" : 8888 }, \"transform\" : { \"cmd\" : [ \"start-notebook.sh\" ], \"image\" : \"jupyter/datascience-notebook\" } } The service section specifies the following parameters: Parameter Description \"internal_port\" The port that the code running inside the container binds to. \"external_port\" The port that is exposed outside of the container. You must set this value in the range of 30000 \u2014 32767 . You can access the service from any Kubernetes node through the following address: http://<kubernetes-host>:<external_port> . See Also Service","title":"Service"},{"location":"concepts/pipeline-concepts/pipeline/service/#service","text":"Service is a special type of pipeline that does not process data but provides a capability to expose it to the outside world. For example, you can use a service to serve a machine learning model as an API that has the most up-to-date version of your data. The following pipeline spec extract is an example of how you can expose your Jupyter notebook as a service by adding a service field: { \"input\" : { \"pfs\" : { \"glob\" : \"/\" , \"repo\" : \"input\" } }, \"service\" : { \"external_port\" : 30888 , \"internal_port\" : 8888 }, \"transform\" : { \"cmd\" : [ \"start-notebook.sh\" ], \"image\" : \"jupyter/datascience-notebook\" } } The service section specifies the following parameters: Parameter Description \"internal_port\" The port that the code running inside the container binds to. \"external_port\" The port that is exposed outside of the container. You must set this value in the range of 30000 \u2014 32767 . You can access the service from any Kubernetes node through the following address: http://<kubernetes-host>:<external_port> . See Also Service","title":"Service"},{"location":"concepts/pipeline-concepts/pipeline/spout/","text":"Spout \u00b6 A spout is a type of pipeline that ingests streaming data. Generally, you use spouts for situations when the interval between new data generation is large or sporadic, but the latency requirement to start the processing is short. Therefore, a regular pipeline with a cron input that polls for new data might not be an optimal solution. Examples of streaming data include a message queue, a database transactions log, event notifications, and others. In spouts, your code runs continuously and writes the results to the pipeline's output location, pfs/out . Every time you create a complete .tar archive, Pachyderm creates a new commit and triggers the pipeline to process it. One main difference from regular pipelines is that spouts ingest their data from outside sources. Therefore, they do not take an input. Another important aspect is that in spouts, pfs/out is a named pipe , or First in, First Out (FIFO), and is not a directory like in standard pipelines. Unlike the traditional pipe, that is familiar to most Linux users, a named pipe enables two system processes to access the pipe simultaneously and gives one of the processes read-only and the other process write-only access. Therefore, the two processes can simultaneously read and write to the same pipe. To create a spout pipeline, you need the following items: A source of streaming data A Docker container with your spout code that reads from the data source A spout pipeline specification file that uses the container Your spout code performs the following actions: Connects to the specified streaming data source. Opens /pfs/out as a named pipe. Reads the data from the streaming data source. Packages the data into a tar stream. Writes the tar stream into the pfs/out pipe. In case of transient errors produced by closing a previous write to the pipe, retries the write operation. Closes the tar stream and connection to /pfs/out , which produces the commit. A minimum spout specification must include the following parameters: Parameter Description name The name of your data pipeline and the output repository. You can set an arbitrary name that is meaningful to the code you want to run. transform Specifies the code that you want to run against your data, such as a Python or Go script. Also, it specifies a Docker image that you want to use to run that script. overwrite (Optional) Specifies whether to overwrite the existing content of the file from previous commits or previous calls to the put file command within this commit. The default value is false . The following text is an example of a minimum specification: Note: The env property is an optional argument. You can define your data stream source from within the container in which you run your script. For simplicity, in this example, env specifies the source of the Kafka host. { \"pipeline\": { \"name\": \"my-spout\" }, \"transform\": { \"cmd\": [ \"go\", \"run\", \"./main.go\" ], \"image\": \"myaccount/myimage:0.1\" }, \"env\": { \"HOST\": \"kafkahost\", \"TOPIC\": \"mytopic\", \"PORT\": \"9092\" }, \"spout\": { \"overwrite\": false } }","title":"Spout"},{"location":"concepts/pipeline-concepts/pipeline/spout/#spout","text":"A spout is a type of pipeline that ingests streaming data. Generally, you use spouts for situations when the interval between new data generation is large or sporadic, but the latency requirement to start the processing is short. Therefore, a regular pipeline with a cron input that polls for new data might not be an optimal solution. Examples of streaming data include a message queue, a database transactions log, event notifications, and others. In spouts, your code runs continuously and writes the results to the pipeline's output location, pfs/out . Every time you create a complete .tar archive, Pachyderm creates a new commit and triggers the pipeline to process it. One main difference from regular pipelines is that spouts ingest their data from outside sources. Therefore, they do not take an input. Another important aspect is that in spouts, pfs/out is a named pipe , or First in, First Out (FIFO), and is not a directory like in standard pipelines. Unlike the traditional pipe, that is familiar to most Linux users, a named pipe enables two system processes to access the pipe simultaneously and gives one of the processes read-only and the other process write-only access. Therefore, the two processes can simultaneously read and write to the same pipe. To create a spout pipeline, you need the following items: A source of streaming data A Docker container with your spout code that reads from the data source A spout pipeline specification file that uses the container Your spout code performs the following actions: Connects to the specified streaming data source. Opens /pfs/out as a named pipe. Reads the data from the streaming data source. Packages the data into a tar stream. Writes the tar stream into the pfs/out pipe. In case of transient errors produced by closing a previous write to the pipe, retries the write operation. Closes the tar stream and connection to /pfs/out , which produces the commit. A minimum spout specification must include the following parameters: Parameter Description name The name of your data pipeline and the output repository. You can set an arbitrary name that is meaningful to the code you want to run. transform Specifies the code that you want to run against your data, such as a Python or Go script. Also, it specifies a Docker image that you want to use to run that script. overwrite (Optional) Specifies whether to overwrite the existing content of the file from previous commits or previous calls to the put file command within this commit. The default value is false . The following text is an example of a minimum specification: Note: The env property is an optional argument. You can define your data stream source from within the container in which you run your script. For simplicity, in this example, env specifies the source of the Kafka host. { \"pipeline\": { \"name\": \"my-spout\" }, \"transform\": { \"cmd\": [ \"go\", \"run\", \"./main.go\" ], \"image\": \"myaccount/myimage:0.1\" }, \"env\": { \"HOST\": \"kafkahost\", \"TOPIC\": \"mytopic\", \"PORT\": \"9092\" }, \"spout\": { \"overwrite\": false } }","title":"Spout"},{"location":"contributing/coding-conventions/","text":"Coding Conventions \u00b6 All code in this repo should be written in Go, Shell or Make. Exceptions are made for things under examples because we want to be able to give people examples of using the product in other languages. And for things like doc/conf.py which configures an outside tool we want to use for docs. However in choosing outside tooling we prefer tools that we can interface with entirely using Go. Go's new enough that it's not always possible to find such a tool so we expect to make compromises on this. In general you should operate under the assumption that code written in Go, Shell or Make is accessible to all developers of the project and code written in other languages is accessible to only a subset and thus represents a higher liability. Shell \u00b6 https://google.github.io/styleguide/shell.xml Scripts should work on macOS as well as Linux. Go \u00b6 Go has pretty unified conventions for style, we vastly prefer embracing these standards to developing our own. Stylistic Conventions \u00b6 We have several Go checks that run as part of CI, those should pass. You can run them with make pretest and make lint . Go Code Review Comments Effective Go Command-line flags should use dashes, not underscores. Naming Please consider package name when selecting an interface name, and avoid redundancy. e.g.: storage.Interface is better than storage.StorageInterface . Do not use uppercase characters, underscores, or dashes in package names. Unless there's a good reason, the package foo line should match the name of the directory in which the .go file exists. Importers can use a different name if they need to disambiguate. Locks should be called lock and should never be embedded (always lock sync.Mutex ). When multiple locks are present, give each lock a distinct name following Go conventions - stateLock , mapLock etc. Testing Conventions \u00b6 All new packages and most new significant functionality must come with test coverage Avoid waiting for asynchronous things to happen (e.g. waiting 10 seconds and assuming that a service will be afterward). Instead you try, wait, retry, etc. with a limited number of tries. If possible use a method of waiting directly (e.g. 'flush commit' is much better than repeatedly trying to read from a commit). Go Modules/Third-Party Code \u00b6 Go dependencies are managed with go modules (as of 07/11/2019). To add a new package or update a package. Do: go get foo or for a more specific version go get foo@v1.2.3 , go get foo@master , go get foo@e3702bed2 import foo package to you go code as needed. Run go mod vendor Note: Go modules requires you clone the repo outside of the $GOPATH or you must pass the GO111MODULE=on flag to any go commands. See wiki page on activating module support See The official go modules wiki for more info. Docs \u00b6 PRs for code must include documentation updates that reflect the changes that the code introduces. When writing documentation, follow the Style Guide conventions. PRs that have only documentation changes, such as typos, is a great place to start and we welcome your help! For most documentation PRs, you need to make assets and push the new assets.go file as well.","title":"Coding Conventions"},{"location":"contributing/coding-conventions/#coding-conventions","text":"All code in this repo should be written in Go, Shell or Make. Exceptions are made for things under examples because we want to be able to give people examples of using the product in other languages. And for things like doc/conf.py which configures an outside tool we want to use for docs. However in choosing outside tooling we prefer tools that we can interface with entirely using Go. Go's new enough that it's not always possible to find such a tool so we expect to make compromises on this. In general you should operate under the assumption that code written in Go, Shell or Make is accessible to all developers of the project and code written in other languages is accessible to only a subset and thus represents a higher liability.","title":"Coding Conventions"},{"location":"contributing/coding-conventions/#shell","text":"https://google.github.io/styleguide/shell.xml Scripts should work on macOS as well as Linux.","title":"Shell"},{"location":"contributing/coding-conventions/#go","text":"Go has pretty unified conventions for style, we vastly prefer embracing these standards to developing our own.","title":"Go"},{"location":"contributing/coding-conventions/#stylistic-conventions","text":"We have several Go checks that run as part of CI, those should pass. You can run them with make pretest and make lint . Go Code Review Comments Effective Go Command-line flags should use dashes, not underscores. Naming Please consider package name when selecting an interface name, and avoid redundancy. e.g.: storage.Interface is better than storage.StorageInterface . Do not use uppercase characters, underscores, or dashes in package names. Unless there's a good reason, the package foo line should match the name of the directory in which the .go file exists. Importers can use a different name if they need to disambiguate. Locks should be called lock and should never be embedded (always lock sync.Mutex ). When multiple locks are present, give each lock a distinct name following Go conventions - stateLock , mapLock etc.","title":"Stylistic Conventions"},{"location":"contributing/coding-conventions/#testing-conventions","text":"All new packages and most new significant functionality must come with test coverage Avoid waiting for asynchronous things to happen (e.g. waiting 10 seconds and assuming that a service will be afterward). Instead you try, wait, retry, etc. with a limited number of tries. If possible use a method of waiting directly (e.g. 'flush commit' is much better than repeatedly trying to read from a commit).","title":"Testing Conventions"},{"location":"contributing/coding-conventions/#go-modulesthird-party-code","text":"Go dependencies are managed with go modules (as of 07/11/2019). To add a new package or update a package. Do: go get foo or for a more specific version go get foo@v1.2.3 , go get foo@master , go get foo@e3702bed2 import foo package to you go code as needed. Run go mod vendor Note: Go modules requires you clone the repo outside of the $GOPATH or you must pass the GO111MODULE=on flag to any go commands. See wiki page on activating module support See The official go modules wiki for more info.","title":"Go Modules/Third-Party Code"},{"location":"contributing/coding-conventions/#docs","text":"PRs for code must include documentation updates that reflect the changes that the code introduces. When writing documentation, follow the Style Guide conventions. PRs that have only documentation changes, such as typos, is a great place to start and we welcome your help! For most documentation PRs, you need to make assets and push the new assets.go file as well.","title":"Docs"},{"location":"contributing/docs-style-guide/","text":"Pachyderm Technical Documentation Style Guide \u00b6 This document provides main guidelines for creating technical content that describes Pachyderm concepts and operations. This style guide is based on Google Developer Documentation Style Guide and serves as a quick reference for everyone who wants to contribute to the Pachyderm documentation. For a more detailed overview, see the Google Developer Documentation Style Guide . Overview \u00b6 We welcome all contributions to the Pachyderm technical documentation and are happy to help incorporate your content into our docs! We hope that this document will assits in answering some of your questions about our contributing guidelines. Writing Style \u00b6 Friendly but not overly colloquial. Avoid jargon, idioms, and references to pop culture. Use shorter sentences and words over longer alternatives. Things to avoid: \"Please\" and \"thank you\". Exclamation marks. Announcement of features that have not yet been developed. All caps to emphasize the importance. Parenthesis as much as possible. Use commas instead. Do not use Use We'll walk you through the installation of the product X. It might be a bit difficult, but don't worry, we are here to help. This guide walks you through the process of installation of the product X. Write in the present tense and in second person \u00b6 Say \"you\" instead of \"we\" and use the present tense where possible. Only use the future tense when talking about the events that will not happen immediately but sometime in the future. The future tense introduces uncertainty about when an action takes places. Therefore, in most cases, use the present tense. Do not use Use We are going to create a new configuration file that will describe our deployment. To create a new configuration file that describes our deployment, complete the following steps. Write for an international audience \u00b6 Avoid idioms and jargon and write in simple American English. The content that you are writing might later be translated into other foreign languages. Translating simple short phrases is much easier than long sentences. Use consistent terminology and avoid misplacing modifiers. Spell out abbreviations on the first occurrence. Do not use Use After completing these steps, you are off to the races! After you complete these steps, you can start using Pachyderm. Write in Active Voice \u00b6 Sentences written in active voice are easier for the reader to understand. A well-written text has about 95% of sentences written in active voice. Use passive voice only when the performer of the action is unknown or to avoid blaming the user for an error. Do not use Use This behavior means that transform.err_cmd can be used to ignore failed datums. You can use transform.err_cmd to ignore failed datums. Put the Condition Before the Steps \u00b6 If your sentence has a condition, start the sentence with the conditional clause and add the descriptive instructions after the clause. Do not use Use See the Spark documentation for more information. For more information, see the Spark documentation. Use Numbered Lists for a Sequence of Steps \u00b6 If the user needs to follow a set of instructions, organize them in a numbered list rather than in a bulleted list. Options can be described in a bulleted list. An exception to this rule is when you have just one step. Do not use Use * Create a configuration and run the following command. 1. Create a configuration file. 2. Run the following command: Break Your Content Into Smaller Chunks \u00b6 Users do not read the whole body of the text. Instead, they skip and scan through looking for the text structures that stand out, such as headings, numbered and bulleted lists, tables, and so on. Try to structure your content so that it is easy to scan through by adding more titles, organizing instructions in sequences of steps, and adding tables and lists for properties and descriptions. Avoid Ending a Sentence with a Preposition Phrasal verbs are a little bit less formal than single-word verbs. If possible, replace a phrasal word with a single-word verb equivalent and if you have to use a phrasal word, avoid finishing the sentence with a preposition. Do not use Use The put file API includes an option for splitting up the file into separate datums automatically. The put file API includes an option for splitting the file into separate datums automatically. Use meaningful links \u00b6 Link text should mean something to the users when they read it. Phrases like Click here and Read more do not provide useful information. They might be good for call-to-action (CA) buttons on the marketing part of the website, but in technical content they introduce uncertainty and confusion. Furthermore, if a user generates a list of links or uses a speech recognition technology to navigate through the page, they use keywords and phrases, such as \"Click \". Generic links are not helpful for them. Also, use a standard phrase For more information, see to introduce a link. Do not use Use More information about getting your FREE trial token and activating the dashboard can be found here . For more information, see Activate your token by using the dashboard . Markdown \u00b6 The Pachyderm documentation uses Python Markdown, and many PyMdown Extensions are supported. See the mkdocs.yaml file for the list of supported extensions. In general, follow these guidelines: Do not use the admonitions in numbered lists because they break the order and Markdown does not support starting lists from an arbitrary number. Enclose code blocks in \"```\" and specify the correct highlighting. While PyMdown Extensions provide advanced UI features, use them sparingly because not all browsers fully support all of them. I hope you'll have fun with Python Markdown!","title":"Documentation Style Guide"},{"location":"contributing/docs-style-guide/#pachyderm-technical-documentation-style-guide","text":"This document provides main guidelines for creating technical content that describes Pachyderm concepts and operations. This style guide is based on Google Developer Documentation Style Guide and serves as a quick reference for everyone who wants to contribute to the Pachyderm documentation. For a more detailed overview, see the Google Developer Documentation Style Guide .","title":"Pachyderm Technical Documentation Style Guide"},{"location":"contributing/docs-style-guide/#overview","text":"We welcome all contributions to the Pachyderm technical documentation and are happy to help incorporate your content into our docs! We hope that this document will assits in answering some of your questions about our contributing guidelines.","title":"Overview"},{"location":"contributing/docs-style-guide/#writing-style","text":"Friendly but not overly colloquial. Avoid jargon, idioms, and references to pop culture. Use shorter sentences and words over longer alternatives. Things to avoid: \"Please\" and \"thank you\". Exclamation marks. Announcement of features that have not yet been developed. All caps to emphasize the importance. Parenthesis as much as possible. Use commas instead. Do not use Use We'll walk you through the installation of the product X. It might be a bit difficult, but don't worry, we are here to help. This guide walks you through the process of installation of the product X.","title":"Writing Style"},{"location":"contributing/docs-style-guide/#write-in-the-present-tense-and-in-second-person","text":"Say \"you\" instead of \"we\" and use the present tense where possible. Only use the future tense when talking about the events that will not happen immediately but sometime in the future. The future tense introduces uncertainty about when an action takes places. Therefore, in most cases, use the present tense. Do not use Use We are going to create a new configuration file that will describe our deployment. To create a new configuration file that describes our deployment, complete the following steps.","title":"Write in the present tense and in second person"},{"location":"contributing/docs-style-guide/#write-for-an-international-audience","text":"Avoid idioms and jargon and write in simple American English. The content that you are writing might later be translated into other foreign languages. Translating simple short phrases is much easier than long sentences. Use consistent terminology and avoid misplacing modifiers. Spell out abbreviations on the first occurrence. Do not use Use After completing these steps, you are off to the races! After you complete these steps, you can start using Pachyderm.","title":"Write for an international audience"},{"location":"contributing/docs-style-guide/#write-in-active-voice","text":"Sentences written in active voice are easier for the reader to understand. A well-written text has about 95% of sentences written in active voice. Use passive voice only when the performer of the action is unknown or to avoid blaming the user for an error. Do not use Use This behavior means that transform.err_cmd can be used to ignore failed datums. You can use transform.err_cmd to ignore failed datums.","title":"Write in Active Voice"},{"location":"contributing/docs-style-guide/#put-the-condition-before-the-steps","text":"If your sentence has a condition, start the sentence with the conditional clause and add the descriptive instructions after the clause. Do not use Use See the Spark documentation for more information. For more information, see the Spark documentation.","title":"Put the Condition Before the Steps"},{"location":"contributing/docs-style-guide/#use-numbered-lists-for-a-sequence-of-steps","text":"If the user needs to follow a set of instructions, organize them in a numbered list rather than in a bulleted list. Options can be described in a bulleted list. An exception to this rule is when you have just one step. Do not use Use * Create a configuration and run the following command. 1. Create a configuration file. 2. Run the following command:","title":"Use Numbered Lists for a Sequence of Steps"},{"location":"contributing/docs-style-guide/#break-your-content-into-smaller-chunks","text":"Users do not read the whole body of the text. Instead, they skip and scan through looking for the text structures that stand out, such as headings, numbered and bulleted lists, tables, and so on. Try to structure your content so that it is easy to scan through by adding more titles, organizing instructions in sequences of steps, and adding tables and lists for properties and descriptions. Avoid Ending a Sentence with a Preposition Phrasal verbs are a little bit less formal than single-word verbs. If possible, replace a phrasal word with a single-word verb equivalent and if you have to use a phrasal word, avoid finishing the sentence with a preposition. Do not use Use The put file API includes an option for splitting up the file into separate datums automatically. The put file API includes an option for splitting the file into separate datums automatically.","title":"Break Your Content Into Smaller Chunks"},{"location":"contributing/docs-style-guide/#use-meaningful-links","text":"Link text should mean something to the users when they read it. Phrases like Click here and Read more do not provide useful information. They might be good for call-to-action (CA) buttons on the marketing part of the website, but in technical content they introduce uncertainty and confusion. Furthermore, if a user generates a list of links or uses a speech recognition technology to navigate through the page, they use keywords and phrases, such as \"Click \". Generic links are not helpful for them. Also, use a standard phrase For more information, see to introduce a link. Do not use Use More information about getting your FREE trial token and activating the dashboard can be found here . For more information, see Activate your token by using the dashboard .","title":"Use meaningful links"},{"location":"contributing/docs-style-guide/#markdown","text":"The Pachyderm documentation uses Python Markdown, and many PyMdown Extensions are supported. See the mkdocs.yaml file for the list of supported extensions. In general, follow these guidelines: Do not use the admonitions in numbered lists because they break the order and Markdown does not support starting lists from an arbitrary number. Enclose code blocks in \"```\" and specify the correct highlighting. While PyMdown Extensions provide advanced UI features, use them sparingly because not all browsers fully support all of them. I hope you'll have fun with Python Markdown!","title":"Markdown"},{"location":"contributing/gcloud-setup/","text":"Gcloud cluster setup \u00b6 In order to develop pachyderm against a gcloud-deployed cluster, follow these instructions. First steps \u00b6 First follow the general setup instructions . gcloud \u00b6 Download Page Setup Google Cloud Platform via the web login with your Gmail or G Suite account click the silhouette in the upper right to make sure you're logged in with the right account get your owner/admin to setup a project for you (e.g. YOURNAME-dev) then they need to go into the project > settings > permissions and add you hint to owner/admin: its the permissions button in one of the left hand popin menus (GKE UI can be confusing) you should have an email invite to accept click 'use google APIS' (or something along the lines of enable/manage APIs) click through to google compute engine API and enable it or click the 'get started' button to make it provision Then, locally, run the following commands one at a time: gcloud auth login gcloud init # This should have you logged in / w gcloud # The following will only work after your GKE owner/admin adds you to the right project on gcloud: gcloud config set project YOURNAME-dev gcloud compute instances list # Now create instance using our bash helper create_docker_machine # And attach to the right docker daemon eval \"$(docker-machine env dev)\" Setup a project on gcloud go to console.cloud.google.com/start make sure you're logged in w your gmail account create project 'YOURNAME-dev' kubectl \u00b6 Now that you have gcloud, just do: gcloud components update kubectl # Now you need to start port forwarding to allow kubectl client talk to the kubernetes service on GCE portfowarding # To see this alias, look at the bash_helpers kubectl version # should report a client version, not a server version yet make launch-kube # to deploy kubernetes service kubectl version # now you should see a client and server version docker ps # you should see a few processes Pachyderm cluster deployment \u00b6 make launch","title":"Gcloud Cluster Setup"},{"location":"contributing/gcloud-setup/#gcloud-cluster-setup","text":"In order to develop pachyderm against a gcloud-deployed cluster, follow these instructions.","title":"Gcloud cluster setup"},{"location":"contributing/gcloud-setup/#first-steps","text":"First follow the general setup instructions .","title":"First steps"},{"location":"contributing/gcloud-setup/#gcloud","text":"Download Page Setup Google Cloud Platform via the web login with your Gmail or G Suite account click the silhouette in the upper right to make sure you're logged in with the right account get your owner/admin to setup a project for you (e.g. YOURNAME-dev) then they need to go into the project > settings > permissions and add you hint to owner/admin: its the permissions button in one of the left hand popin menus (GKE UI can be confusing) you should have an email invite to accept click 'use google APIS' (or something along the lines of enable/manage APIs) click through to google compute engine API and enable it or click the 'get started' button to make it provision Then, locally, run the following commands one at a time: gcloud auth login gcloud init # This should have you logged in / w gcloud # The following will only work after your GKE owner/admin adds you to the right project on gcloud: gcloud config set project YOURNAME-dev gcloud compute instances list # Now create instance using our bash helper create_docker_machine # And attach to the right docker daemon eval \"$(docker-machine env dev)\" Setup a project on gcloud go to console.cloud.google.com/start make sure you're logged in w your gmail account create project 'YOURNAME-dev'","title":"gcloud"},{"location":"contributing/gcloud-setup/#kubectl","text":"Now that you have gcloud, just do: gcloud components update kubectl # Now you need to start port forwarding to allow kubectl client talk to the kubernetes service on GCE portfowarding # To see this alias, look at the bash_helpers kubectl version # should report a client version, not a server version yet make launch-kube # to deploy kubernetes service kubectl version # now you should see a client and server version docker ps # you should see a few processes","title":"kubectl"},{"location":"contributing/gcloud-setup/#pachyderm-cluster-deployment","text":"make launch","title":"Pachyderm cluster deployment"},{"location":"contributing/repo-layout/","text":"Repo layout \u00b6 Following is a layout of the various directories that make up the pachyderm repo, and their purpose. build debian doc - documentation used on readthedocs \u251c\u2500\u2500 pachctl - cobra auto-generated docs on command-line usage etc - everything else \u251c\u2500\u2500 build - scripts for building releases \u251c\u2500\u2500 compatibility - contains mappings of pachyderm versions to the dash versions they're compatible with \u251c\u2500\u2500 compile - scripts to facilitate compiling and building docker images \u251c\u2500\u2500 contributing - contains helper scripts/assets for contributors \u251c\u2500\u2500 deploy - scripts/assets for pachyderm deployments \u2502 \u251c\u2500\u2500 cloudfront \u2502 \u251c\u2500\u2500 gpu - scripts to help enable GPU resources on k8s/pachyderm \u2502 \u2514\u2500\u2500 tracing - k8s manifests for enabling Jaeger tracing of pachyderm \u251c\u2500\u2500 initdev - scripts to stand up a vagrant environment for pachyderm \u251c\u2500\u2500 kube - internal scripts for working with k8s \u251c\u2500\u2500 kubernetes-kafka \u251c\u2500\u2500 kubernetes-prometheus \u251c\u2500\u2500 netcat \u251c\u2500\u2500 plugin \u2502 \u251c\u2500\u2500 logging \u2502 \u2514\u2500\u2500 monitoring \u251c\u2500\u2500 proto - scripts for compiling protobufs \u251c\u2500\u2500 testing - scripts/assets used for testing \u2502 \u251c\u2500\u2500 artifacts - static assets used in testing/mocking \u2502 \u251c\u2500\u2500 deploy - scripts to assist in deploying pachyderm on various cloud providers \u2502 \u251c\u2500\u2500 entrypoint \u2502 \u251c\u2500\u2500 migration - sample data used in testing pachyderm migrations \u2502 \u251c\u2500\u2500 s3gateway - scripts for running conformance tests on the s3gateway \u2502 \u2514\u2500\u2500 vault-s3-client \u251c\u2500\u2500 user-job \u2514\u2500\u2500 worker examples - example projects; see readme for details of each one src - source code \u251c\u2500\u2500 client - contains protobufs and the source code for pachyderm's go client \u2502 \u251c\u2500\u2500 admin - admin-related functionality \u2502 \u2502 \u2514\u2500\u2500 1_7 - old, v1.7-compatible protobufs \u2502 \u251c\u2500\u2500 auth - auth-related functionality \u2502 \u251c\u2500\u2500 debug - debug-related functionality \u2502 \u251c\u2500\u2500 deploy - deployment-related functionality \u2502 \u251c\u2500\u2500 enterprise - enterprise-related functionality \u2502 \u251c\u2500\u2500 health - health check-related functionality \u2502 \u251c\u2500\u2500 limit - limit-related functionality \u2502 \u251c\u2500\u2500 pfs - PFS-related functionality \u2502 \u251c\u2500\u2500 pkg - utility packages \u2502 \u2502 \u251c\u2500\u2500 config - pachyderm config file reading/writing \u2502 \u2502 \u251c\u2500\u2500 discovery \u2502 \u2502 \u251c\u2500\u2500 grpcutil - utilities for working with gRPC clients/servers \u2502 \u2502 \u251c\u2500\u2500 pbutil - utilities for working with protobufs \u2502 \u2502 \u251c\u2500\u2500 require - utilities for making unit tests terser \u2502 \u2502 \u251c\u2500\u2500 shard \u2502 \u2502 \u2514\u2500\u2500 tracing - facilitates pachyderm cluster Jaeger tracing \u2502 \u251c\u2500\u2500 pps - PPS-related functionality \u2502 \u2514\u2500\u2500 version - version check-related functionality \u251c\u2500\u2500 plugin \u2502 \u2514\u2500\u2500 vault \u2502 \u251c\u2500\u2500 etc \u2502 \u251c\u2500\u2500 pachyderm \u2502 \u251c\u2500\u2500 pachyderm-plugin \u2502 \u2514\u2500\u2500 vendor - vendored libraries for the vault plugin \u251c\u2500\u2500 server - contains server-side logic and CLI \u2502 \u251c\u2500\u2500 admin - cluster admin functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - cluster admin CLI \u2502 \u2502 \u2514\u2500\u2500 server - cluster admin server \u2502 \u251c\u2500\u2500 auth - auth functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - auth CLI \u2502 \u2502 \u251c\u2500\u2500 server - auth server \u2502 \u2502 \u2514\u2500\u2500 testing - a mock auth server used for testing \u2502 \u251c\u2500\u2500 cmd - contains the various pachyderm entrypoints \u2502 \u2502 \u251c\u2500\u2500 pachctl - the CLI entrypoint \u2502 \u2502 \u251c\u2500\u2500 pachctl-doc - helps generate docs for the CLI \u2502 \u2502 \u251c\u2500\u2500 pachd - the server entrypoint \u2502 \u2502 \u2514\u2500\u2500 worker - the worker entrypoint \u2502 \u251c\u2500\u2500 debug - debug functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - debug CLI \u2502 \u2502 \u2514\u2500\u2500 server - debug server \u2502 \u251c\u2500\u2500 deploy - storage secret deployment server \u2502 \u251c\u2500\u2500 enterprise - enterprise functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - enterprise CLI \u2502 \u2502 \u2514\u2500\u2500 server - enterprise server \u2502 \u251c\u2500\u2500 health - health check server \u2502 \u251c\u2500\u2500 http - PFS-over-HTTP server, used by the dash to serve PFS content \u2502 \u251c\u2500\u2500 pfs - PFS functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - PFS CLI \u2502 \u2502 \u251c\u2500\u2500 fuse - support mounting PFS repos via FUSE \u2502 \u2502 \u251c\u2500\u2500 pretty - pretty-printing of PFS metadata in the CLI \u2502 \u2502 \u251c\u2500\u2500 s3 - the s3gateway, an s3-like HTTP API for serving PFS content \u2502 \u2502 \u2514\u2500\u2500 server - PFS server \u2502 \u251c\u2500\u2500 pkg - utility packages \u2502 \u2502 \u251c\u2500\u2500 ancestry - parses git ancestry reference strings \u2502 \u2502 \u251c\u2500\u2500 backoff - backoff algorithms for retrying operations \u2502 \u2502 \u251c\u2500\u2500 cache - a gRPC server for serving cached content \u2502 \u2502 \u251c\u2500\u2500 cert - functionality for generating x509 certificates \u2502 \u2502 \u251c\u2500\u2500 cmdutil - functionality for helping creating CLIs \u2502 \u2502 \u251c\u2500\u2500 collection - etcd collection management \u2502 \u2502 \u251c\u2500\u2500 dag - a simple in-memory directed acyclic graph data structure \u2502 \u2502 \u251c\u2500\u2500 deploy - functionality for deploying pachyderm \u2502 \u2502 \u2502 \u251c\u2500\u2500 assets - generates k8s manifests and other assets used in deployment \u2502 \u2502 \u2502 \u251c\u2500\u2500 cmds - deployment CLI \u2502 \u2502 \u2502 \u2514\u2500\u2500 images - handling of docker images \u2502 \u2502 \u251c\u2500\u2500 dlock - distributed lock on etcd \u2502 \u2502 \u251c\u2500\u2500 errutil - utility functions for error handling \u2502 \u2502 \u251c\u2500\u2500 exec - utilities for running external commands \u2502 \u2502 \u251c\u2500\u2500 hashtree - a Merkle tree library \u2502 \u2502 \u251c\u2500\u2500 lease - utility for managing resources with expirable leases \u2502 \u2502 \u251c\u2500\u2500 localcache - a concurrency-safe local disk cache \u2502 \u2502 \u251c\u2500\u2500 log - logging utilities \u2502 \u2502 \u251c\u2500\u2500 metrics - cluster metrics service using segment.io \u2502 \u2502 \u251c\u2500\u2500 migration \u2502 \u2502 \u251c\u2500\u2500 netutil - networking utilities \u2502 \u2502 \u251c\u2500\u2500 obj - tools for working with various object stores (e.g. S3) \u2502 \u2502 \u251c\u2500\u2500 pfsdb - the etcd database schema that PFS uses \u2502 \u2502 \u251c\u2500\u2500 pool - gRPC connection pooling \u2502 \u2502 \u251c\u2500\u2500 ppsconsts - PPS-related constants \u2502 \u2502 \u251c\u2500\u2500 ppsdb - the etcd database schema that PPS uses \u2502 \u2502 \u251c\u2500\u2500 ppsutil - PPS-related utility functions \u2502 \u2502 \u251c\u2500\u2500 pretty - function for pretty printing values \u2502 \u2502 \u251c\u2500\u2500 serviceenv - management of connections to pach services \u2502 \u2502 \u251c\u2500\u2500 sql - tools for working with postgres database dumps \u2502 \u2502 \u251c\u2500\u2500 sync - tools for syncing PFS content \u2502 \u2502 \u251c\u2500\u2500 tabwriter - tool for writing tab-delimited content \u2502 \u2502 \u251c\u2500\u2500 testutil - test-related utilities \u2502 \u2502 \u251c\u2500\u2500 uuid - UUID generation \u2502 \u2502 \u251c\u2500\u2500 watch - tool for watching etcd databases for changes \u2502 \u2502 \u2514\u2500\u2500 workload \u2502 \u251c\u2500\u2500 pps - PPS functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - - PPS CLI \u2502 \u2502 \u251c\u2500\u2500 example - example PPS requests \u2502 \u2502 \u251c\u2500\u2500 pretty - pretty printing of PPS output to the CLI \u2502 \u2502 \u2514\u2500\u2500 server - PPS server \u2502 \u2502 \u2514\u2500\u2500 githook - support for github PPS sources \u2502 \u251c\u2500\u2500 vendor - vendored packages \u2502 \u2514\u2500\u2500 worker - pachd master and sidecar \u2514\u2500\u2500 testing - testing tools \u251c\u2500\u2500 loadtest - load tests for pachyderm \u2502 \u2514\u2500\u2500 split - stress tests of PFS merge functionality \u251c\u2500\u2500 match - a grep-like tool used in testing \u251c\u2500\u2500 saml-idp \u2514\u2500\u2500 vendor - vendored packages","title":"Repo Layout"},{"location":"contributing/repo-layout/#repo-layout","text":"Following is a layout of the various directories that make up the pachyderm repo, and their purpose. build debian doc - documentation used on readthedocs \u251c\u2500\u2500 pachctl - cobra auto-generated docs on command-line usage etc - everything else \u251c\u2500\u2500 build - scripts for building releases \u251c\u2500\u2500 compatibility - contains mappings of pachyderm versions to the dash versions they're compatible with \u251c\u2500\u2500 compile - scripts to facilitate compiling and building docker images \u251c\u2500\u2500 contributing - contains helper scripts/assets for contributors \u251c\u2500\u2500 deploy - scripts/assets for pachyderm deployments \u2502 \u251c\u2500\u2500 cloudfront \u2502 \u251c\u2500\u2500 gpu - scripts to help enable GPU resources on k8s/pachyderm \u2502 \u2514\u2500\u2500 tracing - k8s manifests for enabling Jaeger tracing of pachyderm \u251c\u2500\u2500 initdev - scripts to stand up a vagrant environment for pachyderm \u251c\u2500\u2500 kube - internal scripts for working with k8s \u251c\u2500\u2500 kubernetes-kafka \u251c\u2500\u2500 kubernetes-prometheus \u251c\u2500\u2500 netcat \u251c\u2500\u2500 plugin \u2502 \u251c\u2500\u2500 logging \u2502 \u2514\u2500\u2500 monitoring \u251c\u2500\u2500 proto - scripts for compiling protobufs \u251c\u2500\u2500 testing - scripts/assets used for testing \u2502 \u251c\u2500\u2500 artifacts - static assets used in testing/mocking \u2502 \u251c\u2500\u2500 deploy - scripts to assist in deploying pachyderm on various cloud providers \u2502 \u251c\u2500\u2500 entrypoint \u2502 \u251c\u2500\u2500 migration - sample data used in testing pachyderm migrations \u2502 \u251c\u2500\u2500 s3gateway - scripts for running conformance tests on the s3gateway \u2502 \u2514\u2500\u2500 vault-s3-client \u251c\u2500\u2500 user-job \u2514\u2500\u2500 worker examples - example projects; see readme for details of each one src - source code \u251c\u2500\u2500 client - contains protobufs and the source code for pachyderm's go client \u2502 \u251c\u2500\u2500 admin - admin-related functionality \u2502 \u2502 \u2514\u2500\u2500 1_7 - old, v1.7-compatible protobufs \u2502 \u251c\u2500\u2500 auth - auth-related functionality \u2502 \u251c\u2500\u2500 debug - debug-related functionality \u2502 \u251c\u2500\u2500 deploy - deployment-related functionality \u2502 \u251c\u2500\u2500 enterprise - enterprise-related functionality \u2502 \u251c\u2500\u2500 health - health check-related functionality \u2502 \u251c\u2500\u2500 limit - limit-related functionality \u2502 \u251c\u2500\u2500 pfs - PFS-related functionality \u2502 \u251c\u2500\u2500 pkg - utility packages \u2502 \u2502 \u251c\u2500\u2500 config - pachyderm config file reading/writing \u2502 \u2502 \u251c\u2500\u2500 discovery \u2502 \u2502 \u251c\u2500\u2500 grpcutil - utilities for working with gRPC clients/servers \u2502 \u2502 \u251c\u2500\u2500 pbutil - utilities for working with protobufs \u2502 \u2502 \u251c\u2500\u2500 require - utilities for making unit tests terser \u2502 \u2502 \u251c\u2500\u2500 shard \u2502 \u2502 \u2514\u2500\u2500 tracing - facilitates pachyderm cluster Jaeger tracing \u2502 \u251c\u2500\u2500 pps - PPS-related functionality \u2502 \u2514\u2500\u2500 version - version check-related functionality \u251c\u2500\u2500 plugin \u2502 \u2514\u2500\u2500 vault \u2502 \u251c\u2500\u2500 etc \u2502 \u251c\u2500\u2500 pachyderm \u2502 \u251c\u2500\u2500 pachyderm-plugin \u2502 \u2514\u2500\u2500 vendor - vendored libraries for the vault plugin \u251c\u2500\u2500 server - contains server-side logic and CLI \u2502 \u251c\u2500\u2500 admin - cluster admin functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - cluster admin CLI \u2502 \u2502 \u2514\u2500\u2500 server - cluster admin server \u2502 \u251c\u2500\u2500 auth - auth functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - auth CLI \u2502 \u2502 \u251c\u2500\u2500 server - auth server \u2502 \u2502 \u2514\u2500\u2500 testing - a mock auth server used for testing \u2502 \u251c\u2500\u2500 cmd - contains the various pachyderm entrypoints \u2502 \u2502 \u251c\u2500\u2500 pachctl - the CLI entrypoint \u2502 \u2502 \u251c\u2500\u2500 pachctl-doc - helps generate docs for the CLI \u2502 \u2502 \u251c\u2500\u2500 pachd - the server entrypoint \u2502 \u2502 \u2514\u2500\u2500 worker - the worker entrypoint \u2502 \u251c\u2500\u2500 debug - debug functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - debug CLI \u2502 \u2502 \u2514\u2500\u2500 server - debug server \u2502 \u251c\u2500\u2500 deploy - storage secret deployment server \u2502 \u251c\u2500\u2500 enterprise - enterprise functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - enterprise CLI \u2502 \u2502 \u2514\u2500\u2500 server - enterprise server \u2502 \u251c\u2500\u2500 health - health check server \u2502 \u251c\u2500\u2500 http - PFS-over-HTTP server, used by the dash to serve PFS content \u2502 \u251c\u2500\u2500 pfs - PFS functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - PFS CLI \u2502 \u2502 \u251c\u2500\u2500 fuse - support mounting PFS repos via FUSE \u2502 \u2502 \u251c\u2500\u2500 pretty - pretty-printing of PFS metadata in the CLI \u2502 \u2502 \u251c\u2500\u2500 s3 - the s3gateway, an s3-like HTTP API for serving PFS content \u2502 \u2502 \u2514\u2500\u2500 server - PFS server \u2502 \u251c\u2500\u2500 pkg - utility packages \u2502 \u2502 \u251c\u2500\u2500 ancestry - parses git ancestry reference strings \u2502 \u2502 \u251c\u2500\u2500 backoff - backoff algorithms for retrying operations \u2502 \u2502 \u251c\u2500\u2500 cache - a gRPC server for serving cached content \u2502 \u2502 \u251c\u2500\u2500 cert - functionality for generating x509 certificates \u2502 \u2502 \u251c\u2500\u2500 cmdutil - functionality for helping creating CLIs \u2502 \u2502 \u251c\u2500\u2500 collection - etcd collection management \u2502 \u2502 \u251c\u2500\u2500 dag - a simple in-memory directed acyclic graph data structure \u2502 \u2502 \u251c\u2500\u2500 deploy - functionality for deploying pachyderm \u2502 \u2502 \u2502 \u251c\u2500\u2500 assets - generates k8s manifests and other assets used in deployment \u2502 \u2502 \u2502 \u251c\u2500\u2500 cmds - deployment CLI \u2502 \u2502 \u2502 \u2514\u2500\u2500 images - handling of docker images \u2502 \u2502 \u251c\u2500\u2500 dlock - distributed lock on etcd \u2502 \u2502 \u251c\u2500\u2500 errutil - utility functions for error handling \u2502 \u2502 \u251c\u2500\u2500 exec - utilities for running external commands \u2502 \u2502 \u251c\u2500\u2500 hashtree - a Merkle tree library \u2502 \u2502 \u251c\u2500\u2500 lease - utility for managing resources with expirable leases \u2502 \u2502 \u251c\u2500\u2500 localcache - a concurrency-safe local disk cache \u2502 \u2502 \u251c\u2500\u2500 log - logging utilities \u2502 \u2502 \u251c\u2500\u2500 metrics - cluster metrics service using segment.io \u2502 \u2502 \u251c\u2500\u2500 migration \u2502 \u2502 \u251c\u2500\u2500 netutil - networking utilities \u2502 \u2502 \u251c\u2500\u2500 obj - tools for working with various object stores (e.g. S3) \u2502 \u2502 \u251c\u2500\u2500 pfsdb - the etcd database schema that PFS uses \u2502 \u2502 \u251c\u2500\u2500 pool - gRPC connection pooling \u2502 \u2502 \u251c\u2500\u2500 ppsconsts - PPS-related constants \u2502 \u2502 \u251c\u2500\u2500 ppsdb - the etcd database schema that PPS uses \u2502 \u2502 \u251c\u2500\u2500 ppsutil - PPS-related utility functions \u2502 \u2502 \u251c\u2500\u2500 pretty - function for pretty printing values \u2502 \u2502 \u251c\u2500\u2500 serviceenv - management of connections to pach services \u2502 \u2502 \u251c\u2500\u2500 sql - tools for working with postgres database dumps \u2502 \u2502 \u251c\u2500\u2500 sync - tools for syncing PFS content \u2502 \u2502 \u251c\u2500\u2500 tabwriter - tool for writing tab-delimited content \u2502 \u2502 \u251c\u2500\u2500 testutil - test-related utilities \u2502 \u2502 \u251c\u2500\u2500 uuid - UUID generation \u2502 \u2502 \u251c\u2500\u2500 watch - tool for watching etcd databases for changes \u2502 \u2502 \u2514\u2500\u2500 workload \u2502 \u251c\u2500\u2500 pps - PPS functionality \u2502 \u2502 \u251c\u2500\u2500 cmds - - PPS CLI \u2502 \u2502 \u251c\u2500\u2500 example - example PPS requests \u2502 \u2502 \u251c\u2500\u2500 pretty - pretty printing of PPS output to the CLI \u2502 \u2502 \u2514\u2500\u2500 server - PPS server \u2502 \u2502 \u2514\u2500\u2500 githook - support for github PPS sources \u2502 \u251c\u2500\u2500 vendor - vendored packages \u2502 \u2514\u2500\u2500 worker - pachd master and sidecar \u2514\u2500\u2500 testing - testing tools \u251c\u2500\u2500 loadtest - load tests for pachyderm \u2502 \u2514\u2500\u2500 split - stress tests of PFS merge functionality \u251c\u2500\u2500 match - a grep-like tool used in testing \u251c\u2500\u2500 saml-idp \u2514\u2500\u2500 vendor - vendored packages","title":"Repo layout"},{"location":"contributing/setup/","text":"Setup for contributors \u00b6 General requirements \u00b6 First, go through the general local installation instructions here . Additionally, make sure you have the following installed: golang 1.12+ docker jq pv Bash helpers \u00b6 To stay up to date, we recommend doing the following. First clone the code: (Note, as of 07/11/19 pachyderm is using go modules and recommends cloning the code outside of the $GOPATH, we use the location ~/workspace as an example, but the code can live anywhere) cd ~/workspace git clone git@github.com:pachyderm/pachyderm Then update your ~/.bash_profile by adding the line: source ~/workspace/pachyderm/etc/contributing/bash_helpers And you'll stay up to date! Special macOS configuration \u00b6 File descriptor limit \u00b6 If you're running tests locally, you'll need to up your file descriptor limit. To do this, first setup a LaunchDaemon to up the limit with sudo privileges: sudo cp ~/workspace/pachyderm/etc/contributing/com.apple.launchd.limit.plist /Library/LaunchDaemons/ Once you restart, this will take effect. To see the limits, run: launchctl limit maxfiles Before the change is in place you'll see something like 256 unlimited . After the change you'll see a much bigger number in the first field. This ups the system wide limit, but you'll also need to set a per-process limit. Second, up the per process limit by adding something like this to your ~/.bash_profile : ulimit -n 12288 Unfortunately, even after setting that limit it never seems to report the updated version. So if you try ulimit And just see unlimited , don't worry, it took effect. To make sure all of these settings are working, you can test that you have the proper setup by running: make test-pfs-server If this fails with a timeout, you'll probably also see 'too many files' type of errors. If that test passes, you're all good! Timeout helper \u00b6 You'll need the timeout utility to run the make launch task. To install on mac, do: brew install coreutils And then make sure to prepend the following to your path: PATH=\"/usr/local/opt/coreutils/libexec/gnubin:$PATH\" Dev cluster \u00b6 Now launch the dev cluster: make launch-dev-vm . And check it's status: kubectl get all . pachctl \u00b6 This will install the dev version of pachctl : cd ~/workspace/pachyderm make install pachctl version And make sure that $GOPATH/bin is on your $PATH somewhere Fully resetting \u00b6 Instead of running the makefile targets to re-compile pachctl and redeploy a dev cluster, we have a script that you can use to fully reset your pachyderm environment: 1) All existing cluster data is deleted 2) If possible, the virtual machine that the cluster is running on is wiped out 3) pachctl is recompiled 4) The dev cluster is re-deployed This reset is a bit more time consuming than running one-off Makefile targets, but comprehensively ensures that the cluster is in its expected state, and is especially helpful when you're first getting started with contributions and don't yet have a complete intuition on the various ways a cluster may get in an unexpected state. It's been tested on docker for mac and minikube, but likely works in other kubernetes environments as well. To run it, simply call ./etc/reset.py from the pachyderm repo root.","title":"Setup for Contributors"},{"location":"contributing/setup/#setup-for-contributors","text":"","title":"Setup for contributors"},{"location":"contributing/setup/#general-requirements","text":"First, go through the general local installation instructions here . Additionally, make sure you have the following installed: golang 1.12+ docker jq pv","title":"General requirements"},{"location":"contributing/setup/#bash-helpers","text":"To stay up to date, we recommend doing the following. First clone the code: (Note, as of 07/11/19 pachyderm is using go modules and recommends cloning the code outside of the $GOPATH, we use the location ~/workspace as an example, but the code can live anywhere) cd ~/workspace git clone git@github.com:pachyderm/pachyderm Then update your ~/.bash_profile by adding the line: source ~/workspace/pachyderm/etc/contributing/bash_helpers And you'll stay up to date!","title":"Bash helpers"},{"location":"contributing/setup/#special-macos-configuration","text":"","title":"Special macOS configuration"},{"location":"contributing/setup/#file-descriptor-limit","text":"If you're running tests locally, you'll need to up your file descriptor limit. To do this, first setup a LaunchDaemon to up the limit with sudo privileges: sudo cp ~/workspace/pachyderm/etc/contributing/com.apple.launchd.limit.plist /Library/LaunchDaemons/ Once you restart, this will take effect. To see the limits, run: launchctl limit maxfiles Before the change is in place you'll see something like 256 unlimited . After the change you'll see a much bigger number in the first field. This ups the system wide limit, but you'll also need to set a per-process limit. Second, up the per process limit by adding something like this to your ~/.bash_profile : ulimit -n 12288 Unfortunately, even after setting that limit it never seems to report the updated version. So if you try ulimit And just see unlimited , don't worry, it took effect. To make sure all of these settings are working, you can test that you have the proper setup by running: make test-pfs-server If this fails with a timeout, you'll probably also see 'too many files' type of errors. If that test passes, you're all good!","title":"File descriptor limit"},{"location":"contributing/setup/#timeout-helper","text":"You'll need the timeout utility to run the make launch task. To install on mac, do: brew install coreutils And then make sure to prepend the following to your path: PATH=\"/usr/local/opt/coreutils/libexec/gnubin:$PATH\"","title":"Timeout helper"},{"location":"contributing/setup/#dev-cluster","text":"Now launch the dev cluster: make launch-dev-vm . And check it's status: kubectl get all .","title":"Dev cluster"},{"location":"contributing/setup/#pachctl","text":"This will install the dev version of pachctl : cd ~/workspace/pachyderm make install pachctl version And make sure that $GOPATH/bin is on your $PATH somewhere","title":"pachctl"},{"location":"contributing/setup/#fully-resetting","text":"Instead of running the makefile targets to re-compile pachctl and redeploy a dev cluster, we have a script that you can use to fully reset your pachyderm environment: 1) All existing cluster data is deleted 2) If possible, the virtual machine that the cluster is running on is wiped out 3) pachctl is recompiled 4) The dev cluster is re-deployed This reset is a bit more time consuming than running one-off Makefile targets, but comprehensively ensures that the cluster is in its expected state, and is especially helpful when you're first getting started with contributions and don't yet have a complete intuition on the various ways a cluster may get in an unexpected state. It's been tested on docker for mac and minikube, but likely works in other kubernetes environments as well. To run it, simply call ./etc/reset.py from the pachyderm repo root.","title":"Fully resetting"},{"location":"deploy-manage/deploy/","text":"Overview \u00b6 Pachyderm runs on Kubernetes and is backed by an object store of your choice. As such, Pachyderm can run on any platform that supports Kubernetes and an object store. These following docs cover common deployments and related topics: Getting Started with PacHub Google Cloud Platform Amazon Web Services Azure OpenShift On Premises Create a Custom Pachyderm Deployment Configure Tracing with Jaeger Connect to a Pachyderm Cluster","title":"Overview"},{"location":"deploy-manage/deploy/#overview","text":"Pachyderm runs on Kubernetes and is backed by an object store of your choice. As such, Pachyderm can run on any platform that supports Kubernetes and an object store. These following docs cover common deployments and related topics: Getting Started with PacHub Google Cloud Platform Amazon Web Services Azure OpenShift On Premises Create a Custom Pachyderm Deployment Configure Tracing with Jaeger Connect to a Pachyderm Cluster","title":"Overview"},{"location":"deploy-manage/deploy/azure/","text":"Azure \u00b6 You can deploy Pachyderm in a new or existing Microsoft\u00ae Azure\u00ae Kubernetes Service environment and use Azure's resource to run your Pachyderm workloads. To deploy Pachyderm to AKS, you need to: Install Prerequisites Deploy Kubernetes Deploy Pachyderm Install Prerequisites \u00b6 Before you can deploy Pachyderm on Azure, you need to configure a few prerequisites on your client machine. If not explicitly specified, use the latest available version of the components listed below. Install the following prerequisites: Azure CLI 2.0.1 or later jq kubectl pachctl Install pachctl \u00b6 pachctl is a primary command-line utility for interacting with Pachyderm clusters. You can run the tool on Linux\u00ae, macOS\u00ae, and Microsoft\u00ae Windows\u00ae 10 or later operating systems and install it by using your favorite command line package manager. This section describes how you can install pachctl by using brew and curl . If you are installing pachctl on Windows, you need to first install Windows Subsystem (WSL) for Linux. To install pachctl , complete the following steps: To install on macOS by using brew , run the following command: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 * To install on Linux 64-bit or Windows 10 or later, run the following command: $ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_amd64.deb && sudo dpkg -i /tmp/pachctl.deb Verify your installation by running pachctl version : $ pachctl version --client-only COMPONENT VERSION pachctl 1 .9.0 Deploy Kubernetes \u00b6 You can deploy Kubernetes on Azure by following the official Azure Container Service documentation or by following the steps in this section. When you deploy Kubernetes on Azure, you need to specify the following parameters: .tg {border-collapse:collapse;border-spacing:0;border-color:#ccc;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Variable Description RESOURCE_GROUP A unique name for the resource group where Pachyderm is deployed. For example, `pach-resource-group`. LOCATION An Azure availability zone where AKS is available. For example, `centralus`. NODE_SIZE The size of the Kubernetes virtual machine (VM) instances. To avoid performance issues, Pachyderm recommends that you set this value to at least `Standard_DS4_v2` which gives you 8 CPUs, 28 Gib of Memory, 56 Gib SSD. CLUSTER_NAME A unique name for the Pachyderm cluster. For example, `pach-aks-cluster`. To deploy Kubernetes on Azure, complete the following steps: Log in to Azure: $ az login Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\" If you have not already logged in this command opens a browser window. Log in with your Azure credentials. After you log in, the following message appears in the command prompt: You have logged in. Now let us find all the subscriptions to which you have access... [ { \"cloudName\" : \"AzureCloud\" , \"id\" : \"your_id\" , \"isDefault\" : true, \"name\" : \"Microsoft Azure Sponsorship\" , \"state\" : \"Enabled\" , \"tenantId\" : \"your_tenant_id\" , \"user\" : { \"name\" : \"your_contact_id\" , \"type\" : \"user\" } } ] Create an Azure resource group. $ az group create --name = ${ RESOURCE_GROUP } --location = ${ LOCATION } Example: $ az group create --name = \"test-group\" --location = centralus { \"id\" : \"/subscriptions/6c9f2e1e-0eba-4421-b4cc-172f959ee110/resourceGroups/pach-resource-group\" , \"location\" : \"centralus\" , \"managedBy\" : null, \"name\" : \"pach-resource-group\" , \"properties\" : { \"provisioningState\" : \"Succeeded\" } , \"tags\" : null, \"type\" : null } Create an AKS cluster: $ az aks create --resource-group ${ RESOURCE_GROUP } --name ${ CLUSTER_NAME } --generate-ssh-keys --node-vm-size ${ NODE_SIZE } Example: $ az aks create --resource-group test-group --name test-cluster --generate-ssh-keys --node-vm-size Standard_DS4_v2 { \"aadProfile\" : null, \"addonProfiles\" : null, \"agentPoolProfiles\" : [ { \"availabilityZones\" : null, \"count\" : 3 , \"enableAutoScaling\" : null, \"maxCount\" : null, \"maxPods\" : 110 , \"minCount\" : null, \"name\" : \"nodepool1\" , \"orchestratorVersion\" : \"1.12.8\" , \"osDiskSizeGb\" : 100 , \"osType\" : \"Linux\" , \"provisioningState\" : \"Succeeded\" , \"type\" : \"AvailabilitySet\" , \"vmSize\" : \"Standard_DS4_v2\" , \"vnetSubnetId\" : null } ] , ... Confirm the version of the Kubernetes server: $ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.4\" , GitCommit: \"c27b913fddd1a6c480c229191a087698aa92f0b1\" , GitTreeState: \"clean\" , BuildDate: \"2019-03-01T23:36:43Z\" , GoVersion: \"go1.12\" , Compiler: \"gc\" , Platform: \"darwin/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.4\" , GitCommit: \"c27b913fddd1a6c480c229191a087698aa92f0b1\" , GitTreeState: \"clean\" , BuildDate: \"2019-02-28T13:30:26Z\" , GoVersion: \"go1.11.5\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } See also: Azure Virtual Machine sizes Add storage resources \u00b6 Pachyderm requires you to deploy an object store and a persistent volume in your cloud environment to function correctly. For best results, you need to use faster disk drives, such as Premium SSD Managed Disks that are available with the Azure Premium Storage offering. You need to specify the following parameters when you create storage resources: .tg {border-collapse:collapse;border-spacing:0;border-color:#ccc;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Variable Description STORAGE_ACCOUNT The name of the storage account where you store your data, unique in the Azure location CONTAINER_NAME The name of the Azure blob container where you store your data STORAGE_SIZE The size of the persistent volume to create in GBs. Allocate at least 10 GB. To create these resources, follow these steps: Clone the Pachyderm GitHub repo . Change the directory to the root directory of the pachyderm repository. Create an Azure storage account: $ az storage account create \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --location = \" ${ LOCATION } \" \\ --sku = Premium_LRS \\ --name = \" ${ STORAGE_ACCOUNT } \" \\ --kind = BlockBlobStorage System response: { \"accessTier\": null, \"creationTime\": \"2019-06-20T16:05:55.616832+00:00\", \"customDomain\": null, \"enableAzureFilesAadIntegration\": null, \"enableHttpsTrafficOnly\": false, \"encryption\": { \"keySource\": \"Microsoft.Storage\", \"keyVaultProperties\": null, \"services\": { \"blob\": { \"enabled\": true, ... Make sure that you set Stock Keeping Unit (SKU) to Premium_LRS and the kind parameter is set to BlockBlobStorage . This configuration results in a storage that uses SSDs rather than standard Hard Disk Drives (HDD). If you set this parameter to an HDD-based storage option, your Pachyderm cluster will be too slow and might malfunction. Verify that your storage account has been successfully created: $ az storage account list Build a Microsoft tool for creating Azure VMs from an image: $ STORAGE_KEY = \" $( az storage account keys list \\ --account-name = \" ${ STORAGE_ACCOUNT } \" \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --output = json \\ | jq '.[0].value' -r ) \" Find the generated key in the Storage accounts > Access keys section in the Azure Portal or by running the following command: $ az storage account keys list --account-name = ${ STORAGE_ACCOUNT } [ { \"keyName\" : \"key1\" , \"permissions\" : \"Full\" , \"value\" : \"\" } ] See Also Azure Storage Deploy Pachyderm \u00b6 After you complete all the sections above, you can deploy Pachyderm on Azure. If you have previously tried to run Pachyderm locally, make sure that you are using the right Kubernetes context. Otherwise, you might accidentally deploy your cluster on Minikube. Verify cluster context: $ kubectl config current-context This command should return the name of your Kubernetes cluster that runs on Azure. If you have a different contents displayed, configure kubectl to use your Azure configuration: $ az aks get-credentials --resource-group ${ RESOURCE_GROUP } --name ${ CLUSTER_NAME } Merged \" ${ CLUSTER_NAME } \" as current context in /Users/test-user/.kube/config Run the following command: $ pachctl deploy microsoft ${ CONTAINER_NAME } ${ STORAGE_ACCOUNT } ${ STORAGE_KEY } ${ STORAGE_SIZE } --dynamic-etcd-nodes 1 Example: $ pachctl deploy microsoft test-container teststorage <key> 10 --dynamic-etcd-nodes 1 serviceaccount/pachyderm configured clusterrole.rbac.authorization.k8s.io/pachyderm configured clusterrolebinding.rbac.authorization.k8s.io/pachyderm configured service/etcd-headless created statefulset.apps/etcd created service/etcd configured service/pachd configured deployment.apps/pachd configured service/dash configured deployment.apps/dash configured secret/pachyderm-storage-secret configured Pachyderm is launching. Check its status with \"kubectl get all\" Once launched, access the dashboard by running \"pachctl port-forward\" Because Pachyderm pulls containers from DockerHub, it might take some time before the pachd pods start. You can check the status of the deployment by periodically running kubectl get all . When pachyderm is up and running, get the information about the pods: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-482120938-vdlg9 2 /2 Running 0 54m etcd-0 1 /1 Running 0 54m pachd-1971105989-mjn61 1 /1 Running 0 54m Note: Sometimes Kubernetes tries to start pachd nodes before the etcd nodes are ready which might result in the pachd nodes restarting. You can safely ignore those restarts. To connect to the cluster from your local machine, such as your laptop, set up port forwarding to enable pachctl and cluster communication: $ pachctl port-forward Verify that the cluster is up and running: $ pachctl version COMPONENT VERSION pachctl 1 .9.0 pachd 1 .9.0","title":"Deploy on Azure"},{"location":"deploy-manage/deploy/azure/#azure","text":"You can deploy Pachyderm in a new or existing Microsoft\u00ae Azure\u00ae Kubernetes Service environment and use Azure's resource to run your Pachyderm workloads. To deploy Pachyderm to AKS, you need to: Install Prerequisites Deploy Kubernetes Deploy Pachyderm","title":"Azure"},{"location":"deploy-manage/deploy/azure/#install-prerequisites","text":"Before you can deploy Pachyderm on Azure, you need to configure a few prerequisites on your client machine. If not explicitly specified, use the latest available version of the components listed below. Install the following prerequisites: Azure CLI 2.0.1 or later jq kubectl pachctl","title":"Install Prerequisites"},{"location":"deploy-manage/deploy/azure/#install-pachctl","text":"pachctl is a primary command-line utility for interacting with Pachyderm clusters. You can run the tool on Linux\u00ae, macOS\u00ae, and Microsoft\u00ae Windows\u00ae 10 or later operating systems and install it by using your favorite command line package manager. This section describes how you can install pachctl by using brew and curl . If you are installing pachctl on Windows, you need to first install Windows Subsystem (WSL) for Linux. To install pachctl , complete the following steps: To install on macOS by using brew , run the following command: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 * To install on Linux 64-bit or Windows 10 or later, run the following command: $ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_amd64.deb && sudo dpkg -i /tmp/pachctl.deb Verify your installation by running pachctl version : $ pachctl version --client-only COMPONENT VERSION pachctl 1 .9.0","title":"Install pachctl"},{"location":"deploy-manage/deploy/azure/#deploy-kubernetes","text":"You can deploy Kubernetes on Azure by following the official Azure Container Service documentation or by following the steps in this section. When you deploy Kubernetes on Azure, you need to specify the following parameters: .tg {border-collapse:collapse;border-spacing:0;border-color:#ccc;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Variable Description RESOURCE_GROUP A unique name for the resource group where Pachyderm is deployed. For example, `pach-resource-group`. LOCATION An Azure availability zone where AKS is available. For example, `centralus`. NODE_SIZE The size of the Kubernetes virtual machine (VM) instances. To avoid performance issues, Pachyderm recommends that you set this value to at least `Standard_DS4_v2` which gives you 8 CPUs, 28 Gib of Memory, 56 Gib SSD. CLUSTER_NAME A unique name for the Pachyderm cluster. For example, `pach-aks-cluster`. To deploy Kubernetes on Azure, complete the following steps: Log in to Azure: $ az login Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\" If you have not already logged in this command opens a browser window. Log in with your Azure credentials. After you log in, the following message appears in the command prompt: You have logged in. Now let us find all the subscriptions to which you have access... [ { \"cloudName\" : \"AzureCloud\" , \"id\" : \"your_id\" , \"isDefault\" : true, \"name\" : \"Microsoft Azure Sponsorship\" , \"state\" : \"Enabled\" , \"tenantId\" : \"your_tenant_id\" , \"user\" : { \"name\" : \"your_contact_id\" , \"type\" : \"user\" } } ] Create an Azure resource group. $ az group create --name = ${ RESOURCE_GROUP } --location = ${ LOCATION } Example: $ az group create --name = \"test-group\" --location = centralus { \"id\" : \"/subscriptions/6c9f2e1e-0eba-4421-b4cc-172f959ee110/resourceGroups/pach-resource-group\" , \"location\" : \"centralus\" , \"managedBy\" : null, \"name\" : \"pach-resource-group\" , \"properties\" : { \"provisioningState\" : \"Succeeded\" } , \"tags\" : null, \"type\" : null } Create an AKS cluster: $ az aks create --resource-group ${ RESOURCE_GROUP } --name ${ CLUSTER_NAME } --generate-ssh-keys --node-vm-size ${ NODE_SIZE } Example: $ az aks create --resource-group test-group --name test-cluster --generate-ssh-keys --node-vm-size Standard_DS4_v2 { \"aadProfile\" : null, \"addonProfiles\" : null, \"agentPoolProfiles\" : [ { \"availabilityZones\" : null, \"count\" : 3 , \"enableAutoScaling\" : null, \"maxCount\" : null, \"maxPods\" : 110 , \"minCount\" : null, \"name\" : \"nodepool1\" , \"orchestratorVersion\" : \"1.12.8\" , \"osDiskSizeGb\" : 100 , \"osType\" : \"Linux\" , \"provisioningState\" : \"Succeeded\" , \"type\" : \"AvailabilitySet\" , \"vmSize\" : \"Standard_DS4_v2\" , \"vnetSubnetId\" : null } ] , ... Confirm the version of the Kubernetes server: $ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.4\" , GitCommit: \"c27b913fddd1a6c480c229191a087698aa92f0b1\" , GitTreeState: \"clean\" , BuildDate: \"2019-03-01T23:36:43Z\" , GoVersion: \"go1.12\" , Compiler: \"gc\" , Platform: \"darwin/amd64\" } Server Version: version.Info { Major: \"1\" , Minor: \"13\" , GitVersion: \"v1.13.4\" , GitCommit: \"c27b913fddd1a6c480c229191a087698aa92f0b1\" , GitTreeState: \"clean\" , BuildDate: \"2019-02-28T13:30:26Z\" , GoVersion: \"go1.11.5\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } See also: Azure Virtual Machine sizes","title":"Deploy Kubernetes"},{"location":"deploy-manage/deploy/azure/#add-storage-resources","text":"Pachyderm requires you to deploy an object store and a persistent volume in your cloud environment to function correctly. For best results, you need to use faster disk drives, such as Premium SSD Managed Disks that are available with the Azure Premium Storage offering. You need to specify the following parameters when you create storage resources: .tg {border-collapse:collapse;border-spacing:0;border-color:#ccc;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Variable Description STORAGE_ACCOUNT The name of the storage account where you store your data, unique in the Azure location CONTAINER_NAME The name of the Azure blob container where you store your data STORAGE_SIZE The size of the persistent volume to create in GBs. Allocate at least 10 GB. To create these resources, follow these steps: Clone the Pachyderm GitHub repo . Change the directory to the root directory of the pachyderm repository. Create an Azure storage account: $ az storage account create \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --location = \" ${ LOCATION } \" \\ --sku = Premium_LRS \\ --name = \" ${ STORAGE_ACCOUNT } \" \\ --kind = BlockBlobStorage System response: { \"accessTier\": null, \"creationTime\": \"2019-06-20T16:05:55.616832+00:00\", \"customDomain\": null, \"enableAzureFilesAadIntegration\": null, \"enableHttpsTrafficOnly\": false, \"encryption\": { \"keySource\": \"Microsoft.Storage\", \"keyVaultProperties\": null, \"services\": { \"blob\": { \"enabled\": true, ... Make sure that you set Stock Keeping Unit (SKU) to Premium_LRS and the kind parameter is set to BlockBlobStorage . This configuration results in a storage that uses SSDs rather than standard Hard Disk Drives (HDD). If you set this parameter to an HDD-based storage option, your Pachyderm cluster will be too slow and might malfunction. Verify that your storage account has been successfully created: $ az storage account list Build a Microsoft tool for creating Azure VMs from an image: $ STORAGE_KEY = \" $( az storage account keys list \\ --account-name = \" ${ STORAGE_ACCOUNT } \" \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --output = json \\ | jq '.[0].value' -r ) \" Find the generated key in the Storage accounts > Access keys section in the Azure Portal or by running the following command: $ az storage account keys list --account-name = ${ STORAGE_ACCOUNT } [ { \"keyName\" : \"key1\" , \"permissions\" : \"Full\" , \"value\" : \"\" } ] See Also Azure Storage","title":"Add storage resources"},{"location":"deploy-manage/deploy/azure/#deploy-pachyderm","text":"After you complete all the sections above, you can deploy Pachyderm on Azure. If you have previously tried to run Pachyderm locally, make sure that you are using the right Kubernetes context. Otherwise, you might accidentally deploy your cluster on Minikube. Verify cluster context: $ kubectl config current-context This command should return the name of your Kubernetes cluster that runs on Azure. If you have a different contents displayed, configure kubectl to use your Azure configuration: $ az aks get-credentials --resource-group ${ RESOURCE_GROUP } --name ${ CLUSTER_NAME } Merged \" ${ CLUSTER_NAME } \" as current context in /Users/test-user/.kube/config Run the following command: $ pachctl deploy microsoft ${ CONTAINER_NAME } ${ STORAGE_ACCOUNT } ${ STORAGE_KEY } ${ STORAGE_SIZE } --dynamic-etcd-nodes 1 Example: $ pachctl deploy microsoft test-container teststorage <key> 10 --dynamic-etcd-nodes 1 serviceaccount/pachyderm configured clusterrole.rbac.authorization.k8s.io/pachyderm configured clusterrolebinding.rbac.authorization.k8s.io/pachyderm configured service/etcd-headless created statefulset.apps/etcd created service/etcd configured service/pachd configured deployment.apps/pachd configured service/dash configured deployment.apps/dash configured secret/pachyderm-storage-secret configured Pachyderm is launching. Check its status with \"kubectl get all\" Once launched, access the dashboard by running \"pachctl port-forward\" Because Pachyderm pulls containers from DockerHub, it might take some time before the pachd pods start. You can check the status of the deployment by periodically running kubectl get all . When pachyderm is up and running, get the information about the pods: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-482120938-vdlg9 2 /2 Running 0 54m etcd-0 1 /1 Running 0 54m pachd-1971105989-mjn61 1 /1 Running 0 54m Note: Sometimes Kubernetes tries to start pachd nodes before the etcd nodes are ready which might result in the pachd nodes restarting. You can safely ignore those restarts. To connect to the cluster from your local machine, such as your laptop, set up port forwarding to enable pachctl and cluster communication: $ pachctl port-forward Verify that the cluster is up and running: $ pachctl version COMPONENT VERSION pachctl 1 .9.0 pachd 1 .9.0","title":"Deploy Pachyderm"},{"location":"deploy-manage/deploy/configuring_k8s_ingress/","text":"Configuring Kubernetes Ingress for Pachyderm \u00b6 Coming soon. This document, when complete, will detail the Kubernetes ingress configuration you'd need for using pachctl and the dashboard outside of the Kubernetes cluster.","title":"Configuring Kubernetes Ingress for Pachyderm"},{"location":"deploy-manage/deploy/configuring_k8s_ingress/#configuring-kubernetes-ingress-for-pachyderm","text":"Coming soon. This document, when complete, will detail the Kubernetes ingress configuration you'd need for using pachctl and the dashboard outside of the Kubernetes cluster.","title":"Configuring Kubernetes Ingress for Pachyderm"},{"location":"deploy-manage/deploy/connect-to-cluster/","text":"Connect to a Pachyderm cluster \u00b6 After you deploy a Pachyderm cluster, you can continue to use the command-line interface, connect to the Pachyderm dashboard, or configure third-party applications to access your cluster programmatically. Often all you need to do is just continue to use the command-line interface to create repositories, pipelines, and upload your data. At other times, you might have multiple Pachyderm clusters deployed and need to switch between them to perform management operations. You do not need to configure anything specific to start using the Pachyderm CLI right after deployment. However, the Pachyderm dashboard and the S3 gateway require explicit port-forwarding or direct access through an externally exposed IP address and port. This section describes the various options available for you to connect to your Pachyderm cluster. Connect to a Local Cluster \u00b6 If you are just exploring Pachyderm, use port-forwarding to access both pachd and the Pachyderm dashboard. By default, Pachyderm enables port-forwarding from pachctl to pachd . If you do not want to use port-forwarding for pachctl operations, configure a pachd_address as described in Connect by using a Pachyderm context . To connect to the Pachyderm dashboard, you can either use port-forwarding, or the IP address of the virtual machine on which your Kubernetes cluster is running. The following example shows how to access a Pachyderm cluster that runs in minikube . To connect to a Pachyderm dashboard, complete the following steps: To use port-forwarding, run: $ pachctl port-forward To use the IP address of the VM:` Get the minikube IP address: $ minikube ip Point your browser to the following address: <minikube_ip>:30080 Connect to a Cluster Deployed on a Cloud Platform \u00b6 As in a local cluster deployment, a Pachyderm cluster deployed on a cloud platform has impllcit port-forwarding enabled. This means that, if you are connected to the cluster so that kubectl works, pachctl can communicate with pachd without any additional configuration. Other services still need explicit port forwarding. For example, to access the Pachyderm dashboard, you need to run pachctl port-forward. Since a Pachyderm cluster deployed on a cloud platform is more likely to become a production deployment, configuring a pachd_address as described in Connect by using a Pachyderm context is the preferred way. Connect by using a Pachyderm context \u00b6 You can specify an IP address that you use to connect to the Pachyderm UI and the S3 gateway by storing that address in the Pachyderm configuration file as the pachd_address parameter. If you have already deployed a Pachyderm cluster, you can set a Pachyderm IP address by updating your cluster configuration file. This configuration is supported for deployments that do not have a firewall set up between the Pachyderm cluster deployed in the cloud and the client machine. Defining a dedicated pachd IP address and host is a more reliable way that might also result in better performance compared to port-forwarding. Therefore, Pachyderm recommends that you use contexts in all production environments. This configuration requires that you deploy an ingress controller and a reliable security method to protect the traffic between the Pachyderm pods and your client machine. Remember that exposing your traffic through a public ingress might create a security issue. Therefore, if you expose your pachd endpoint, you need to make sure that you take steps to protect the endpoint and traffic against common container security threats. Port-forwarding might be an alternative which might result in sufficient performance if you place the data that is consumed by your pipeline in object store buckets located in the same region. For more information about Pachyderm contexts, see Manage Cluster Access . To connect by using a Pachyderm context, complete the following steps: Get the current context: $ pachctl config get active-context This command returns the name of the currently active context. Use this as the argument to the command below. If no IP address is set up for this cluster, you get the following output: $ pachctl config get context <name> { } Set up pachd_address : $ pachctl config update context <name> --pachd-address <host:port> Example: $ pachctl config update context local --pachd-address 192 .168.1.15:30650 Note: By default, the pachd port is 30650 . Verify that the configuration has been updated: $ pachctl config get context local { \"pachd_address\" : \"192.168.1.15:30650\" } Connect by Using Port-Forwarding \u00b6 The Pachyderm port-forwarding is the simplest way to enable cluster access and test basic Pachyderm functionality. Pachyderm automatically starts port-forwarding from pachctl to pachd . Therefore, the traffic from the local machine goes to the pachd endpoint through the Kubernetes API. However, to open a persistent tunnel to other ports, including the Pachyderm dashboard, git and authentication hooks, the built-in HTTP file API, and other, you need to run port-forwarding explicitly. Also, if you are connecting with port-forward, you are using the 0.0.0.0 . Therefore, if you are using a proxy, it needs to handle that appropriately. Although port-forwarding is convenient for testing, for production environments, this connection might be too slow and unreliable. The speed of the port-forwarded traffic is limited to 1 MB/s . Therefore, if you experience high latency with put file and get file operations, or if you anticipate high throughput in your Pachyderm environment, you need to enable ingress access to your Kubernetes cluster. Each cloud provider has its own requirements and procedures for enabling ingress controller and load balancing traffic at the application layer for better performance. Remember that exposing your traffic through a public ingress might create a security issue. Therefore, if you expose your pachd endpoint, you need to make that you take steps to protect the endpoint and traffic against common container security threats. Port-forwarding might be an alternative which might provide satisfactory performance if you place the data that is consumed by your pipeline in object store buckets located in the same region. To enable port-forwarding, complete the following steps: Open a new terminal window. Run: $ pachctl port-forward This command does not stop unless you manually interrupt it. You can run other pachctl commands from another window. If any of your pachctl commands hang, verify if the kubectl port-forwarding has had issues that prevent pachctl port-forward from running properly.","title":"Connect to a Pachyderm cluster"},{"location":"deploy-manage/deploy/connect-to-cluster/#connect-to-a-pachyderm-cluster","text":"After you deploy a Pachyderm cluster, you can continue to use the command-line interface, connect to the Pachyderm dashboard, or configure third-party applications to access your cluster programmatically. Often all you need to do is just continue to use the command-line interface to create repositories, pipelines, and upload your data. At other times, you might have multiple Pachyderm clusters deployed and need to switch between them to perform management operations. You do not need to configure anything specific to start using the Pachyderm CLI right after deployment. However, the Pachyderm dashboard and the S3 gateway require explicit port-forwarding or direct access through an externally exposed IP address and port. This section describes the various options available for you to connect to your Pachyderm cluster.","title":"Connect to a Pachyderm cluster"},{"location":"deploy-manage/deploy/connect-to-cluster/#connect-to-a-local-cluster","text":"If you are just exploring Pachyderm, use port-forwarding to access both pachd and the Pachyderm dashboard. By default, Pachyderm enables port-forwarding from pachctl to pachd . If you do not want to use port-forwarding for pachctl operations, configure a pachd_address as described in Connect by using a Pachyderm context . To connect to the Pachyderm dashboard, you can either use port-forwarding, or the IP address of the virtual machine on which your Kubernetes cluster is running. The following example shows how to access a Pachyderm cluster that runs in minikube . To connect to a Pachyderm dashboard, complete the following steps: To use port-forwarding, run: $ pachctl port-forward To use the IP address of the VM:` Get the minikube IP address: $ minikube ip Point your browser to the following address: <minikube_ip>:30080","title":"Connect to a Local Cluster"},{"location":"deploy-manage/deploy/connect-to-cluster/#connect-to-a-cluster-deployed-on-a-cloud-platform","text":"As in a local cluster deployment, a Pachyderm cluster deployed on a cloud platform has impllcit port-forwarding enabled. This means that, if you are connected to the cluster so that kubectl works, pachctl can communicate with pachd without any additional configuration. Other services still need explicit port forwarding. For example, to access the Pachyderm dashboard, you need to run pachctl port-forward. Since a Pachyderm cluster deployed on a cloud platform is more likely to become a production deployment, configuring a pachd_address as described in Connect by using a Pachyderm context is the preferred way.","title":"Connect to a Cluster Deployed on a Cloud Platform"},{"location":"deploy-manage/deploy/connect-to-cluster/#connect-by-using-a-pachyderm-context","text":"You can specify an IP address that you use to connect to the Pachyderm UI and the S3 gateway by storing that address in the Pachyderm configuration file as the pachd_address parameter. If you have already deployed a Pachyderm cluster, you can set a Pachyderm IP address by updating your cluster configuration file. This configuration is supported for deployments that do not have a firewall set up between the Pachyderm cluster deployed in the cloud and the client machine. Defining a dedicated pachd IP address and host is a more reliable way that might also result in better performance compared to port-forwarding. Therefore, Pachyderm recommends that you use contexts in all production environments. This configuration requires that you deploy an ingress controller and a reliable security method to protect the traffic between the Pachyderm pods and your client machine. Remember that exposing your traffic through a public ingress might create a security issue. Therefore, if you expose your pachd endpoint, you need to make sure that you take steps to protect the endpoint and traffic against common container security threats. Port-forwarding might be an alternative which might result in sufficient performance if you place the data that is consumed by your pipeline in object store buckets located in the same region. For more information about Pachyderm contexts, see Manage Cluster Access . To connect by using a Pachyderm context, complete the following steps: Get the current context: $ pachctl config get active-context This command returns the name of the currently active context. Use this as the argument to the command below. If no IP address is set up for this cluster, you get the following output: $ pachctl config get context <name> { } Set up pachd_address : $ pachctl config update context <name> --pachd-address <host:port> Example: $ pachctl config update context local --pachd-address 192 .168.1.15:30650 Note: By default, the pachd port is 30650 . Verify that the configuration has been updated: $ pachctl config get context local { \"pachd_address\" : \"192.168.1.15:30650\" }","title":"Connect by using a Pachyderm context"},{"location":"deploy-manage/deploy/connect-to-cluster/#connect-by-using-port-forwarding","text":"The Pachyderm port-forwarding is the simplest way to enable cluster access and test basic Pachyderm functionality. Pachyderm automatically starts port-forwarding from pachctl to pachd . Therefore, the traffic from the local machine goes to the pachd endpoint through the Kubernetes API. However, to open a persistent tunnel to other ports, including the Pachyderm dashboard, git and authentication hooks, the built-in HTTP file API, and other, you need to run port-forwarding explicitly. Also, if you are connecting with port-forward, you are using the 0.0.0.0 . Therefore, if you are using a proxy, it needs to handle that appropriately. Although port-forwarding is convenient for testing, for production environments, this connection might be too slow and unreliable. The speed of the port-forwarded traffic is limited to 1 MB/s . Therefore, if you experience high latency with put file and get file operations, or if you anticipate high throughput in your Pachyderm environment, you need to enable ingress access to your Kubernetes cluster. Each cloud provider has its own requirements and procedures for enabling ingress controller and load balancing traffic at the application layer for better performance. Remember that exposing your traffic through a public ingress might create a security issue. Therefore, if you expose your pachd endpoint, you need to make that you take steps to protect the endpoint and traffic against common container security threats. Port-forwarding might be an alternative which might provide satisfactory performance if you place the data that is consumed by your pipeline in object store buckets located in the same region. To enable port-forwarding, complete the following steps: Open a new terminal window. Run: $ pachctl port-forward This command does not stop unless you manually interrupt it. You can run other pachctl commands from another window. If any of your pachctl commands hang, verify if the kubectl port-forwarding has had issues that prevent pachctl port-forward from running properly.","title":"Connect by Using Port-Forwarding"},{"location":"deploy-manage/deploy/custom_object_stores/","text":"Custom Object Stores \u00b6 In other sections of this guide was have demonstrated how to deploy Pachyderm in a single cloud using that cloud's object store offering. However, Pachyderm can be backed by any object store, and you are not restricted to the object store service provided by the cloud in which you are deploying. As long as you are running an object store that has an S3 compatible API, you can easily deploy Pachyderm in a way that will allow you to back Pachyderm by that object store. For example, we have seen Pachyderm be backed by Minio , GlusterFS , Ceph , and more. To deploy Pachyderm with your choice of object store in Google, Azure, or AWS, see the below guides. To deploy Pachyderm on premise with a custom object store, see the on premise docs . Common Prerequisites \u00b6 A working Kubernetes cluster and kubectl . An account on or running instance of an object store with an S3 compatible API. You should be able to get an ID, secret, bucket name, and endpoint that point to this object store. Google + Custom Object Store \u00b6 Additional prerequisites: Google Cloud SDK >= 124.0.0 - If this is the first time you use the SDK, make sure to follow the quick start guide . First, we need to create a persistent disk for Pachyderm's metadata: # Name this whatever you want, we chose pach-disk as a default $ STORAGE_NAME = pach-disk # For a demo you should only need 10 GB. This stores PFS metadata. For reference, 1GB # should work for 1000 commits on 1000 files. $ STORAGE_SIZE =[ the size of the volume that you are going to create, in GBs. e.g. \"10\" ] # Create the disk. gcloud compute disks create --size = ${ STORAGE_SIZE } GB ${ STORAGE_NAME } Then we can deploy Pachyderm: pachctl deploy custom --persistent-disk google --object-store s3 ${ STORAGE_NAME } ${ STORAGE_SIZE } <object store bucket> <object store id> <object store secret> <object store endpoint> --static-etcd-volume = ${ STORAGE_NAME } AWS + Custom Object Store \u00b6 Additional prerequisites: AWS CLI - have it installed and have your AWS credentials configured. First, we need to create a persistent disk for Pachyderm's metadata: # We recommend between 1 and 10 GB. This stores PFS metadata. For reference 1GB # should work for 1000 commits on 1000 files. $ STORAGE_SIZE =[ the size of the EBS volume that you are going to create, in GBs. e.g. \"10\" ] $ AWS_REGION =[ the AWS region of your Kubernetes cluster. e.g. \"us-west-2\" ( not us-west-2a )] $ AWS_AVAILABILITY_ZONE =[ the AWS availability zone of your Kubernetes cluster. e.g. \"us-west-2a\" ] # Create the volume. $ aws ec2 create-volume --size ${ STORAGE_SIZE } --region ${ AWS_REGION } --availability-zone ${ AWS_AVAILABILITY_ZONE } --volume-type gp2 # Store the volume ID. $ aws ec2 describe-volumes $ STORAGE_NAME =[ volume id ] The we can deploy Pachyderm: pachctl deploy custom --persistent-disk aws --object-store s3 ${ STORAGE_NAME } ${ STORAGE_SIZE } <object store bucket> <object store id> <object store secret> <object store endpoint> --static-etcd-volume = ${ STORAGE_NAME } Azure + Custom Object Store \u00b6 Additional prerequisites: Install Azure CLI >= 2.0.1 Install jq Clone github.com/pachyderm/pachyderm and work from the root of that project. First, we need to create a persistent disk for Pachyderm's metadata. To do this, start by declaring some environmental variables: # Needs to be globally unique across the entire Azure location $ RESOURCE_GROUP =[ The name of the resource group where the Azure resources will be organized ] $ LOCATION =[ The Azure region of your Kubernetes cluster. e.g. \"West US2\" ] # Needs to be globally unique across the entire Azure location $ STORAGE_ACCOUNT =[ The name of the storage account where your data will be stored ] # Needs to end in a \".vhd\" extension $ STORAGE_NAME = pach-disk.vhd # We recommend between 1 and 10 GB. This stores PFS metadata. For reference 1GB # should work for 1000 commits on 1000 files. $ STORAGE_SIZE =[ the size of the data disk volume that you are going to create, in GBs. e.g. \"10\" ] And then run: # Create a resource group $ az group create --name = ${ RESOURCE_GROUP } --location = ${ LOCATION } # Create azure storage account az storage account create \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --location = \" ${ LOCATION } \" \\ --sku = Standard_LRS \\ --name = \" ${ STORAGE_ACCOUNT } \" \\ --kind = Storage # Build microsoft tool for creating Azure VMs from an image $ STORAGE_KEY = \" $( az storage account keys list \\ --account-name = \" ${ STORAGE_ACCOUNT } \" \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --output = json \\ | jq . [ 0 ] .value -r ) \" $ make docker-build-microsoft-vhd $ VOLUME_URI = \" $( docker run -it microsoft_vhd \\ \" ${ STORAGE_ACCOUNT } \" \\ \" ${ STORAGE_KEY } \" \\ \" ${ CONTAINER_NAME } \" \\ \" ${ STORAGE_NAME } \" \\ \" ${ STORAGE_SIZE } G\" ) \" To check that everything has been setup correctly, try: $ az storage account list | jq '.[].name' The we can deploy Pachyderm: pachctl deploy custom --persistent-disk azure --object-store s3 ${ VOLUME_URI } ${ STORAGE_SIZE } <object store bucket> <object store id> <object store secret> <object store endpoint> --static-etcd-volume = ${ VOLUME_URI }","title":"Custom Object Stores"},{"location":"deploy-manage/deploy/custom_object_stores/#custom-object-stores","text":"In other sections of this guide was have demonstrated how to deploy Pachyderm in a single cloud using that cloud's object store offering. However, Pachyderm can be backed by any object store, and you are not restricted to the object store service provided by the cloud in which you are deploying. As long as you are running an object store that has an S3 compatible API, you can easily deploy Pachyderm in a way that will allow you to back Pachyderm by that object store. For example, we have seen Pachyderm be backed by Minio , GlusterFS , Ceph , and more. To deploy Pachyderm with your choice of object store in Google, Azure, or AWS, see the below guides. To deploy Pachyderm on premise with a custom object store, see the on premise docs .","title":"Custom Object Stores"},{"location":"deploy-manage/deploy/custom_object_stores/#common-prerequisites","text":"A working Kubernetes cluster and kubectl . An account on or running instance of an object store with an S3 compatible API. You should be able to get an ID, secret, bucket name, and endpoint that point to this object store.","title":"Common Prerequisites"},{"location":"deploy-manage/deploy/custom_object_stores/#google-custom-object-store","text":"Additional prerequisites: Google Cloud SDK >= 124.0.0 - If this is the first time you use the SDK, make sure to follow the quick start guide . First, we need to create a persistent disk for Pachyderm's metadata: # Name this whatever you want, we chose pach-disk as a default $ STORAGE_NAME = pach-disk # For a demo you should only need 10 GB. This stores PFS metadata. For reference, 1GB # should work for 1000 commits on 1000 files. $ STORAGE_SIZE =[ the size of the volume that you are going to create, in GBs. e.g. \"10\" ] # Create the disk. gcloud compute disks create --size = ${ STORAGE_SIZE } GB ${ STORAGE_NAME } Then we can deploy Pachyderm: pachctl deploy custom --persistent-disk google --object-store s3 ${ STORAGE_NAME } ${ STORAGE_SIZE } <object store bucket> <object store id> <object store secret> <object store endpoint> --static-etcd-volume = ${ STORAGE_NAME }","title":"Google + Custom Object Store"},{"location":"deploy-manage/deploy/custom_object_stores/#aws-custom-object-store","text":"Additional prerequisites: AWS CLI - have it installed and have your AWS credentials configured. First, we need to create a persistent disk for Pachyderm's metadata: # We recommend between 1 and 10 GB. This stores PFS metadata. For reference 1GB # should work for 1000 commits on 1000 files. $ STORAGE_SIZE =[ the size of the EBS volume that you are going to create, in GBs. e.g. \"10\" ] $ AWS_REGION =[ the AWS region of your Kubernetes cluster. e.g. \"us-west-2\" ( not us-west-2a )] $ AWS_AVAILABILITY_ZONE =[ the AWS availability zone of your Kubernetes cluster. e.g. \"us-west-2a\" ] # Create the volume. $ aws ec2 create-volume --size ${ STORAGE_SIZE } --region ${ AWS_REGION } --availability-zone ${ AWS_AVAILABILITY_ZONE } --volume-type gp2 # Store the volume ID. $ aws ec2 describe-volumes $ STORAGE_NAME =[ volume id ] The we can deploy Pachyderm: pachctl deploy custom --persistent-disk aws --object-store s3 ${ STORAGE_NAME } ${ STORAGE_SIZE } <object store bucket> <object store id> <object store secret> <object store endpoint> --static-etcd-volume = ${ STORAGE_NAME }","title":"AWS + Custom Object Store"},{"location":"deploy-manage/deploy/custom_object_stores/#azure-custom-object-store","text":"Additional prerequisites: Install Azure CLI >= 2.0.1 Install jq Clone github.com/pachyderm/pachyderm and work from the root of that project. First, we need to create a persistent disk for Pachyderm's metadata. To do this, start by declaring some environmental variables: # Needs to be globally unique across the entire Azure location $ RESOURCE_GROUP =[ The name of the resource group where the Azure resources will be organized ] $ LOCATION =[ The Azure region of your Kubernetes cluster. e.g. \"West US2\" ] # Needs to be globally unique across the entire Azure location $ STORAGE_ACCOUNT =[ The name of the storage account where your data will be stored ] # Needs to end in a \".vhd\" extension $ STORAGE_NAME = pach-disk.vhd # We recommend between 1 and 10 GB. This stores PFS metadata. For reference 1GB # should work for 1000 commits on 1000 files. $ STORAGE_SIZE =[ the size of the data disk volume that you are going to create, in GBs. e.g. \"10\" ] And then run: # Create a resource group $ az group create --name = ${ RESOURCE_GROUP } --location = ${ LOCATION } # Create azure storage account az storage account create \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --location = \" ${ LOCATION } \" \\ --sku = Standard_LRS \\ --name = \" ${ STORAGE_ACCOUNT } \" \\ --kind = Storage # Build microsoft tool for creating Azure VMs from an image $ STORAGE_KEY = \" $( az storage account keys list \\ --account-name = \" ${ STORAGE_ACCOUNT } \" \\ --resource-group = \" ${ RESOURCE_GROUP } \" \\ --output = json \\ | jq . [ 0 ] .value -r ) \" $ make docker-build-microsoft-vhd $ VOLUME_URI = \" $( docker run -it microsoft_vhd \\ \" ${ STORAGE_ACCOUNT } \" \\ \" ${ STORAGE_KEY } \" \\ \" ${ CONTAINER_NAME } \" \\ \" ${ STORAGE_NAME } \" \\ \" ${ STORAGE_SIZE } G\" ) \" To check that everything has been setup correctly, try: $ az storage account list | jq '.[].name' The we can deploy Pachyderm: pachctl deploy custom --persistent-disk azure --object-store s3 ${ VOLUME_URI } ${ STORAGE_SIZE } <object store bucket> <object store id> <object store secret> <object store endpoint> --static-etcd-volume = ${ VOLUME_URI }","title":"Azure + Custom Object Store"},{"location":"deploy-manage/deploy/docker_registries/","text":"Working With Docker Registries \u00b6 Coming soon. This document, when complete, will take you through on-premises, private Docker registry configuration.","title":"Working With Docker Registries"},{"location":"deploy-manage/deploy/docker_registries/#working-with-docker-registries","text":"Coming soon. This document, when complete, will take you through on-premises, private Docker registry configuration.","title":"Working With Docker Registries"},{"location":"deploy-manage/deploy/google_cloud_platform/","text":"Google Cloud Platform \u00b6 Google Cloud Platform provides seamless support for Kubernetes. Therefore, Pachyderm is fully supported on Google Kubernetes Engine (GKE). The following section walks you through deploying a Pachyderm cluster on GKE. Prerequisites \u00b6 Google Cloud SDK >= 124.0.0 kubectl pachctl Note When you follow the QuickStart Guide, you might update your ~/.bash_profile and point your $PATH at the location where you extracted google-cloud-sdk . However, Pachyderm recommends that you extract the SDK to ~/bin . Tip You can install kubectl by using the Google Cloud SDK and running the following command: $ gcloud components install kubectl Deploy Kubernetes \u00b6 To create a new Kubernetes cluster by using GKE, run: $ CLUSTER_NAME=<any unique name, e.g. \"pach-cluster\"> $ GCP_ZONE=<a GCP availability zone. e.g. \"us-west1-a\"> $ gcloud config set compute/zone ${GCP_ZONE} $ gcloud config set container/cluster ${CLUSTER_NAME} $ MACHINE_TYPE=<machine type for the k8s nodes, we recommend \"n1-standard-4\" or larger> # By default the following command spins up a 3-node cluster. You can change the default with `--num-nodes VAL`. $ gcloud container clusters create ${CLUSTER_NAME} --scopes storage-rw --machine-type ${MACHINE_TYPE} # By default, GKE clusters have RBAC enabled. To allow 'pachctl deploy' to give the 'pachyderm' service account # the requisite privileges via clusterrolebindings, you will need to grant *your user account* the privileges # needed to create those clusterrolebindings. # # Note that this command is simple and concise, but gives your user account more privileges than necessary. See # https://docs.pachyderm.io/en/latest/deployment/rbac.html for the complete list of privileges that the # pachyderm serviceaccount needs. $ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) Important You must create the Kubernetes cluster by using the gcloud command-line tool rather than the Google Cloud Console, as you can grant the storage-rw scope through the command-line tool only. This migth take a few minutes to start up. You can check the status on the GCP Console . A kubeconfig entry is automatically generated and set as the current context. As a sanity check, make sure your cluster is up and running by running the following kubectl command: # List all pods in the kube-system namespace. $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE event-exporter-v0.1.7-5c4d9556cf-fd9j2 2 /2 Running 0 1m fluentd-gcp-v2.0.9-68vhs 2 /2 Running 0 1m fluentd-gcp-v2.0.9-fzfpw 2 /2 Running 0 1m fluentd-gcp-v2.0.9-qvk8f 2 /2 Running 0 1m heapster-v1.4.3-5fbfb6bf55-xgdwx 3 /3 Running 0 55s kube-dns-778977457c-7hbrv 3 /3 Running 0 1m kube-dns-778977457c-dpff4 3 /3 Running 0 1m kube-dns-autoscaler-7db47cb9b7-gp5ns 1 /1 Running 0 1m kube-proxy-gke-pach-cluster-default-pool-9762dc84-bzcz 1 /1 Running 0 1m kube-proxy-gke-pach-cluster-default-pool-9762dc84-hqkr 1 /1 Running 0 1m kube-proxy-gke-pach-cluster-default-pool-9762dc84-jcbg 1 /1 Running 0 1m kubernetes-dashboard-768854d6dc-t75rp 1 /1 Running 0 1m l7-default-backend-6497bcdb4d-w72k5 1 /1 Running 0 1m If you don't see something similar to the above output, you can point kubectl to the new cluster manually by running the following command: # Update your kubeconfig to point at your newly created cluster. $ gcloud container clusters get-credentials ${ CLUSTER_NAME } Deploy Pachyderm \u00b6 To deploy Pachyderm we will need to: Create storage resources , Install the Pachyderm CLI tool, pachctl , and Deploy Pachyderm on the Kubernetes cluster Set up the Storage Resources \u00b6 Pachyderm needs a GCS bucket and a persistent disk to function correctly. You can specify the size of the persistent disk, the bucket name, and create the bucket by running the following commands: # For the persistent disk, 10GB is a good size to start with. # This stores PFS metadata. For reference, 1GB # should work fine for 1000 commits on 1000 files. $ STORAGE_SIZE = <the size of the volume that you are going to create, in GBs. e.g. \"10\" > # The Pachyderm bucket name needs to be globally unique across the entire GCP region. $ BUCKET_NAME = <The name of the GCS bucket where your data will be stored> # Create the bucket. $ gsutil mb gs:// ${ BUCKET_NAME } To check that everything has been set up correctly, run: $ gsutil ls # You should see the bucket you created. Install pachctl \u00b6 pachctl is a command-line utility for interacting with a Pachyderm cluster. You can install it locally as follows: # For macOS: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 # For Linux (64 bit) or Window 10+ on WSL: $ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_amd64.deb && sudo dpkg -i /tmp/pachctl.deb You can then run pachctl version --client-only to check that the installation was successful. $ pachctl version --client-only 1 .9.5 Deploy Pachyderm on the Kubernetes cluster \u00b6 Now you can deploy a Pachyderm cluster by running this command: $ pachctl deploy google ${ BUCKET_NAME } ${ STORAGE_SIZE } --dynamic-etcd-nodes = 1 serviceaccount \"pachyderm\" created storageclass \"etcd-storage-class\" created service \"etcd-headless\" created statefulset \"etcd\" created service \"etcd\" created service \"pachd\" created deployment \"pachd\" created service \"dash\" created deployment \"dash\" created secret \"pachyderm-storage-secret\" created Pachyderm is launching. Check its status with \"kubectl get all\" Once launched, access the dashboard by running \"pachctl port-forward\" Note Pachyderm uses one etcd node to manage Pachyderm metadata. Important If RBAC authorization is a requirement or you run into any RBAC errors see Configure RBAC . It may take a few minutes for the pachd nodes to be running because Pachyderm pulls containers from DockerHub. You can see the cluster status with kubectl , which should output the following when Pachyderm is up and running: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-482120938-np8cc 2 /2 Running 0 4m etcd-0 1 /1 Running 0 4m pachd-3677268306-9sqm0 1 /1 Running 0 4m If you see a few restarts on the pachd pod, you can safely ignore them. That simply means that Kubernetes tried to bring up those containers before other components were ready, so it restarted them. Finally, assuming your pachd is running as shown above, set up forward a port so that pachctl can talk to the cluster. # Forward the ports. We background this process because it blocks. $ pachctl port-forward & And you're done! You can test to make sure the cluster is working by running pachctl version or even creating a new repo. $ pachctl version COMPONENT VERSION pachctl 1 .9.5 pachd 1 .9.5 Increasing Ingress Throughput \u00b6 One way to improve Ingress performance is to restrict Pachd to a specific, more powerful node in the cluster. This is accomplished by the use of node-taints in GKE. By creating a node-taint for pachd , you configure the Kubernetes scheduler to run only the pachd pod on that node. After that\u2019s completed, you can deploy Pachyderm with the --pachd-cpu-request and --pachd-memory-request set to match the resources limits of the machine type. And finally, you need to modify the pachd deployment so that it has an appropriate toleration: tolerations: - key: \"dedicated\" operator: \"Equal\" value: \"pachd\" effect: \"NoSchedule\" Increasing upload performance \u00b6 The most straightfoward approach to increasing upload performance is to leverage SSD\u2019s as the boot disk in your cluster because SSDs provide higher throughput and lower latency than HDD disks. Additionally, you can increase the size of the SSD for further performance gains because the number of IOPS increases with disk size. Increasing merge performance \u00b6 Performance tweaks when it comes to merges can be done directly in the Pachyderm pipeline spec . More specifically, you can increase the number of hashtrees (hashtree spec) in the pipeline spec. This number determines the number of shards for the filesystem metadata. In general this number should be lower than the number of workers (parallelism spec) and should not be increased unless merge time (the time before the job is done and after the number of processed datums + skipped datums is equal to the total datums) is too slow.","title":"Deploy on GKE"},{"location":"deploy-manage/deploy/google_cloud_platform/#google-cloud-platform","text":"Google Cloud Platform provides seamless support for Kubernetes. Therefore, Pachyderm is fully supported on Google Kubernetes Engine (GKE). The following section walks you through deploying a Pachyderm cluster on GKE.","title":"Google Cloud Platform"},{"location":"deploy-manage/deploy/google_cloud_platform/#prerequisites","text":"Google Cloud SDK >= 124.0.0 kubectl pachctl Note When you follow the QuickStart Guide, you might update your ~/.bash_profile and point your $PATH at the location where you extracted google-cloud-sdk . However, Pachyderm recommends that you extract the SDK to ~/bin . Tip You can install kubectl by using the Google Cloud SDK and running the following command: $ gcloud components install kubectl","title":"Prerequisites"},{"location":"deploy-manage/deploy/google_cloud_platform/#deploy-kubernetes","text":"To create a new Kubernetes cluster by using GKE, run: $ CLUSTER_NAME=<any unique name, e.g. \"pach-cluster\"> $ GCP_ZONE=<a GCP availability zone. e.g. \"us-west1-a\"> $ gcloud config set compute/zone ${GCP_ZONE} $ gcloud config set container/cluster ${CLUSTER_NAME} $ MACHINE_TYPE=<machine type for the k8s nodes, we recommend \"n1-standard-4\" or larger> # By default the following command spins up a 3-node cluster. You can change the default with `--num-nodes VAL`. $ gcloud container clusters create ${CLUSTER_NAME} --scopes storage-rw --machine-type ${MACHINE_TYPE} # By default, GKE clusters have RBAC enabled. To allow 'pachctl deploy' to give the 'pachyderm' service account # the requisite privileges via clusterrolebindings, you will need to grant *your user account* the privileges # needed to create those clusterrolebindings. # # Note that this command is simple and concise, but gives your user account more privileges than necessary. See # https://docs.pachyderm.io/en/latest/deployment/rbac.html for the complete list of privileges that the # pachyderm serviceaccount needs. $ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) Important You must create the Kubernetes cluster by using the gcloud command-line tool rather than the Google Cloud Console, as you can grant the storage-rw scope through the command-line tool only. This migth take a few minutes to start up. You can check the status on the GCP Console . A kubeconfig entry is automatically generated and set as the current context. As a sanity check, make sure your cluster is up and running by running the following kubectl command: # List all pods in the kube-system namespace. $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE event-exporter-v0.1.7-5c4d9556cf-fd9j2 2 /2 Running 0 1m fluentd-gcp-v2.0.9-68vhs 2 /2 Running 0 1m fluentd-gcp-v2.0.9-fzfpw 2 /2 Running 0 1m fluentd-gcp-v2.0.9-qvk8f 2 /2 Running 0 1m heapster-v1.4.3-5fbfb6bf55-xgdwx 3 /3 Running 0 55s kube-dns-778977457c-7hbrv 3 /3 Running 0 1m kube-dns-778977457c-dpff4 3 /3 Running 0 1m kube-dns-autoscaler-7db47cb9b7-gp5ns 1 /1 Running 0 1m kube-proxy-gke-pach-cluster-default-pool-9762dc84-bzcz 1 /1 Running 0 1m kube-proxy-gke-pach-cluster-default-pool-9762dc84-hqkr 1 /1 Running 0 1m kube-proxy-gke-pach-cluster-default-pool-9762dc84-jcbg 1 /1 Running 0 1m kubernetes-dashboard-768854d6dc-t75rp 1 /1 Running 0 1m l7-default-backend-6497bcdb4d-w72k5 1 /1 Running 0 1m If you don't see something similar to the above output, you can point kubectl to the new cluster manually by running the following command: # Update your kubeconfig to point at your newly created cluster. $ gcloud container clusters get-credentials ${ CLUSTER_NAME }","title":"Deploy Kubernetes"},{"location":"deploy-manage/deploy/google_cloud_platform/#deploy-pachyderm","text":"To deploy Pachyderm we will need to: Create storage resources , Install the Pachyderm CLI tool, pachctl , and Deploy Pachyderm on the Kubernetes cluster","title":"Deploy Pachyderm"},{"location":"deploy-manage/deploy/google_cloud_platform/#set-up-the-storage-resources","text":"Pachyderm needs a GCS bucket and a persistent disk to function correctly. You can specify the size of the persistent disk, the bucket name, and create the bucket by running the following commands: # For the persistent disk, 10GB is a good size to start with. # This stores PFS metadata. For reference, 1GB # should work fine for 1000 commits on 1000 files. $ STORAGE_SIZE = <the size of the volume that you are going to create, in GBs. e.g. \"10\" > # The Pachyderm bucket name needs to be globally unique across the entire GCP region. $ BUCKET_NAME = <The name of the GCS bucket where your data will be stored> # Create the bucket. $ gsutil mb gs:// ${ BUCKET_NAME } To check that everything has been set up correctly, run: $ gsutil ls # You should see the bucket you created.","title":"Set up the Storage Resources"},{"location":"deploy-manage/deploy/google_cloud_platform/#install-pachctl","text":"pachctl is a command-line utility for interacting with a Pachyderm cluster. You can install it locally as follows: # For macOS: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 # For Linux (64 bit) or Window 10+ on WSL: $ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_amd64.deb && sudo dpkg -i /tmp/pachctl.deb You can then run pachctl version --client-only to check that the installation was successful. $ pachctl version --client-only 1 .9.5","title":"Install pachctl"},{"location":"deploy-manage/deploy/google_cloud_platform/#deploy-pachyderm-on-the-kubernetes-cluster","text":"Now you can deploy a Pachyderm cluster by running this command: $ pachctl deploy google ${ BUCKET_NAME } ${ STORAGE_SIZE } --dynamic-etcd-nodes = 1 serviceaccount \"pachyderm\" created storageclass \"etcd-storage-class\" created service \"etcd-headless\" created statefulset \"etcd\" created service \"etcd\" created service \"pachd\" created deployment \"pachd\" created service \"dash\" created deployment \"dash\" created secret \"pachyderm-storage-secret\" created Pachyderm is launching. Check its status with \"kubectl get all\" Once launched, access the dashboard by running \"pachctl port-forward\" Note Pachyderm uses one etcd node to manage Pachyderm metadata. Important If RBAC authorization is a requirement or you run into any RBAC errors see Configure RBAC . It may take a few minutes for the pachd nodes to be running because Pachyderm pulls containers from DockerHub. You can see the cluster status with kubectl , which should output the following when Pachyderm is up and running: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-482120938-np8cc 2 /2 Running 0 4m etcd-0 1 /1 Running 0 4m pachd-3677268306-9sqm0 1 /1 Running 0 4m If you see a few restarts on the pachd pod, you can safely ignore them. That simply means that Kubernetes tried to bring up those containers before other components were ready, so it restarted them. Finally, assuming your pachd is running as shown above, set up forward a port so that pachctl can talk to the cluster. # Forward the ports. We background this process because it blocks. $ pachctl port-forward & And you're done! You can test to make sure the cluster is working by running pachctl version or even creating a new repo. $ pachctl version COMPONENT VERSION pachctl 1 .9.5 pachd 1 .9.5","title":"Deploy Pachyderm on the Kubernetes cluster"},{"location":"deploy-manage/deploy/google_cloud_platform/#increasing-ingress-throughput","text":"One way to improve Ingress performance is to restrict Pachd to a specific, more powerful node in the cluster. This is accomplished by the use of node-taints in GKE. By creating a node-taint for pachd , you configure the Kubernetes scheduler to run only the pachd pod on that node. After that\u2019s completed, you can deploy Pachyderm with the --pachd-cpu-request and --pachd-memory-request set to match the resources limits of the machine type. And finally, you need to modify the pachd deployment so that it has an appropriate toleration: tolerations: - key: \"dedicated\" operator: \"Equal\" value: \"pachd\" effect: \"NoSchedule\"","title":"Increasing Ingress Throughput"},{"location":"deploy-manage/deploy/google_cloud_platform/#increasing-upload-performance","text":"The most straightfoward approach to increasing upload performance is to leverage SSD\u2019s as the boot disk in your cluster because SSDs provide higher throughput and lower latency than HDD disks. Additionally, you can increase the size of the SSD for further performance gains because the number of IOPS increases with disk size.","title":"Increasing upload performance"},{"location":"deploy-manage/deploy/google_cloud_platform/#increasing-merge-performance","text":"Performance tweaks when it comes to merges can be done directly in the Pachyderm pipeline spec . More specifically, you can increase the number of hashtrees (hashtree spec) in the pipeline spec. This number determines the number of shards for the filesystem metadata. In general this number should be lower than the number of workers (parallelism spec) and should not be increased unless merge time (the time before the job is done and after the number of processed datums + skipped datums is equal to the total datums) is too slow.","title":"Increasing merge performance"},{"location":"deploy-manage/deploy/namespaces/","text":"Non-Default Namespaces \u00b6 Often, production deploys of Pachyderm involve deploying Pachyderm to a non-default namespace. This helps administrators of the cluster more easily manage Pachyderm components alongside other things that might be running inside of Kubernetes (DataDog, TensorFlow Serving, etc.). To deploy Pachyderm to a non-default namespace, you just need to create that namespace with kubectl and then add the --namespace flag to your deploy command: $ kubectl create namespace pachyderm $ kubectl config set-context $(kubectl config current-context) --namespace=pachyderm $ pachctl deploy <args> --namespace pachyderm After the Pachyderm pods are up and running, you should see something similar to: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-68578d4bb4-mmtbj 2/2 Running 0 3m etcd-69fcfb5fcf-dgc8j 1/1 Running 0 3m pachd-784bdf7cd7-7dzxr 1/1 Running 0 3m","title":"Deploy in a Custom Namespace"},{"location":"deploy-manage/deploy/namespaces/#non-default-namespaces","text":"Often, production deploys of Pachyderm involve deploying Pachyderm to a non-default namespace. This helps administrators of the cluster more easily manage Pachyderm components alongside other things that might be running inside of Kubernetes (DataDog, TensorFlow Serving, etc.). To deploy Pachyderm to a non-default namespace, you just need to create that namespace with kubectl and then add the --namespace flag to your deploy command: $ kubectl create namespace pachyderm $ kubectl config set-context $(kubectl config current-context) --namespace=pachyderm $ pachctl deploy <args> --namespace pachyderm After the Pachyderm pods are up and running, you should see something similar to: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-68578d4bb4-mmtbj 2/2 Running 0 3m etcd-69fcfb5fcf-dgc8j 1/1 Running 0 3m pachd-784bdf7cd7-7dzxr 1/1 Running 0 3m","title":"Non-Default Namespaces"},{"location":"deploy-manage/deploy/non-cloud-object-stores/","text":"Deploying Pachyderm On-Premises With Non-Cloud Object Stores \u00b6 Coming soon. This document, when complete, will discuss common configurations for on-premises objects stores. General information on non-cloud object stores \u00b6 Please see the on-premises introduction to object stores for some general information on object stores and how they're used with Pachyderm. EMC ECS \u00b6 Coming soon. MinIO \u00b6 Coming soon. SwiftStack \u00b6 Coming soon. Notes \u00b6 S3 API Signature Algorithms and Regions \u00b6 The S3 protocol has two different ways of authenticating requests through its api. S3v2 has been officially deprecated by Amazon , so it's not likely that you'll run into it. You don't need to know the details of how they work (though you can follow these links, S3v2 & S3v4 , if you're curious), but you may run into issues with either mismatch of the signature method or availability regions. If you have trouble getting Pachyderm to run, check your Kubernetes logs for the pachd pod, Use kubectl get pods to find the name of the pachd pod and kubectl logs <pachd-pod-name> to get the logs. If you see an error beginning with INFO error starting grpc server pfs.NewBlockAPIServer It could have either of two causes. Availability Region Mismatch \u00b6 If the error is of the form INFO error starting grpc server pfs.NewBlockAPIServer: storage is unable to discern NotExist errors, \"The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'z1-a'\" should count as NotExist It may be a known issue with hardcoded region us-east-1 in the minio libraries. You can correct this by either using the --isS3V2 flag on your the pachctl deploy custom ... command or by changing the region of your storage to us-east-1 . Signature version mismatch \u00b6 You're not likely to run into this in an on-premises deployment unless your object store has deliberately been set up to use the deprecated S3v2 signature or you are running your on-premises deployment against Google Cloud Storage, which is not recommended. See Infrastructure in general . You'll need to determine what signature algorithm your object store uses in its S3-compatible API: S3v2 or S3v4 . If it's S3V2 , you can solve this by using the --isS3V2 flag on your the pachctl deploy custom ... command.","title":"Deploy a Custom Object Store"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#deploying-pachyderm-on-premises-with-non-cloud-object-stores","text":"Coming soon. This document, when complete, will discuss common configurations for on-premises objects stores.","title":"Deploying Pachyderm On-Premises With Non-Cloud Object Stores"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#general-information-on-non-cloud-object-stores","text":"Please see the on-premises introduction to object stores for some general information on object stores and how they're used with Pachyderm.","title":"General information on non-cloud object stores"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#emc-ecs","text":"Coming soon.","title":"EMC ECS"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#minio","text":"Coming soon.","title":"MinIO"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#swiftstack","text":"Coming soon.","title":"SwiftStack"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#notes","text":"","title":"Notes"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#s3-api-signature-algorithms-and-regions","text":"The S3 protocol has two different ways of authenticating requests through its api. S3v2 has been officially deprecated by Amazon , so it's not likely that you'll run into it. You don't need to know the details of how they work (though you can follow these links, S3v2 & S3v4 , if you're curious), but you may run into issues with either mismatch of the signature method or availability regions. If you have trouble getting Pachyderm to run, check your Kubernetes logs for the pachd pod, Use kubectl get pods to find the name of the pachd pod and kubectl logs <pachd-pod-name> to get the logs. If you see an error beginning with INFO error starting grpc server pfs.NewBlockAPIServer It could have either of two causes.","title":"S3 API Signature Algorithms and Regions"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#availability-region-mismatch","text":"If the error is of the form INFO error starting grpc server pfs.NewBlockAPIServer: storage is unable to discern NotExist errors, \"The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'z1-a'\" should count as NotExist It may be a known issue with hardcoded region us-east-1 in the minio libraries. You can correct this by either using the --isS3V2 flag on your the pachctl deploy custom ... command or by changing the region of your storage to us-east-1 .","title":"Availability Region Mismatch"},{"location":"deploy-manage/deploy/non-cloud-object-stores/#signature-version-mismatch","text":"You're not likely to run into this in an on-premises deployment unless your object store has deliberately been set up to use the deprecated S3v2 signature or you are running your on-premises deployment against Google Cloud Storage, which is not recommended. See Infrastructure in general . You'll need to determine what signature algorithm your object store uses in its S3-compatible API: S3v2 or S3v4 . If it's S3V2 , you can solve this by using the --isS3V2 flag on your the pachctl deploy custom ... command.","title":"Signature version mismatch"},{"location":"deploy-manage/deploy/on_premises/","text":"On Premises \u00b6 This document is broken down into the following sections, available at the links below Introduction to on-premises deployments takes you through what you need to know about Kubernetes, persistent volumes, object stores and best practices. That's this page. Customizing your Pachyderm deployment for on-premises use details the various options of the pachctl deploy custom ... command for an on-premises deployment. Single-node Pachyderm deployment is the document you should read when deploying Pachyderm for personal, low-volume usage. Registries takes you through on-premises, private Docker registry configuration. Ingress details the Kubernetes ingress configuration you'd need for using pachctl and the dashboard outside of the Kubernetes cluster Non-cloud object stores discusses common configurations for on-premises object stores. Need information on a particular flavor of Kubernetes or object store? Check out the see also section. Troubleshooting a deployment? Check out Troubleshooting Deployments . Introduction \u00b6 Deploying Pachyderm successfully on-premises requires a few prerequisites and some planning. Pachyderm is built on Kubernetes . Before you can deploy Pachyderm, you or your Kubernetes administrator will need to perform the following actions: Deploy Kubernetes on-premises. Deploy a Kubernetes persistent volume that Pachyderm will use to store administrative data. Deploy an on-premises object store using a storage provider like MinIO , EMC's ECS , or SwiftStack to provide S3-compatible access to your on-premises storage. Create a Pachyderm manifest by running the pachctl deploy custom command with appropriate arguments and the --dry-run flag to create a Kubernetes manifest for the Pachyderm deployment. Edit the Pachyderm manifest for your particular Kubernetes deployment In this series of documents, we'll take you through the steps unique to Pachyderm. We assume you have some Kubernetes knowledge. We will point you to external resources for the general Kubernetes steps to give you background. Best practices \u00b6 Infrastructure as code \u00b6 We highly encourage you to apply the best practices used in developing software to managing the deployment process. Create scripts that automate as much of your processes as possible and keep them under version control. Keep copies of all artifacts, such as manifests, produced by those scripts and keep those under version control. Document your practices in the code and outside it. Infrastructure in general \u00b6 Be sure that you design your Kubernetes infrastructure in accordance with recommended guidelines. Don't mix on-premises Kubernetes and cloud-based storage. It's important that bandwidth to your storage deployment meet the guidelines of your storage provider. Prerequisites \u00b6 Software you will need \u00b6 kubectl pachctl Setting up to deploy on-premises \u00b6 Deploying Kubernetes \u00b6 The Kubernetes docs have instructions for deploying Kubernetes in a variety of on-premise scenarios . We recommend following one of these guides to get Kubernetes running on premise. Deploying a persistent volume \u00b6 Persistent volumes: how do they work? \u00b6 A Kubernetes persistent volume is used by Pachyderm's etcd for storage of system metatada. In Kubernetes, persistent volumes are a mechanism for providing storage for consumption by the users of the cluster. They are provisioned by the cluster administrators. In a typical enterprise Kubernetes deployment, the administrators have configured persistent volumes that your Pachyderm deployment will consume by means of a persistent volume claim in the Pachyderm manifest you generate. You can deploy PV's to Pachyderm using our command-line arguments in three ways: using a static PV, with StatefulSets, or with StatefulSets using a StorageClass. If your administrators are using selectors , or you want to use StorageClasses in a different way, you'll need to edit the Pachyderm manifest appropriately before applying it. Static PV \u00b6 In this case, etcd will be deployed in Pachyderm as a ReplicationController with one (1) pod that uses a static PV. This is a common deployment for testing. StatefulSets \u00b6 StatefulSets are a mechanism provided in Kubernetes 1.9 and newer to manage the deployment and scaling of applications. It uses either Persistent Volume Provisioning or pre-provisioned PV's. If you're using StatefulSets in your Kubernetes cluster, you will need to find out the particulars of your cluster's PV configuration and use appropriate flags to pachctl deploy custom StorageClasses \u00b6 If your administrators require specification of classes to consume persistent volumes, you will need to find out the particulars of your cluster's PV configuration and use appropriate flags to pachctl deploy custom . Common tasks to all types of PV deployments \u00b6 Sizing the PV \u00b6 You'll need to use a PV with enough space for the metadata associated with the data you plan to store in Pachyderm. We're currently developing good rules of thumb for scaling this storage as your Pachyderm deployment grows, but it looks like 10G of disk space is sufficient for most purposes. Creating the PV \u00b6 In the case of cloud-based deployments, the pachctl deploy command for AWS, GCP and Azure creates persistent volumes for you, when you follow the instructions for those infrastructures. In the case of on-premises deployments, the kind of PV you provision will be dependent on what kind of storage your Kubernetes administrators have attached to your cluster and configured, and whether you are expected to consume that storage as a static PV, with Persistent Volume Provisioning or as a StorageClass. For example, many on-premises deployments use Network File System (NFS) to access to some kind of enterprise storage. Persistent volumes are provisioned in Kubernetes like all things in Kubernetes: by means of a manifest. You can learn about creating volumes and persistent volumes in the Kubernetes documentation. You or your Kubernetes administrators will be responsible for configuring the PVs you create to be consumable as static PV's, with Persistent Volume Provisioning or as a StorageClass. What you'll need for Pachyderm configuration of PV storage \u00b6 Keep the information below at hand for when you run pachctl deploy custom further on Configuring with static volumes \u00b6 You'll need the name of the PV and the amount of space you can use, in gigabytes. We'll refer to those, respectively, as PVC_STORAGE_NAME and PVC_STORAGE_SIZE further on. With this kind of PV, you'll use the flag --static-etcd-volume with PVC_STORAGE_NAME as its argument in your deployment. Note: this will override any attempt to configure with StorageClasses, below. Configuring with StatefulSets \u00b6 If you're deploying using StatefulSets , you'll just need the amount of space you can use, in gigabytes, which we'll refer to as PVC_STORAGE_SIZE further on.. Note: The --etcd-storage-class flag and argument will be ignored if you use the flag --static-etcd-volume along with it. Configuring with StatefulSets using StorageClasses \u00b6 If you're deploying using StatefulSets with StorageClasses , you'll need the name of the storage class and the amount of space you can use, in gigabytes. We'll refer to those, respectively, as PVC_STORAGECLASS and PVC_STORAGE_SIZE further on. With this kind of PV, you'll use the flag --etcd-storage-class with PVC_STORAGECLASS as its argument in your deployment. Note: The --etcd-storage-class flag and argument will be ignored if you use the flag --static-etcd-volume along with it. Deploying an object store \u00b6 Object store: what's it for? \u00b6 An object store is used by Pachyderm's pachd for storing all your data. The object store you use must be accessible via a low-latency, high-bandwidth connection like Gigabit or 10G Ethernet . For an on-premises deployment, it's not advisable to use a cloud-based storage mechanism. Don't deploy an on-premises Pachyderm cluster against cloud-based object stores such as S3 from AWS , GCS from Google Cloud Platform , Azure Blob Storage from Azure . Object store prerequisites \u00b6 Object stores are accessible using the S3 protocol, created by Amazon. Storage providers like MinIO , EMC's ECS , or SwiftStack provide S3-compatible access to enterprise storage for on-premises deployment. You can find links to instructions for providers of particular object stores in the See also section. Sizing the object store \u00b6 Size your object store generously. Once you start using Pachyderm, you'll start versioning all your data. We're currently developing good rules of thumb for scaling your object store as your Pachyderm deployment grows, but it's a good idea to start with a large multiple of your current data set size. What you'll need for Pachyderm configuration of the object store \u00b6 You'll need four items to configure the object store. We're prefixing each item with how we'll refer to it further on. OS_ENDPOINT : The access endpoint. For example, MinIO's endpoints are usually something like minio-server:9000 . Don't begin it with the protocol; it's an endpoint, not an url. OS_BUCKET_NAME : The bucket name you're dedicating to Pachyderm. Pachyderm will need exclusive access to this bucket. OS_ACCESS_KEY_ID : The access key id for the object store. This is like a user name for logging into the object store. OS_SECRET_KEY : The secret key for the object store. This is like the above user's password. Keep this information handy. Next step: creating a custom deploy manifest for Pachyderm \u00b6 Once you have Kubernetes deployed, your persistent volume created, and your object store configured, it's time to create the Pachyderm manifest for deploying to Kubernetes . See Also \u00b6 Kubernetes variants \u00b6 OpenShift Object storage variants \u00b6 EMC ECS MinIO SwiftStack","title":"Deploy On-Premises"},{"location":"deploy-manage/deploy/on_premises/#on-premises","text":"This document is broken down into the following sections, available at the links below Introduction to on-premises deployments takes you through what you need to know about Kubernetes, persistent volumes, object stores and best practices. That's this page. Customizing your Pachyderm deployment for on-premises use details the various options of the pachctl deploy custom ... command for an on-premises deployment. Single-node Pachyderm deployment is the document you should read when deploying Pachyderm for personal, low-volume usage. Registries takes you through on-premises, private Docker registry configuration. Ingress details the Kubernetes ingress configuration you'd need for using pachctl and the dashboard outside of the Kubernetes cluster Non-cloud object stores discusses common configurations for on-premises object stores. Need information on a particular flavor of Kubernetes or object store? Check out the see also section. Troubleshooting a deployment? Check out Troubleshooting Deployments .","title":"On Premises"},{"location":"deploy-manage/deploy/on_premises/#introduction","text":"Deploying Pachyderm successfully on-premises requires a few prerequisites and some planning. Pachyderm is built on Kubernetes . Before you can deploy Pachyderm, you or your Kubernetes administrator will need to perform the following actions: Deploy Kubernetes on-premises. Deploy a Kubernetes persistent volume that Pachyderm will use to store administrative data. Deploy an on-premises object store using a storage provider like MinIO , EMC's ECS , or SwiftStack to provide S3-compatible access to your on-premises storage. Create a Pachyderm manifest by running the pachctl deploy custom command with appropriate arguments and the --dry-run flag to create a Kubernetes manifest for the Pachyderm deployment. Edit the Pachyderm manifest for your particular Kubernetes deployment In this series of documents, we'll take you through the steps unique to Pachyderm. We assume you have some Kubernetes knowledge. We will point you to external resources for the general Kubernetes steps to give you background.","title":"Introduction"},{"location":"deploy-manage/deploy/on_premises/#best-practices","text":"","title":"Best practices"},{"location":"deploy-manage/deploy/on_premises/#infrastructure-as-code","text":"We highly encourage you to apply the best practices used in developing software to managing the deployment process. Create scripts that automate as much of your processes as possible and keep them under version control. Keep copies of all artifacts, such as manifests, produced by those scripts and keep those under version control. Document your practices in the code and outside it.","title":"Infrastructure as code"},{"location":"deploy-manage/deploy/on_premises/#infrastructure-in-general","text":"Be sure that you design your Kubernetes infrastructure in accordance with recommended guidelines. Don't mix on-premises Kubernetes and cloud-based storage. It's important that bandwidth to your storage deployment meet the guidelines of your storage provider.","title":"Infrastructure in general"},{"location":"deploy-manage/deploy/on_premises/#prerequisites","text":"","title":"Prerequisites"},{"location":"deploy-manage/deploy/on_premises/#software-you-will-need","text":"kubectl pachctl","title":"Software you will need"},{"location":"deploy-manage/deploy/on_premises/#setting-up-to-deploy-on-premises","text":"","title":"Setting up to deploy on-premises"},{"location":"deploy-manage/deploy/on_premises/#deploying-kubernetes","text":"The Kubernetes docs have instructions for deploying Kubernetes in a variety of on-premise scenarios . We recommend following one of these guides to get Kubernetes running on premise.","title":"Deploying Kubernetes"},{"location":"deploy-manage/deploy/on_premises/#deploying-a-persistent-volume","text":"","title":"Deploying a persistent volume"},{"location":"deploy-manage/deploy/on_premises/#persistent-volumes-how-do-they-work","text":"A Kubernetes persistent volume is used by Pachyderm's etcd for storage of system metatada. In Kubernetes, persistent volumes are a mechanism for providing storage for consumption by the users of the cluster. They are provisioned by the cluster administrators. In a typical enterprise Kubernetes deployment, the administrators have configured persistent volumes that your Pachyderm deployment will consume by means of a persistent volume claim in the Pachyderm manifest you generate. You can deploy PV's to Pachyderm using our command-line arguments in three ways: using a static PV, with StatefulSets, or with StatefulSets using a StorageClass. If your administrators are using selectors , or you want to use StorageClasses in a different way, you'll need to edit the Pachyderm manifest appropriately before applying it.","title":"Persistent volumes: how do they work?"},{"location":"deploy-manage/deploy/on_premises/#static-pv","text":"In this case, etcd will be deployed in Pachyderm as a ReplicationController with one (1) pod that uses a static PV. This is a common deployment for testing.","title":"Static PV"},{"location":"deploy-manage/deploy/on_premises/#statefulsets","text":"StatefulSets are a mechanism provided in Kubernetes 1.9 and newer to manage the deployment and scaling of applications. It uses either Persistent Volume Provisioning or pre-provisioned PV's. If you're using StatefulSets in your Kubernetes cluster, you will need to find out the particulars of your cluster's PV configuration and use appropriate flags to pachctl deploy custom","title":"StatefulSets"},{"location":"deploy-manage/deploy/on_premises/#storageclasses","text":"If your administrators require specification of classes to consume persistent volumes, you will need to find out the particulars of your cluster's PV configuration and use appropriate flags to pachctl deploy custom .","title":"StorageClasses"},{"location":"deploy-manage/deploy/on_premises/#common-tasks-to-all-types-of-pv-deployments","text":"","title":"Common tasks to all types of PV deployments"},{"location":"deploy-manage/deploy/on_premises/#sizing-the-pv","text":"You'll need to use a PV with enough space for the metadata associated with the data you plan to store in Pachyderm. We're currently developing good rules of thumb for scaling this storage as your Pachyderm deployment grows, but it looks like 10G of disk space is sufficient for most purposes.","title":"Sizing the PV"},{"location":"deploy-manage/deploy/on_premises/#creating-the-pv","text":"In the case of cloud-based deployments, the pachctl deploy command for AWS, GCP and Azure creates persistent volumes for you, when you follow the instructions for those infrastructures. In the case of on-premises deployments, the kind of PV you provision will be dependent on what kind of storage your Kubernetes administrators have attached to your cluster and configured, and whether you are expected to consume that storage as a static PV, with Persistent Volume Provisioning or as a StorageClass. For example, many on-premises deployments use Network File System (NFS) to access to some kind of enterprise storage. Persistent volumes are provisioned in Kubernetes like all things in Kubernetes: by means of a manifest. You can learn about creating volumes and persistent volumes in the Kubernetes documentation. You or your Kubernetes administrators will be responsible for configuring the PVs you create to be consumable as static PV's, with Persistent Volume Provisioning or as a StorageClass.","title":"Creating the PV"},{"location":"deploy-manage/deploy/on_premises/#what-youll-need-for-pachyderm-configuration-of-pv-storage","text":"Keep the information below at hand for when you run pachctl deploy custom further on","title":"What you'll need for Pachyderm configuration of PV storage"},{"location":"deploy-manage/deploy/on_premises/#configuring-with-static-volumes","text":"You'll need the name of the PV and the amount of space you can use, in gigabytes. We'll refer to those, respectively, as PVC_STORAGE_NAME and PVC_STORAGE_SIZE further on. With this kind of PV, you'll use the flag --static-etcd-volume with PVC_STORAGE_NAME as its argument in your deployment. Note: this will override any attempt to configure with StorageClasses, below.","title":"Configuring with static volumes"},{"location":"deploy-manage/deploy/on_premises/#configuring-with-statefulsets","text":"If you're deploying using StatefulSets , you'll just need the amount of space you can use, in gigabytes, which we'll refer to as PVC_STORAGE_SIZE further on.. Note: The --etcd-storage-class flag and argument will be ignored if you use the flag --static-etcd-volume along with it.","title":"Configuring with StatefulSets"},{"location":"deploy-manage/deploy/on_premises/#configuring-with-statefulsets-using-storageclasses","text":"If you're deploying using StatefulSets with StorageClasses , you'll need the name of the storage class and the amount of space you can use, in gigabytes. We'll refer to those, respectively, as PVC_STORAGECLASS and PVC_STORAGE_SIZE further on. With this kind of PV, you'll use the flag --etcd-storage-class with PVC_STORAGECLASS as its argument in your deployment. Note: The --etcd-storage-class flag and argument will be ignored if you use the flag --static-etcd-volume along with it.","title":"Configuring with StatefulSets using StorageClasses"},{"location":"deploy-manage/deploy/on_premises/#deploying-an-object-store","text":"","title":"Deploying an object store"},{"location":"deploy-manage/deploy/on_premises/#object-store-whats-it-for","text":"An object store is used by Pachyderm's pachd for storing all your data. The object store you use must be accessible via a low-latency, high-bandwidth connection like Gigabit or 10G Ethernet . For an on-premises deployment, it's not advisable to use a cloud-based storage mechanism. Don't deploy an on-premises Pachyderm cluster against cloud-based object stores such as S3 from AWS , GCS from Google Cloud Platform , Azure Blob Storage from Azure .","title":"Object store: what's it for?"},{"location":"deploy-manage/deploy/on_premises/#object-store-prerequisites","text":"Object stores are accessible using the S3 protocol, created by Amazon. Storage providers like MinIO , EMC's ECS , or SwiftStack provide S3-compatible access to enterprise storage for on-premises deployment. You can find links to instructions for providers of particular object stores in the See also section.","title":"Object store prerequisites"},{"location":"deploy-manage/deploy/on_premises/#sizing-the-object-store","text":"Size your object store generously. Once you start using Pachyderm, you'll start versioning all your data. We're currently developing good rules of thumb for scaling your object store as your Pachyderm deployment grows, but it's a good idea to start with a large multiple of your current data set size.","title":"Sizing the object store"},{"location":"deploy-manage/deploy/on_premises/#what-youll-need-for-pachyderm-configuration-of-the-object-store","text":"You'll need four items to configure the object store. We're prefixing each item with how we'll refer to it further on. OS_ENDPOINT : The access endpoint. For example, MinIO's endpoints are usually something like minio-server:9000 . Don't begin it with the protocol; it's an endpoint, not an url. OS_BUCKET_NAME : The bucket name you're dedicating to Pachyderm. Pachyderm will need exclusive access to this bucket. OS_ACCESS_KEY_ID : The access key id for the object store. This is like a user name for logging into the object store. OS_SECRET_KEY : The secret key for the object store. This is like the above user's password. Keep this information handy.","title":"What you'll need for Pachyderm configuration of the object store"},{"location":"deploy-manage/deploy/on_premises/#next-step-creating-a-custom-deploy-manifest-for-pachyderm","text":"Once you have Kubernetes deployed, your persistent volume created, and your object store configured, it's time to create the Pachyderm manifest for deploying to Kubernetes .","title":"Next step: creating a custom deploy manifest for Pachyderm"},{"location":"deploy-manage/deploy/on_premises/#see-also","text":"","title":"See Also"},{"location":"deploy-manage/deploy/on_premises/#kubernetes-variants","text":"OpenShift","title":"Kubernetes variants"},{"location":"deploy-manage/deploy/on_premises/#object-storage-variants","text":"EMC ECS MinIO SwiftStack","title":"Object storage variants"},{"location":"deploy-manage/deploy/openshift/","text":"OpenShift \u00b6 OpenShift is a popular enterprise Kubernetes distribution. Pachyderm can run on OpenShift with a few small tweaks in the deployment process, which will be outlined below. Please see known issues below for currently issues with OpenShift deployments. Prerequisites \u00b6 Pachyderm needs a few things to install and run successfully in any Kubernetes environment A persistent volume, used by Pachyderm's etcd for storage of system metatada. The kind of PV you provision will be dependent on your infrastructure. For example, many on-premises deployments use Network File System (NFS) access to some kind of enterprise storage. An object store, used by Pachyderm's pachd for storing all your data. The object store you use will probably be dependent on where you're going to run OpenShift: S3 for AWS , GCS for Google Cloud Platform , Azure Blob Storage for Azure , or a storage provider like Minio, EMC's ECS or Swift providing S3-compatible access to enterprise storage for on-premises deployment. Access to particular TCP/IP ports for communication. Persistent volume \u00b6 You'll need to create a persistent volume with enough space for the metadata associated with the data you plan to store Pachyderm. The pachctl deploy command for AWS, GCP and Azure creates persistent storage for you, when you follow the instructions below. A custom deploy can also create storage. We'll show you below how to take out the PV that's automatically created, in case you want to create it outside of the Pachyderm deployment and just consume it. We're currently developing good rules of thumb for scaling this storage as your Pachyderm deployment grows, but it looks like 10G of disk space is sufficient for most purposes. Object store \u00b6 Size your object store generously, once you start using Pachyderm, you'll start versioning all your data. You'll need four items to configure object storage The access endpoint. For example, Minio's endpoints are usually something like minio-server:9000 . Don't begin it with the protocol; it's an endpoint, not an url. The bucket name you're dedicating to Pachyderm. Pachyderm will need exclusive access to this bucket. The access key id for the object store. This is like a user name for logging into the object store. The secret key for the object store. This is like the above user's password. TCP/IP ports \u00b6 For more details on how Kubernetes networking and service definitions work, see the Kubernetes services documentation . Incoming ports (port) \u00b6 These are the ports internal to the containers, You'll find these on both the pachd and dash containers. OpenShift runs containers and pods as unprivileged users which don't have access to port numbers below 1024. Pachyderm's default manifests use ports below 1024, so you'll have to modify the manifests to use other port numbers. It's usually as easy as adding a \"1\" in front of the port numbers we use. Pod ports (targetPort) \u00b6 This is the port exposed by the pod to Kubernetes, which is forwarded to the port . You should leave the targetPort set at 0 so it will match the port definition. External ports (nodePorts) \u00b6 This is the port accessible from outside of Kubernetes. You probably don't need to change nodePort values unless your network security requirements or architecture requires you to change to another method of access. Please see the Kubernetes services documentation for details. The OCPify script \u00b6 A bash script that automates many of the substitutions below is available at this gist . You can use it to modify a manifest created using the --dry-run flag to pachctl deploy custom , as detailed below, and then use this guide to ensure the modifications it makes are relevant to your OpenShift environment. It requires certain prerequisites, just as jq and sponge, found in moreutils . This script may be useful as a basis for automating redeploys of Pachyderm as needed. Best practices: Infrastructure as code \u00b6 We highly encourage you to apply the best practices used in developing software to managing the deployment process. Create scripts that automate as much of your processes as possible and keep them under version control. Keep copies of all artifacts, such as manifests, produced by those scripts and keep those under version control. Document your practices in the code and outside it. Preparing to deploy Pachyderm \u00b6 Things you'll need 1. Your PV. It can be created separately. Your object store information. Your project in OpenShift. A text editor for editing your deployment manifest. Deploying Pachyderm \u00b6 1. Setting up PV and object stores \u00b6 How you deploy Pachyderm on OpenShift is largely going to depend on where OpenShift is deployed. Below you'll find links to the documentation for each kind of deployment you can do. Follow the instructions there for setting up persistent volumes and object storage resources. Don't yet deploy your manifest, come back here after you've set up your PV and object store. * OpenShift Deployed on AWS * OpenShift Deployed on GCP * OpenShift Deployed on Azure * OpenShift Deployed on-premise 2. Determine your role security policy \u00b6 Pachyderm is deployed by default with cluster roles. Many institutional Openshift security policies require namespace-local roles rather than cluster roles. If your security policies require namespace-local roles, use the pachctl deploy command below with the --local-roles flag . 3. Run the deploy command with --dry-run \u00b6 Once you have your PV, object store, and project, you can create a manifest for editing using the --dry-run argument to pachctl deploy . That step is detailed in the deployment instructions for each type of deployment, above. Below, find examples, with cluster roles and with namespace-local roles, using AWS elastic block storage as a persistent disk with a custom deploy. We'll show how to remove this PV in case you want to use a PV you create separately. Cluster roles \u00b6 $ pachctl deploy custom --persistent-disk aws --object-store s3 \\ <pv-storage-name> <pv-storage-size> \\ <s3-bucket-name> <s3-access-key-id> <s3-access-secret-key> <s3-access-endpoint-url> \\ --static-etcd-volume=<pv-storage-name> > manifest.json Namespace-local roles \u00b6 $ pachctl deploy custom --persistent-disk aws --object-store s3 \\ <pv-storage-name> <pv-storage-size> \\ <s3-bucket-name> <s3-access-key-id> <s3-access-secret-key> <s3-access-endpoint-url> \\ --static-etcd-volume=<pv-storage-name> --local-roles > manifest.json 4. Modify pachd Service ports \u00b6 In the deployment manifest, which we called manifest.json , above, find the stanza for the pachd Service. An example is shown below. { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"pachd\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" }, \"annotations\": { \"prometheus.io/port\": \"9091\", \"prometheus.io/scrape\": \"true\" } }, \"spec\": { \"ports\": [ { \"name\": \"api-grpc-port\", \"port\": 650, \"targetPort\": 0, \"nodePort\": 30650 }, { \"name\": \"trace-port\", \"port\": 651, \"targetPort\": 0, \"nodePort\": 30651 }, { \"name\": \"api-http-port\", \"port\": 652, \"targetPort\": 0, \"nodePort\": 30652 }, { \"name\": \"saml-port\", \"port\": 654, \"targetPort\": 0, \"nodePort\": 30654 }, { \"name\": \"api-git-port\", \"port\": 999, \"targetPort\": 0, \"nodePort\": 30999 }, { \"name\": \"s3gateway-port\", \"port\": 600, \"targetPort\": 0, \"nodePort\": 30600 } ], \"selector\": { \"app\": \"pachd\" }, \"type\": \"NodePort\" }, \"status\": { \"loadBalancer\": {} } } While the nodePort declarations are fine, the port declarations are too low for OpenShift. Good example values are shown below. \"spec\": { \"ports\": [ { \"name\": \"api-grpc-port\", \"port\": 1650, \"targetPort\": 0, \"nodePort\": 30650 }, { \"name\": \"trace-port\", \"port\": 1651, \"targetPort\": 0, \"nodePort\": 30651 }, { \"name\": \"api-http-port\", \"port\": 1652, \"targetPort\": 0, \"nodePort\": 30652 }, { \"name\": \"saml-port\", \"port\": 1654, \"targetPort\": 0, \"nodePort\": 30654 }, { \"name\": \"api-git-port\", \"port\": 1999, \"targetPort\": 0, \"nodePort\": 30999 }, { \"name\": \"s3gateway-port\", \"port\": 1600, \"targetPort\": 0, \"nodePort\": 30600 } ], 5. Modify pachd Deployment ports and add environment variables \u00b6 In this case you're editing two parts of the pachd Deployment json. Here, we'll omit the example of the unmodified version. Instead, we'll show you the modified version. 5.1 pachd Deployment ports \u00b6 The pachd Deployment also has a set of port numbers in the spec for the pachd container. Those must be modified to match the port numbers you set above for each port. { \"kind\": \"Deployment\", \"apiVersion\": \"apps/v1beta1\", \"metadata\": { \"name\": \"pachd\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" } }, \"spec\": { \"replicas\": 1, \"selector\": { \"matchLabels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" } }, \"template\": { \"metadata\": { \"name\": \"pachd\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" }, \"annotations\": { \"iam.amazonaws.com/role\": \"\" } }, \"spec\": { \"volumes\": [ { \"name\": \"pach-disk\" }, { \"name\": \"pachyderm-storage-secret\", \"secret\": { \"secretName\": \"pachyderm-storage-secret\" } } ], \"containers\": [ { \"name\": \"pachd\", \"image\": \"pachyderm/pachd:1.9.0rc1\", \"ports\": [ { \"name\": \"api-grpc-port\", \"containerPort\": 1650, \"protocol\": \"TCP\" }, { \"name\": \"trace-port\", \"containerPort\": 1651 }, { \"name\": \"api-http-port\", \"containerPort\": 1652, \"protocol\": \"TCP\" }, { \"name\": \"peer-port\", \"containerPort\": 1653, \"protocol\": \"TCP\" }, { \"name\": \"api-git-port\", \"containerPort\": 1999, \"protocol\": \"TCP\" }, { \"name\": \"saml-port\", \"containerPort\": 1654, \"protocol\": \"TCP\" } ], 5.2 Add environment variables \u00b6 There are six environment variables necessary for OpenShift 1. WORKER_USES_ROOT : This controls whether worker pipelines run as the root user or not. You'll need to set it to false 1. PORT : This is the grpc port used by pachd for communication with pachctl and the api. It should be set to the same value you set for api-grpc-port above. 1. PPROF_PORT : This is used for Prometheus. It should be set to the same value as trace-port above. 1. HTTP_PORT : The port for the api proxy. It should be set to api-http-port above. 1. PEER_PORT : Used to coordinate pachd 's. Same as peer-port above. 1. PPS_WORKER_GRPC_PORT : Used to talk to pipelines. Should be set to a value above 1024. The example value of 1680 below is recommended. The added values below are shown inserted above the PACH_ROOT value, which is typically the first value in this array. The rest of the stanza is omitted for clarity. \"env\": [ { \"name\": \"WORKER_USES_ROOT\", \"value\": \"false\" }, { \"name\": \"PORT\", \"value\": \"1650\" }, { \"name\": \"PPROF_PORT\", \"value\": \"1651\" }, { \"name\": \"HTTP_PORT\", \"value\": \"1652\" }, { \"name\": \"PEER_PORT\", \"value\": \"1653\" }, { \"name\": \"PPS_WORKER_GRPC_PORT\", \"value\": \"1680\" }, { \"name\": \"PACH_ROOT\", \"value\": \"/pach\" }, 6. (Optional) Remove the PV created during the deploy command \u00b6 If you're using a PV you've created separately, remove the PV that was added to your manifest by pachctl deploy --dry-run . Here's the example PV we created with the deploy command we used above, so you can recognize it. { \"kind\": \"PersistentVolume\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"etcd-volume\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"etcd\", \"suite\": \"pachyderm\" } }, \"spec\": { \"capacity\": { \"storage\": \"10Gi\" }, \"awsElasticBlockStore\": { \"volumeID\": \"pach-disk\", \"fsType\": \"ext4\" }, \"accessModes\": [ \"ReadWriteOnce\" ], \"persistentVolumeReclaimPolicy\": \"Retain\" }, \"status\": {} } 7. Deploy the Pachyderm manifest you modified. \u00b6 $ oc create -f pachyderm.json You can see the cluster status by using oc get pods as in upstream Kubernetes: $ oc get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-89dv9 2 /2 Running 0 1m etcd-0 1 /1 Running 0 4m pachd-65fd68d6d4-8vjq7 1 /1 Running 0 4m Known issues \u00b6 Problems related to OpenShift deployment are tracked in issues with the \"openshift\" label .","title":"Deploy on OpenShift"},{"location":"deploy-manage/deploy/openshift/#openshift","text":"OpenShift is a popular enterprise Kubernetes distribution. Pachyderm can run on OpenShift with a few small tweaks in the deployment process, which will be outlined below. Please see known issues below for currently issues with OpenShift deployments.","title":"OpenShift"},{"location":"deploy-manage/deploy/openshift/#prerequisites","text":"Pachyderm needs a few things to install and run successfully in any Kubernetes environment A persistent volume, used by Pachyderm's etcd for storage of system metatada. The kind of PV you provision will be dependent on your infrastructure. For example, many on-premises deployments use Network File System (NFS) access to some kind of enterprise storage. An object store, used by Pachyderm's pachd for storing all your data. The object store you use will probably be dependent on where you're going to run OpenShift: S3 for AWS , GCS for Google Cloud Platform , Azure Blob Storage for Azure , or a storage provider like Minio, EMC's ECS or Swift providing S3-compatible access to enterprise storage for on-premises deployment. Access to particular TCP/IP ports for communication.","title":"Prerequisites"},{"location":"deploy-manage/deploy/openshift/#persistent-volume","text":"You'll need to create a persistent volume with enough space for the metadata associated with the data you plan to store Pachyderm. The pachctl deploy command for AWS, GCP and Azure creates persistent storage for you, when you follow the instructions below. A custom deploy can also create storage. We'll show you below how to take out the PV that's automatically created, in case you want to create it outside of the Pachyderm deployment and just consume it. We're currently developing good rules of thumb for scaling this storage as your Pachyderm deployment grows, but it looks like 10G of disk space is sufficient for most purposes.","title":"Persistent volume"},{"location":"deploy-manage/deploy/openshift/#object-store","text":"Size your object store generously, once you start using Pachyderm, you'll start versioning all your data. You'll need four items to configure object storage The access endpoint. For example, Minio's endpoints are usually something like minio-server:9000 . Don't begin it with the protocol; it's an endpoint, not an url. The bucket name you're dedicating to Pachyderm. Pachyderm will need exclusive access to this bucket. The access key id for the object store. This is like a user name for logging into the object store. The secret key for the object store. This is like the above user's password.","title":"Object store"},{"location":"deploy-manage/deploy/openshift/#tcpip-ports","text":"For more details on how Kubernetes networking and service definitions work, see the Kubernetes services documentation .","title":"TCP/IP ports"},{"location":"deploy-manage/deploy/openshift/#incoming-ports-port","text":"These are the ports internal to the containers, You'll find these on both the pachd and dash containers. OpenShift runs containers and pods as unprivileged users which don't have access to port numbers below 1024. Pachyderm's default manifests use ports below 1024, so you'll have to modify the manifests to use other port numbers. It's usually as easy as adding a \"1\" in front of the port numbers we use.","title":"Incoming ports (port)"},{"location":"deploy-manage/deploy/openshift/#pod-ports-targetport","text":"This is the port exposed by the pod to Kubernetes, which is forwarded to the port . You should leave the targetPort set at 0 so it will match the port definition.","title":"Pod ports (targetPort)"},{"location":"deploy-manage/deploy/openshift/#external-ports-nodeports","text":"This is the port accessible from outside of Kubernetes. You probably don't need to change nodePort values unless your network security requirements or architecture requires you to change to another method of access. Please see the Kubernetes services documentation for details.","title":"External ports (nodePorts)"},{"location":"deploy-manage/deploy/openshift/#the-ocpify-script","text":"A bash script that automates many of the substitutions below is available at this gist . You can use it to modify a manifest created using the --dry-run flag to pachctl deploy custom , as detailed below, and then use this guide to ensure the modifications it makes are relevant to your OpenShift environment. It requires certain prerequisites, just as jq and sponge, found in moreutils . This script may be useful as a basis for automating redeploys of Pachyderm as needed.","title":"The OCPify script"},{"location":"deploy-manage/deploy/openshift/#best-practices-infrastructure-as-code","text":"We highly encourage you to apply the best practices used in developing software to managing the deployment process. Create scripts that automate as much of your processes as possible and keep them under version control. Keep copies of all artifacts, such as manifests, produced by those scripts and keep those under version control. Document your practices in the code and outside it.","title":"Best practices: Infrastructure as code"},{"location":"deploy-manage/deploy/openshift/#preparing-to-deploy-pachyderm","text":"Things you'll need 1. Your PV. It can be created separately. Your object store information. Your project in OpenShift. A text editor for editing your deployment manifest.","title":"Preparing to deploy Pachyderm"},{"location":"deploy-manage/deploy/openshift/#deploying-pachyderm","text":"","title":"Deploying Pachyderm"},{"location":"deploy-manage/deploy/openshift/#1-setting-up-pv-and-object-stores","text":"How you deploy Pachyderm on OpenShift is largely going to depend on where OpenShift is deployed. Below you'll find links to the documentation for each kind of deployment you can do. Follow the instructions there for setting up persistent volumes and object storage resources. Don't yet deploy your manifest, come back here after you've set up your PV and object store. * OpenShift Deployed on AWS * OpenShift Deployed on GCP * OpenShift Deployed on Azure * OpenShift Deployed on-premise","title":"1. Setting up PV and object stores"},{"location":"deploy-manage/deploy/openshift/#2-determine-your-role-security-policy","text":"Pachyderm is deployed by default with cluster roles. Many institutional Openshift security policies require namespace-local roles rather than cluster roles. If your security policies require namespace-local roles, use the pachctl deploy command below with the --local-roles flag .","title":"2. Determine your role security policy"},{"location":"deploy-manage/deploy/openshift/#3-run-the-deploy-command-with-dry-run","text":"Once you have your PV, object store, and project, you can create a manifest for editing using the --dry-run argument to pachctl deploy . That step is detailed in the deployment instructions for each type of deployment, above. Below, find examples, with cluster roles and with namespace-local roles, using AWS elastic block storage as a persistent disk with a custom deploy. We'll show how to remove this PV in case you want to use a PV you create separately.","title":"3. Run the deploy command with --dry-run"},{"location":"deploy-manage/deploy/openshift/#cluster-roles","text":"$ pachctl deploy custom --persistent-disk aws --object-store s3 \\ <pv-storage-name> <pv-storage-size> \\ <s3-bucket-name> <s3-access-key-id> <s3-access-secret-key> <s3-access-endpoint-url> \\ --static-etcd-volume=<pv-storage-name> > manifest.json","title":"Cluster roles"},{"location":"deploy-manage/deploy/openshift/#namespace-local-roles","text":"$ pachctl deploy custom --persistent-disk aws --object-store s3 \\ <pv-storage-name> <pv-storage-size> \\ <s3-bucket-name> <s3-access-key-id> <s3-access-secret-key> <s3-access-endpoint-url> \\ --static-etcd-volume=<pv-storage-name> --local-roles > manifest.json","title":"Namespace-local roles"},{"location":"deploy-manage/deploy/openshift/#4-modify-pachd-service-ports","text":"In the deployment manifest, which we called manifest.json , above, find the stanza for the pachd Service. An example is shown below. { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"pachd\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" }, \"annotations\": { \"prometheus.io/port\": \"9091\", \"prometheus.io/scrape\": \"true\" } }, \"spec\": { \"ports\": [ { \"name\": \"api-grpc-port\", \"port\": 650, \"targetPort\": 0, \"nodePort\": 30650 }, { \"name\": \"trace-port\", \"port\": 651, \"targetPort\": 0, \"nodePort\": 30651 }, { \"name\": \"api-http-port\", \"port\": 652, \"targetPort\": 0, \"nodePort\": 30652 }, { \"name\": \"saml-port\", \"port\": 654, \"targetPort\": 0, \"nodePort\": 30654 }, { \"name\": \"api-git-port\", \"port\": 999, \"targetPort\": 0, \"nodePort\": 30999 }, { \"name\": \"s3gateway-port\", \"port\": 600, \"targetPort\": 0, \"nodePort\": 30600 } ], \"selector\": { \"app\": \"pachd\" }, \"type\": \"NodePort\" }, \"status\": { \"loadBalancer\": {} } } While the nodePort declarations are fine, the port declarations are too low for OpenShift. Good example values are shown below. \"spec\": { \"ports\": [ { \"name\": \"api-grpc-port\", \"port\": 1650, \"targetPort\": 0, \"nodePort\": 30650 }, { \"name\": \"trace-port\", \"port\": 1651, \"targetPort\": 0, \"nodePort\": 30651 }, { \"name\": \"api-http-port\", \"port\": 1652, \"targetPort\": 0, \"nodePort\": 30652 }, { \"name\": \"saml-port\", \"port\": 1654, \"targetPort\": 0, \"nodePort\": 30654 }, { \"name\": \"api-git-port\", \"port\": 1999, \"targetPort\": 0, \"nodePort\": 30999 }, { \"name\": \"s3gateway-port\", \"port\": 1600, \"targetPort\": 0, \"nodePort\": 30600 } ],","title":"4. Modify pachd Service ports"},{"location":"deploy-manage/deploy/openshift/#5-modify-pachd-deployment-ports-and-add-environment-variables","text":"In this case you're editing two parts of the pachd Deployment json. Here, we'll omit the example of the unmodified version. Instead, we'll show you the modified version.","title":"5. Modify pachd Deployment ports and add environment variables"},{"location":"deploy-manage/deploy/openshift/#51-pachd-deployment-ports","text":"The pachd Deployment also has a set of port numbers in the spec for the pachd container. Those must be modified to match the port numbers you set above for each port. { \"kind\": \"Deployment\", \"apiVersion\": \"apps/v1beta1\", \"metadata\": { \"name\": \"pachd\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" } }, \"spec\": { \"replicas\": 1, \"selector\": { \"matchLabels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" } }, \"template\": { \"metadata\": { \"name\": \"pachd\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"pachd\", \"suite\": \"pachyderm\" }, \"annotations\": { \"iam.amazonaws.com/role\": \"\" } }, \"spec\": { \"volumes\": [ { \"name\": \"pach-disk\" }, { \"name\": \"pachyderm-storage-secret\", \"secret\": { \"secretName\": \"pachyderm-storage-secret\" } } ], \"containers\": [ { \"name\": \"pachd\", \"image\": \"pachyderm/pachd:1.9.0rc1\", \"ports\": [ { \"name\": \"api-grpc-port\", \"containerPort\": 1650, \"protocol\": \"TCP\" }, { \"name\": \"trace-port\", \"containerPort\": 1651 }, { \"name\": \"api-http-port\", \"containerPort\": 1652, \"protocol\": \"TCP\" }, { \"name\": \"peer-port\", \"containerPort\": 1653, \"protocol\": \"TCP\" }, { \"name\": \"api-git-port\", \"containerPort\": 1999, \"protocol\": \"TCP\" }, { \"name\": \"saml-port\", \"containerPort\": 1654, \"protocol\": \"TCP\" } ],","title":"5.1 pachd Deployment ports"},{"location":"deploy-manage/deploy/openshift/#52-add-environment-variables","text":"There are six environment variables necessary for OpenShift 1. WORKER_USES_ROOT : This controls whether worker pipelines run as the root user or not. You'll need to set it to false 1. PORT : This is the grpc port used by pachd for communication with pachctl and the api. It should be set to the same value you set for api-grpc-port above. 1. PPROF_PORT : This is used for Prometheus. It should be set to the same value as trace-port above. 1. HTTP_PORT : The port for the api proxy. It should be set to api-http-port above. 1. PEER_PORT : Used to coordinate pachd 's. Same as peer-port above. 1. PPS_WORKER_GRPC_PORT : Used to talk to pipelines. Should be set to a value above 1024. The example value of 1680 below is recommended. The added values below are shown inserted above the PACH_ROOT value, which is typically the first value in this array. The rest of the stanza is omitted for clarity. \"env\": [ { \"name\": \"WORKER_USES_ROOT\", \"value\": \"false\" }, { \"name\": \"PORT\", \"value\": \"1650\" }, { \"name\": \"PPROF_PORT\", \"value\": \"1651\" }, { \"name\": \"HTTP_PORT\", \"value\": \"1652\" }, { \"name\": \"PEER_PORT\", \"value\": \"1653\" }, { \"name\": \"PPS_WORKER_GRPC_PORT\", \"value\": \"1680\" }, { \"name\": \"PACH_ROOT\", \"value\": \"/pach\" },","title":"5.2 Add environment variables"},{"location":"deploy-manage/deploy/openshift/#6-optional-remove-the-pv-created-during-the-deploy-command","text":"If you're using a PV you've created separately, remove the PV that was added to your manifest by pachctl deploy --dry-run . Here's the example PV we created with the deploy command we used above, so you can recognize it. { \"kind\": \"PersistentVolume\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"etcd-volume\", \"namespace\": \"default\", \"creationTimestamp\": null, \"labels\": { \"app\": \"etcd\", \"suite\": \"pachyderm\" } }, \"spec\": { \"capacity\": { \"storage\": \"10Gi\" }, \"awsElasticBlockStore\": { \"volumeID\": \"pach-disk\", \"fsType\": \"ext4\" }, \"accessModes\": [ \"ReadWriteOnce\" ], \"persistentVolumeReclaimPolicy\": \"Retain\" }, \"status\": {} }","title":"6. (Optional) Remove the PV created during the deploy command"},{"location":"deploy-manage/deploy/openshift/#7-deploy-the-pachyderm-manifest-you-modified","text":"$ oc create -f pachyderm.json You can see the cluster status by using oc get pods as in upstream Kubernetes: $ oc get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-89dv9 2 /2 Running 0 1m etcd-0 1 /1 Running 0 4m pachd-65fd68d6d4-8vjq7 1 /1 Running 0 4m","title":"7. Deploy the Pachyderm manifest you modified."},{"location":"deploy-manage/deploy/openshift/#known-issues","text":"Problems related to OpenShift deployment are tracked in issues with the \"openshift\" label .","title":"Known issues"},{"location":"deploy-manage/deploy/rbac/","text":"RBAC \u00b6 Pachyderm has support for Kubernetes Role-Based Access Controls (RBAC) and is a default part of all Pachyderm deployments. For most users, you shouldn't have any issues as Pachyderm takes care of setting all the RBAC permissions automatically. However, if you are deploying Pachyderm on a cluster that your company owns, security policies might not allow certain RBAC permissions by default. Therefore, it's suggested that you contact your Kubernetes admin and provide the following to ensure you don't encounter any permissions issues: Pachyderm Permission Requirements Rules: []rbacv1.PolicyRule{{ APIGroups: []string{\"\"}, Verbs: []string{\"get\", \"list\", \"watch\"}, Resources: []string{\"nodes\", \"pods\", \"pods/log\", \"endpoints\"}, }, { APIGroups: []string{\"\"}, Verbs: []string{\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\"}, Resources: []string{\"replicationcontrollers\", \"services\"}, }, { APIGroups: []string{\"\"}, Verbs: []string{\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\"}, Resources: []string{\"secrets\"}, ResourceNames: []string{client.StorageSecretName}, }}, RBAC and DNS \u00b6 Kubernetes currently (as of 1.8.0) has a bug that prevents kube-dns from working with RBAC. Not having DNS will make Pachyderm effectively unusable. You can tell if you're being affected by the bug like so: $ kubectl get all --namespace = kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 1 1 1 0 3m NAME DESIRED CURRENT READY AGE rs/kube-dns-86f6f55dd5 1 1 0 3m NAME READY STATUS RESTARTS AGE po/kube-addon-manager-oryx 1 /1 Running 0 3m po/kube-dns-86f6f55dd5-xksnb 2 /3 Running 4 3m po/kubernetes-dashboard-bzjjh 1 /1 Running 0 3m po/storage-provisioner 1 /1 Running 0 3m NAME DESIRED CURRENT READY AGE rc/kubernetes-dashboard 1 1 1 3m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE svc/kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP 3m svc/kubernetes-dashboard NodePort 10 .97.194.16 <none> 80 :30000/TCP 3m Notice how po/kubernetes-dashboard-bzjjh only has \u2154 pods ready and has 4 restarts. To fix this do: kubectl -n kube-system create sa kube-dns kubectl -n kube-system patch deploy/kube-dns -p '{\"spec\": {\"template\": {\"spec\": {\"serviceAccountName\": \"kube-dns\"}}}}' this will tell Kubernetes that kube-dns should use the appropriate ServiceAccount. Kubernetes creates the ServiceAccount, it just doesn't actually use it. RBAC Permissions on GKE \u00b6 If you're deploying Pachyderm on GKE and run into the following error: Error from server (Forbidden): error when creating \"STDIN\": clusterroles.rbac.authorization.k8s.io \"pachyderm\" is forbidden: attempt to grant extra privileges: Run the following and redeploy Pachyderm: kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)","title":"Configure RBAC"},{"location":"deploy-manage/deploy/rbac/#rbac","text":"Pachyderm has support for Kubernetes Role-Based Access Controls (RBAC) and is a default part of all Pachyderm deployments. For most users, you shouldn't have any issues as Pachyderm takes care of setting all the RBAC permissions automatically. However, if you are deploying Pachyderm on a cluster that your company owns, security policies might not allow certain RBAC permissions by default. Therefore, it's suggested that you contact your Kubernetes admin and provide the following to ensure you don't encounter any permissions issues: Pachyderm Permission Requirements Rules: []rbacv1.PolicyRule{{ APIGroups: []string{\"\"}, Verbs: []string{\"get\", \"list\", \"watch\"}, Resources: []string{\"nodes\", \"pods\", \"pods/log\", \"endpoints\"}, }, { APIGroups: []string{\"\"}, Verbs: []string{\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\"}, Resources: []string{\"replicationcontrollers\", \"services\"}, }, { APIGroups: []string{\"\"}, Verbs: []string{\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\"}, Resources: []string{\"secrets\"}, ResourceNames: []string{client.StorageSecretName}, }},","title":"RBAC"},{"location":"deploy-manage/deploy/rbac/#rbac-and-dns","text":"Kubernetes currently (as of 1.8.0) has a bug that prevents kube-dns from working with RBAC. Not having DNS will make Pachyderm effectively unusable. You can tell if you're being affected by the bug like so: $ kubectl get all --namespace = kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 1 1 1 0 3m NAME DESIRED CURRENT READY AGE rs/kube-dns-86f6f55dd5 1 1 0 3m NAME READY STATUS RESTARTS AGE po/kube-addon-manager-oryx 1 /1 Running 0 3m po/kube-dns-86f6f55dd5-xksnb 2 /3 Running 4 3m po/kubernetes-dashboard-bzjjh 1 /1 Running 0 3m po/storage-provisioner 1 /1 Running 0 3m NAME DESIRED CURRENT READY AGE rc/kubernetes-dashboard 1 1 1 3m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE svc/kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP 3m svc/kubernetes-dashboard NodePort 10 .97.194.16 <none> 80 :30000/TCP 3m Notice how po/kubernetes-dashboard-bzjjh only has \u2154 pods ready and has 4 restarts. To fix this do: kubectl -n kube-system create sa kube-dns kubectl -n kube-system patch deploy/kube-dns -p '{\"spec\": {\"template\": {\"spec\": {\"serviceAccountName\": \"kube-dns\"}}}}' this will tell Kubernetes that kube-dns should use the appropriate ServiceAccount. Kubernetes creates the ServiceAccount, it just doesn't actually use it.","title":"RBAC and DNS"},{"location":"deploy-manage/deploy/rbac/#rbac-permissions-on-gke","text":"If you're deploying Pachyderm on GKE and run into the following error: Error from server (Forbidden): error when creating \"STDIN\": clusterroles.rbac.authorization.k8s.io \"pachyderm\" is forbidden: attempt to grant extra privileges: Run the following and redeploy Pachyderm: kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)","title":"RBAC Permissions on GKE"},{"location":"deploy-manage/deploy/single-node/","text":"Single-node Pachyderm Deployments \u00b6 Coming soon. This document, when complete, will take you through deploying Pachyderm for personal, low-volume usage. This is distinct from pachctl deploy local , which is not suitable for anything beyond training and evaluation.","title":"Single-node Pachyderm Deployments"},{"location":"deploy-manage/deploy/single-node/#single-node-pachyderm-deployments","text":"Coming soon. This document, when complete, will take you through deploying Pachyderm for personal, low-volume usage. This is distinct from pachctl deploy local , which is not suitable for anything beyond training and evaluation.","title":"Single-node Pachyderm Deployments"},{"location":"deploy-manage/deploy/tracing/","text":"Configure Tracing with Jaeger \u00b6 Pachyderm has the ability to trace requests using Jaeger. This can be useful when diagnosing slow clusters. Collecting Traces \u00b6 To use tracing in Pachyderm, complete the following steps: Run Jaeger in Kubernetes kubectl apply -f etc/deploy/tracing/jaeger-all-in-one.yaml Point Pachyderm at Jaeger # For pachctl $ export JAEGER_ENDPOINT = localhost:14268 $ kubectl port-forward svc/jaeger-collector 14268 & # Collector service # For pachd $ kubectl delete po -l suite = pachyderm,app = pachd The port-forward command is necessary because pachctl sends traces to Jaeger (it actually initiates every trace), and reads the JAEGER_ENDPOINT environment variable for the address to which it will send the trace info. Restarting the pachd pod is necessary because pachd also sends trace information to Jaeger, but it reads the environment variables corresponding to the Jaeger service[1] on startup to find Jaeger (the Jaeger service is created by the jaeger-all-in-one.yaml manifest). Killing the pods restarts them, which causes them to connect to Jaeger. Send Pachyderm a traced request Just set the PACH_TRACE environment variable to \"true\" before running any pachctl command (note that JAEGER_ENDPOINT must also be set/exported): PACH_TRACE=true pachctl list job # for example We generally don't recommend exporting PACH_TRACE because tracing calls can slow them down somewhat and make interesting traces hard to find in Jaeger. Therefore you may only want to set this variable for the specific calls you want to trace. However, Pachyderm's client library reads this variable and implements the relevant tracing, so any binary that uses Pachyderm's go client library can trace calls if these variables are set. View Traces \u00b6 To view traces, run: $ kubectl port-forward svc/jaeger-query 16686:80 & # UI service then connect to localhost:16686 in your browser, and you should see all collected traces. See also: Kubernetes Service Environment Variables Troubleshooting \u00b6 If you see <trace-without-root-span> , this likely means that pachd has connected to Jaeger, but pachctl has not. Make sure that the JAEGER_ENDPOINT environment variable is set on your local machine, and that kubectl port-forward \"po/${jaeger_pod}\" 14268 is running. If you see a trace appear in Jaeger with no subtraces, like so: ...this likely means that pachd has not connected to Jaeger, but pachctl has. Make sure to restart the pachd pods after creating the Jaeger service in Kubernetes","title":"Configure Tracing with Jaeger"},{"location":"deploy-manage/deploy/tracing/#configure-tracing-with-jaeger","text":"Pachyderm has the ability to trace requests using Jaeger. This can be useful when diagnosing slow clusters.","title":"Configure Tracing with Jaeger"},{"location":"deploy-manage/deploy/tracing/#collecting-traces","text":"To use tracing in Pachyderm, complete the following steps: Run Jaeger in Kubernetes kubectl apply -f etc/deploy/tracing/jaeger-all-in-one.yaml Point Pachyderm at Jaeger # For pachctl $ export JAEGER_ENDPOINT = localhost:14268 $ kubectl port-forward svc/jaeger-collector 14268 & # Collector service # For pachd $ kubectl delete po -l suite = pachyderm,app = pachd The port-forward command is necessary because pachctl sends traces to Jaeger (it actually initiates every trace), and reads the JAEGER_ENDPOINT environment variable for the address to which it will send the trace info. Restarting the pachd pod is necessary because pachd also sends trace information to Jaeger, but it reads the environment variables corresponding to the Jaeger service[1] on startup to find Jaeger (the Jaeger service is created by the jaeger-all-in-one.yaml manifest). Killing the pods restarts them, which causes them to connect to Jaeger. Send Pachyderm a traced request Just set the PACH_TRACE environment variable to \"true\" before running any pachctl command (note that JAEGER_ENDPOINT must also be set/exported): PACH_TRACE=true pachctl list job # for example We generally don't recommend exporting PACH_TRACE because tracing calls can slow them down somewhat and make interesting traces hard to find in Jaeger. Therefore you may only want to set this variable for the specific calls you want to trace. However, Pachyderm's client library reads this variable and implements the relevant tracing, so any binary that uses Pachyderm's go client library can trace calls if these variables are set.","title":"Collecting Traces"},{"location":"deploy-manage/deploy/tracing/#view-traces","text":"To view traces, run: $ kubectl port-forward svc/jaeger-query 16686:80 & # UI service then connect to localhost:16686 in your browser, and you should see all collected traces. See also: Kubernetes Service Environment Variables","title":"View Traces"},{"location":"deploy-manage/deploy/tracing/#troubleshooting","text":"If you see <trace-without-root-span> , this likely means that pachd has connected to Jaeger, but pachctl has not. Make sure that the JAEGER_ENDPOINT environment variable is set on your local machine, and that kubectl port-forward \"po/${jaeger_pod}\" 14268 is running. If you see a trace appear in Jaeger with no subtraces, like so: ...this likely means that pachd has not connected to Jaeger, but pachctl has. Make sure to restart the pachd pods after creating the Jaeger service in Kubernetes","title":"Troubleshooting"},{"location":"deploy-manage/deploy/amazon_web_services/","text":"Deploy Pachyderm on Amazon AWS \u00b6 Pachyderm can run in a Kubernetes cluster deployed in Amazon\u00ae Web Services (AWS), whether it is an Elastic Container Service (EKS) or a Kubernetes cluster deployed directly on EC2 by using a deployment tool. AWS removes the need to maintain the underlying virtual cloud. This advantage makes AWS a logical choice for organizations that decide to offload the cloud infrastructure operational burden to a third-party vendor. Pachyderm seamlessly integrates with Amazon EKS and runs in the same fashion as on your computer. You can install Pachyderm on Amazon AWS by using one of the following options: Deploy Pachyderm on Amazon EKS If you already have an Amazon EKS cluster, you can quickly deploy Pachyderm on top of it. If you are just starting with Amazon EKS, this section guides you through the EKS deployment process. Deploy Pachyderm on Amazon EC2 by using kops Instead of EKS, you can deploy Kubernetes on AWS EC2 directly by using a Kubernetes deployment tool such as kops and then deploy Pachyderm on that Kubernetes cluster. If you deploy a cluster with kops , you remain responsible for the Kubernetes operations and maintenance. Deploy Pachyderm with CloudFront Use this option in production environments that require high throughput and secure data delivery.","title":"Overview"},{"location":"deploy-manage/deploy/amazon_web_services/#deploy-pachyderm-on-amazon-aws","text":"Pachyderm can run in a Kubernetes cluster deployed in Amazon\u00ae Web Services (AWS), whether it is an Elastic Container Service (EKS) or a Kubernetes cluster deployed directly on EC2 by using a deployment tool. AWS removes the need to maintain the underlying virtual cloud. This advantage makes AWS a logical choice for organizations that decide to offload the cloud infrastructure operational burden to a third-party vendor. Pachyderm seamlessly integrates with Amazon EKS and runs in the same fashion as on your computer. You can install Pachyderm on Amazon AWS by using one of the following options: Deploy Pachyderm on Amazon EKS If you already have an Amazon EKS cluster, you can quickly deploy Pachyderm on top of it. If you are just starting with Amazon EKS, this section guides you through the EKS deployment process. Deploy Pachyderm on Amazon EC2 by using kops Instead of EKS, you can deploy Kubernetes on AWS EC2 directly by using a Kubernetes deployment tool such as kops and then deploy Pachyderm on that Kubernetes cluster. If you deploy a cluster with kops , you remain responsible for the Kubernetes operations and maintenance. Deploy Pachyderm with CloudFront Use this option in production environments that require high throughput and secure data delivery.","title":"Deploy Pachyderm on Amazon AWS"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-kubernetes-kops/","text":"Deploy Kubernetes with kops \u00b6 kops is one of the most popular open-source tools that enable you to deploy, manage, and upgrade a Kubernetes cluster in the cloud. By using kops you can quickly spin-up a highly-available Kubernetes cluster in a supported cloud platform. Prerequisites \u00b6 Before you can deploy Pachyderm on Amazon AWS with kops , you must have the following components configured: Install AWS CLI Install kubectl Install kops Install pachctl Install jq Install uuid Configure kops \u00b6 kops , which stands for Kubernetes Operations , is an open-source tool that deploys a production-grade Kubernetes cluster on a cloud environment of choice. You need to have access to the AWS Management console to add an Identity and Access Management (IAM) user for kops . For more information about kops , see kops AWS documentation . These instructions provide more details about configuring additional cluster parameters, such as enabling version control or encryption on your S3 bucket, and so on. To configure kops , complete the following steps: In the IAM console or by using the command line, create a kops group with the following permissions: AmazonEC2FullAccess AmazonRoute53FullAccess AmazonS3FullAccess IAMFullAccess AmazonVPCFullAccess Add a user that will create a Kubernetes cluster to that group. In the list of users, select that user and navigate to the Security credentials tab. Create an access key and save the access and secret keys in a location on your computer. Configure an AWS CLI client: $ aws configure Use the access and secret keys to configure the AWSL client. Create an S3 bucket for your cluster: $ aws s3api create-bucket --bucket <name> --region <region> Example: $ aws s3api create-bucket --bucket test-pachyderm --region us-east-1 { \"Location\" : \"/test-pachyderm\" } Optionally, configure DNS as described in Configure DNS . In this example, a gossip-based cluster that ends with k8s.local is deployed. Export the name of your cluster and the S3 bucket for the Kubernetes cluster as variables. Example: export NAME = test-pachyderm.k8s.local export KOPS_STATE_STORE = s3://test-pachyderm Create the cluster configuration: kops create cluster --zones <region> ${ NAME } Optionally, edit your cluster: kops edit cluster ${ NAME } Build and deploy the cluster: kops update cluster ${ NAME } --yes The deployment might take some time. Run kops cluster validate periodically to monitor cluster deployment. When kops finishes deploying the cluster, you should see the output similar to the following: $ kops validate cluster Using cluster from kubectl context: test-pachyderm.k8s.local Validating cluster svetkars.k8s.local INSTANCE GROUPS NAME ROLE MACHINETYPE MIN MAX SUBNETS master-us-west-2a Master m3.medium 1 1 us-west-2a nodes Node t2.medium 2 2 us-west-2a NODE STATUS NAME ROLE READY ip-172-20-45-231.us-west-2.compute.internal node True ip-172-20-50-8.us-west-2.compute.internal master True ip-172-20-58-132.us-west-2.compute.internal node True Proceed to Deploy Pachyderm on AWS .","title":"Deploy Kubernetes with kops"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-kubernetes-kops/#deploy-kubernetes-with-kops","text":"kops is one of the most popular open-source tools that enable you to deploy, manage, and upgrade a Kubernetes cluster in the cloud. By using kops you can quickly spin-up a highly-available Kubernetes cluster in a supported cloud platform.","title":"Deploy Kubernetes with kops"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-kubernetes-kops/#prerequisites","text":"Before you can deploy Pachyderm on Amazon AWS with kops , you must have the following components configured: Install AWS CLI Install kubectl Install kops Install pachctl Install jq Install uuid","title":"Prerequisites"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-kubernetes-kops/#configure-kops","text":"kops , which stands for Kubernetes Operations , is an open-source tool that deploys a production-grade Kubernetes cluster on a cloud environment of choice. You need to have access to the AWS Management console to add an Identity and Access Management (IAM) user for kops . For more information about kops , see kops AWS documentation . These instructions provide more details about configuring additional cluster parameters, such as enabling version control or encryption on your S3 bucket, and so on. To configure kops , complete the following steps: In the IAM console or by using the command line, create a kops group with the following permissions: AmazonEC2FullAccess AmazonRoute53FullAccess AmazonS3FullAccess IAMFullAccess AmazonVPCFullAccess Add a user that will create a Kubernetes cluster to that group. In the list of users, select that user and navigate to the Security credentials tab. Create an access key and save the access and secret keys in a location on your computer. Configure an AWS CLI client: $ aws configure Use the access and secret keys to configure the AWSL client. Create an S3 bucket for your cluster: $ aws s3api create-bucket --bucket <name> --region <region> Example: $ aws s3api create-bucket --bucket test-pachyderm --region us-east-1 { \"Location\" : \"/test-pachyderm\" } Optionally, configure DNS as described in Configure DNS . In this example, a gossip-based cluster that ends with k8s.local is deployed. Export the name of your cluster and the S3 bucket for the Kubernetes cluster as variables. Example: export NAME = test-pachyderm.k8s.local export KOPS_STATE_STORE = s3://test-pachyderm Create the cluster configuration: kops create cluster --zones <region> ${ NAME } Optionally, edit your cluster: kops edit cluster ${ NAME } Build and deploy the cluster: kops update cluster ${ NAME } --yes The deployment might take some time. Run kops cluster validate periodically to monitor cluster deployment. When kops finishes deploying the cluster, you should see the output similar to the following: $ kops validate cluster Using cluster from kubectl context: test-pachyderm.k8s.local Validating cluster svetkars.k8s.local INSTANCE GROUPS NAME ROLE MACHINETYPE MIN MAX SUBNETS master-us-west-2a Master m3.medium 1 1 us-west-2a nodes Node t2.medium 2 2 us-west-2a NODE STATUS NAME ROLE READY ip-172-20-45-231.us-west-2.compute.internal node True ip-172-20-50-8.us-west-2.compute.internal master True ip-172-20-58-132.us-west-2.compute.internal node True Proceed to Deploy Pachyderm on AWS .","title":"Configure kops"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-pachyderm/","text":"Deploy Pachyderm on AWS \u00b6 After you deploy Kubernetes cluster by using kops or eksctl , you can deploy Pachyderm on top of that cluster. You need to complete the following steps to deploy Pachyderm: Install pachctl as described in Install pachctl . Add stateful storage for Pachyderm as described in Add Stateful Storage . Deploy Pachyderm by using an IAM role (recommended) or an access key . Add Stateful Storage \u00b6 Pachyderm requires the following types of persistent storage: An S3 object store bucket for data. The S3 bucket name must be globally unique across the whole Amazon region. Therefore, add a descriptive prefix to the S3 bucket name, such as your username. An Elastic Block Storage (EBS) persistent volume (PV) for Pachyderm metadata. Pachyderm recommends that you assign at least 10 GB for this persistent EBS volume. If you expect your cluster to be very long running a scale to thousands of jobs per commits, you might need to go add more storage. However, you can easily increase the size of the persistent volume later. To add stateful storage, complete the following steps: Set up the following system variables: BUCKET_NAME \u2014 A globally unique S3 bucket name. STORAGE_SIZE \u2014 The size of the persistent volume in GB. For example, 10 . AWS_REGION \u2014 The AWS region of your Kubernetes cluster. For example, us-west-2 and not us-west-2a . Create an S3 bucket: If you are creating an S3 bucket in the us-east-1 region, run the following command: $ aws s3api create-bucket --bucket ${ BUCKET_NAME } --region ${ AWS_REGION } If you are creating an S3 bucket in any region but the us-east-1 region, run the following command: $ aws s3api create-bucket --bucket ${ BUCKET_NAME } --region ${ AWS_REGION } --create-bucket-configuration LocationConstraint = ${ AWS_REGION } Verify that the S3 bucket was created: $ aws s3api list-buckets --query 'Buckets[].Name' Deploy Pachyderm with an IAM Role \u00b6 IAM roles provide better user management and security capabilities compared to access keys. If a malicious user gains access to an access key, your data might become compromised. Therefore, enterprises often opt out to use IAM roles rather than access keys for production deployments. You need to configure the following IAM settings: The worker nodes on which Pachyderm is deployed must be associated with the IAM role that is assigned to the Kubernetes cluster. If you created your cluster by using kops or eksctl the nodes must have a dedicated IAM role already assigned. The IAM role must have access to the S3 bucket that you created for Pachyderm. The IAM role must have correct trust relationships. You need to set a system variable IAM_ROLE to the name of the IAM role that you will use to deploy the cluster. This role is different from the Role ARN or the Instance Profile ARN of the role. It iss the actual role name. To deploy Pachyderm with an IAM role, complete the following steps: Find the IAM role assigned to the cluster: Go to the AWS Management console. Select an EC2 instance in the Kubernetes cluster. Click Description . Find the IAM Role field. Enable access to the S3 bucket for the IAM role: In the IAM Role field, click on the IAM role. In the Permissions tab, click Edit policy . Select the JSON tab. Append the following text to the end of the existing JSON: { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::<your-bucket>\" ] } , { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::<your-bucket>/*\" ] } Replace <your-bucket> with the name of your S3 bucket. Note: For the EKS cluster, you might need to use the Add inline policy button and create a name for the new policy. The JSON above is inserted between the square brackets for the Statement element. Set up trust relationships for the IAM role: Click the Trust relationships > Edit trust relationship . Ensure that you see a statement with sts:AssumeRole . Example: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] } Set the system variable IAM_ROLE to the IAM role name for the Pachyderm deployment. Deploy Pachyderm: $ pachctl deploy amazon ${ BUCKET_NAME } ${ AWS_REGION } ${ STORAGE_SIZE } --dynamic-etcd-nodes = 1 --iam-role ${ IAM_ROLE } The deployment takes some time. You can run kubectl get pods periodically to check the status of deployment. When Pachyderm is deployed, the command shows all pods as READY : $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-89dv9 2 /2 Running 0 1m etcd-0 1 /1 Running 0 4m pachd-65fd68d6d4-8vjq7 1 /1 Running 0 4m Note: If you see a few restarts on the pachd nodes, it means that Kubernetes tried to bring up those pods before etcd was ready. Therefore, Kubernetes restarted those pods. You can safely ignore this message. Verify that the Pachyderm cluster is up and running: $ pachctl version COMPONENT VERSION pachctl 1 .9.7 pachd 1 .9.7 If you want to access the Pachyderm UI or use the S3 gateway, you need to forward Pachyderm ports. Open a new terminal window and run the following command: $ pachctl port-forward Deploy Pachyderm with an Access Key \u00b6 When you installed kops , you created a dedicated IAM user with access credentials such as an access key and secret key. You can deploy Pachyderm by using the credentials of this IAM user directly. However, deploying Pachyderm with an access key might not satisfy your enterprise security requirements. Therefore, deploying with an IAM role is preferred. To deploy Pachyderm with an access key, complete the following steps: Run the following command to deploy your Pachyderm cluster: $ pachctl deploy amazon ${ BUCKET_NAME } ${ AWS_REGION } ${ STORAGE_SIZE } --dynamic-etcd-nodes = 1 --credentials \" ${ AWS_ACCESS_KEY_ID } , ${ AWS_SECRET_ACCESS_KEY } ,\" The , at the end of the credentials flag in the deploy command is for an optional temporary AWS token. You might use such a token if you are just experimenting with Pachyderm. However, do not use this token in a production deployment. The deployment takes some time. You can run kubectl get pods periodically to check the status of deployment. When Pachyderm is deployed, the command shows all pods as READY : $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-89dv9 2 /2 Running 0 1m etcd-0 1 /1 Running 0 4m pachd-65fd68d6d4-8vjq7 1 /1 Running 0 4m Note: If you see a few restarts on the pachd nodes, it means that Kubernetes tried to bring up those pods before etcd was ready. Therefore, Kubernetes restarted those pods. You can safely ignore this message. Verify that the Pachyderm cluster is up and running: $ pachctl version COMPONENT VERSION pachctl 1 .9.7 pachd 1 .9.7 If you want to access the Pachyderm UI or use S3 gateway, you need to forward Pachyderm ports. Open a new terminal window and run the following command: $ pachctl port-forward","title":"Deploy Pachyderm on AWS"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-pachyderm/#deploy-pachyderm-on-aws","text":"After you deploy Kubernetes cluster by using kops or eksctl , you can deploy Pachyderm on top of that cluster. You need to complete the following steps to deploy Pachyderm: Install pachctl as described in Install pachctl . Add stateful storage for Pachyderm as described in Add Stateful Storage . Deploy Pachyderm by using an IAM role (recommended) or an access key .","title":"Deploy Pachyderm on AWS"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-pachyderm/#add-stateful-storage","text":"Pachyderm requires the following types of persistent storage: An S3 object store bucket for data. The S3 bucket name must be globally unique across the whole Amazon region. Therefore, add a descriptive prefix to the S3 bucket name, such as your username. An Elastic Block Storage (EBS) persistent volume (PV) for Pachyderm metadata. Pachyderm recommends that you assign at least 10 GB for this persistent EBS volume. If you expect your cluster to be very long running a scale to thousands of jobs per commits, you might need to go add more storage. However, you can easily increase the size of the persistent volume later. To add stateful storage, complete the following steps: Set up the following system variables: BUCKET_NAME \u2014 A globally unique S3 bucket name. STORAGE_SIZE \u2014 The size of the persistent volume in GB. For example, 10 . AWS_REGION \u2014 The AWS region of your Kubernetes cluster. For example, us-west-2 and not us-west-2a . Create an S3 bucket: If you are creating an S3 bucket in the us-east-1 region, run the following command: $ aws s3api create-bucket --bucket ${ BUCKET_NAME } --region ${ AWS_REGION } If you are creating an S3 bucket in any region but the us-east-1 region, run the following command: $ aws s3api create-bucket --bucket ${ BUCKET_NAME } --region ${ AWS_REGION } --create-bucket-configuration LocationConstraint = ${ AWS_REGION } Verify that the S3 bucket was created: $ aws s3api list-buckets --query 'Buckets[].Name'","title":"Add Stateful Storage"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-pachyderm/#deploy-pachyderm-with-an-iam-role","text":"IAM roles provide better user management and security capabilities compared to access keys. If a malicious user gains access to an access key, your data might become compromised. Therefore, enterprises often opt out to use IAM roles rather than access keys for production deployments. You need to configure the following IAM settings: The worker nodes on which Pachyderm is deployed must be associated with the IAM role that is assigned to the Kubernetes cluster. If you created your cluster by using kops or eksctl the nodes must have a dedicated IAM role already assigned. The IAM role must have access to the S3 bucket that you created for Pachyderm. The IAM role must have correct trust relationships. You need to set a system variable IAM_ROLE to the name of the IAM role that you will use to deploy the cluster. This role is different from the Role ARN or the Instance Profile ARN of the role. It iss the actual role name. To deploy Pachyderm with an IAM role, complete the following steps: Find the IAM role assigned to the cluster: Go to the AWS Management console. Select an EC2 instance in the Kubernetes cluster. Click Description . Find the IAM Role field. Enable access to the S3 bucket for the IAM role: In the IAM Role field, click on the IAM role. In the Permissions tab, click Edit policy . Select the JSON tab. Append the following text to the end of the existing JSON: { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::<your-bucket>\" ] } , { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:DeleteObject\" ], \"Resource\" : [ \"arn:aws:s3:::<your-bucket>/*\" ] } Replace <your-bucket> with the name of your S3 bucket. Note: For the EKS cluster, you might need to use the Add inline policy button and create a name for the new policy. The JSON above is inserted between the square brackets for the Statement element. Set up trust relationships for the IAM role: Click the Trust relationships > Edit trust relationship . Ensure that you see a statement with sts:AssumeRole . Example: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] } Set the system variable IAM_ROLE to the IAM role name for the Pachyderm deployment. Deploy Pachyderm: $ pachctl deploy amazon ${ BUCKET_NAME } ${ AWS_REGION } ${ STORAGE_SIZE } --dynamic-etcd-nodes = 1 --iam-role ${ IAM_ROLE } The deployment takes some time. You can run kubectl get pods periodically to check the status of deployment. When Pachyderm is deployed, the command shows all pods as READY : $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-89dv9 2 /2 Running 0 1m etcd-0 1 /1 Running 0 4m pachd-65fd68d6d4-8vjq7 1 /1 Running 0 4m Note: If you see a few restarts on the pachd nodes, it means that Kubernetes tried to bring up those pods before etcd was ready. Therefore, Kubernetes restarted those pods. You can safely ignore this message. Verify that the Pachyderm cluster is up and running: $ pachctl version COMPONENT VERSION pachctl 1 .9.7 pachd 1 .9.7 If you want to access the Pachyderm UI or use the S3 gateway, you need to forward Pachyderm ports. Open a new terminal window and run the following command: $ pachctl port-forward","title":"Deploy Pachyderm with an IAM Role"},{"location":"deploy-manage/deploy/amazon_web_services/aws-deploy-pachyderm/#deploy-pachyderm-with-an-access-key","text":"When you installed kops , you created a dedicated IAM user with access credentials such as an access key and secret key. You can deploy Pachyderm by using the credentials of this IAM user directly. However, deploying Pachyderm with an access key might not satisfy your enterprise security requirements. Therefore, deploying with an IAM role is preferred. To deploy Pachyderm with an access key, complete the following steps: Run the following command to deploy your Pachyderm cluster: $ pachctl deploy amazon ${ BUCKET_NAME } ${ AWS_REGION } ${ STORAGE_SIZE } --dynamic-etcd-nodes = 1 --credentials \" ${ AWS_ACCESS_KEY_ID } , ${ AWS_SECRET_ACCESS_KEY } ,\" The , at the end of the credentials flag in the deploy command is for an optional temporary AWS token. You might use such a token if you are just experimenting with Pachyderm. However, do not use this token in a production deployment. The deployment takes some time. You can run kubectl get pods periodically to check the status of deployment. When Pachyderm is deployed, the command shows all pods as READY : $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-89dv9 2 /2 Running 0 1m etcd-0 1 /1 Running 0 4m pachd-65fd68d6d4-8vjq7 1 /1 Running 0 4m Note: If you see a few restarts on the pachd nodes, it means that Kubernetes tried to bring up those pods before etcd was ready. Therefore, Kubernetes restarted those pods. You can safely ignore this message. Verify that the Pachyderm cluster is up and running: $ pachctl version COMPONENT VERSION pachctl 1 .9.7 pachd 1 .9.7 If you want to access the Pachyderm UI or use S3 gateway, you need to forward Pachyderm ports. Open a new terminal window and run the following command: $ pachctl port-forward","title":"Deploy Pachyderm with an Access Key"},{"location":"deploy-manage/deploy/amazon_web_services/aws_cloudfront/","text":"Deploy a Pachyderm Cluster with CloudFront \u00b6 After you have an EKS cluster or a Kubernetes cluster deployed with kops ready, you can integrate it with Amazon CloudFront\u2122. Amazon CloudFront is a content delivery network (CDN) that streams data to your website, service, or application securely and with great performance. Pachyderm recommends that you set up Pachyderm with CloudFront for all production deployments. To deploy Pachyderm cluster with CloudFront, complete the following steps: Create a CloudFront Distribution Deploy Pachyderm with an IAM role Apply the CloudFront Key Pair Apply the CloudFront Key Pair \u00b6 If you need to create signed URLs and signed cookies for the data that goes to Pachyderm, you need to configure your AWS account to use a valid CloudFront key pair. Only a root AWS account can generate these secure credentials. Therefore, you might need to request your IT department to create them for you. For more information, see the Amazon documentation . The CloudFront key pair includes the following attributes: The private and public key. For this deployment, you only need the private key. The key pair ID. Typically, the key pair ID is recorded in the filename. Example: rsa-APKAXXXXXXXXXXXXXXXX.pem pk-APKAXXXXXXXXXXXXXXXX.pem The key-pair ID is APKAXXXXXXXXXXXXXXXX . The other file is the private key, which looks similar to the following text: Example $ cat pk-APKAXXXXXXXXXXXX.pem -----BEGIN RSA PRIVATE KEY----- ... To apply this key pair to your CloudFront distribution, complete the following steps: Download the secure-cloudfront.sh script from the Pachyderm repository: $ curl -o secure-cloudfront.sh https://raw.githubusercontent.com/pachyderm/pachyderm/master/etc/deploy/cloudfront/secure-cloudfront.sh Make the script executable: $ chmod +x secure-cloudfront.sh From the deploy.log file, obtain the S3 bucket name for your deployment and the CloudFront distribution ID. Apply the key pair to your CloudFront distribution: $ ./secure-cloudfront.sh --region us-west-2 --zone us-west-2c --bucket YYYY-pachyderm-store --cloudfront-distribution-id E1BEBVLIDYTLEV --cloudfront-keypair-id APKAXXXXXXXXXXXX --cloudfront-private-key-file ~/Downloads/pk-APKAXXXXXXXXXXXX.pem Restart the pachd pod for the changes to take effect: $ kubectl scale --replicas = 0 deployment/pachd && kubectl scale --replicas = 1 deployment/pachd && kubectl get pod Verify the setup by checking the pachd logs and confirming that Kubernetes uses the CloudFront credentials: $ kubectl get pod NAME READY STATUS RESTARTS AGE etcd-0 1 /1 Running 0 19h etcd-1 1 /1 Running 0 19h etcd-2 1 /1 Running 0 19h pachd-2796595787-9x0qf 1 /1 Running 0 16h $ kubectl logs pachd-2796595787-9x0qf | grep cloudfront 2017 -06-09T22:56:27Z INFO AWS deployed with cloudfront distribution at d3j9kenawdv8p0 2017 -06-09T22:56:27Z INFO Using cloudfront security credentials - keypair ID ( APKAXXXXXXXXX ) - to sign cloudfront URLs","title":"Deploy a Pachyderm Cluster with CloudFront"},{"location":"deploy-manage/deploy/amazon_web_services/aws_cloudfront/#deploy-a-pachyderm-cluster-with-cloudfront","text":"After you have an EKS cluster or a Kubernetes cluster deployed with kops ready, you can integrate it with Amazon CloudFront\u2122. Amazon CloudFront is a content delivery network (CDN) that streams data to your website, service, or application securely and with great performance. Pachyderm recommends that you set up Pachyderm with CloudFront for all production deployments. To deploy Pachyderm cluster with CloudFront, complete the following steps: Create a CloudFront Distribution Deploy Pachyderm with an IAM role Apply the CloudFront Key Pair","title":"Deploy a Pachyderm Cluster with CloudFront"},{"location":"deploy-manage/deploy/amazon_web_services/aws_cloudfront/#apply-the-cloudfront-key-pair","text":"If you need to create signed URLs and signed cookies for the data that goes to Pachyderm, you need to configure your AWS account to use a valid CloudFront key pair. Only a root AWS account can generate these secure credentials. Therefore, you might need to request your IT department to create them for you. For more information, see the Amazon documentation . The CloudFront key pair includes the following attributes: The private and public key. For this deployment, you only need the private key. The key pair ID. Typically, the key pair ID is recorded in the filename. Example: rsa-APKAXXXXXXXXXXXXXXXX.pem pk-APKAXXXXXXXXXXXXXXXX.pem The key-pair ID is APKAXXXXXXXXXXXXXXXX . The other file is the private key, which looks similar to the following text: Example $ cat pk-APKAXXXXXXXXXXXX.pem -----BEGIN RSA PRIVATE KEY----- ... To apply this key pair to your CloudFront distribution, complete the following steps: Download the secure-cloudfront.sh script from the Pachyderm repository: $ curl -o secure-cloudfront.sh https://raw.githubusercontent.com/pachyderm/pachyderm/master/etc/deploy/cloudfront/secure-cloudfront.sh Make the script executable: $ chmod +x secure-cloudfront.sh From the deploy.log file, obtain the S3 bucket name for your deployment and the CloudFront distribution ID. Apply the key pair to your CloudFront distribution: $ ./secure-cloudfront.sh --region us-west-2 --zone us-west-2c --bucket YYYY-pachyderm-store --cloudfront-distribution-id E1BEBVLIDYTLEV --cloudfront-keypair-id APKAXXXXXXXXXXXX --cloudfront-private-key-file ~/Downloads/pk-APKAXXXXXXXXXXXX.pem Restart the pachd pod for the changes to take effect: $ kubectl scale --replicas = 0 deployment/pachd && kubectl scale --replicas = 1 deployment/pachd && kubectl get pod Verify the setup by checking the pachd logs and confirming that Kubernetes uses the CloudFront credentials: $ kubectl get pod NAME READY STATUS RESTARTS AGE etcd-0 1 /1 Running 0 19h etcd-1 1 /1 Running 0 19h etcd-2 1 /1 Running 0 19h pachd-2796595787-9x0qf 1 /1 Running 0 16h $ kubectl logs pachd-2796595787-9x0qf | grep cloudfront 2017 -06-09T22:56:27Z INFO AWS deployed with cloudfront distribution at d3j9kenawdv8p0 2017 -06-09T22:56:27Z INFO Using cloudfront security credentials - keypair ID ( APKAXXXXXXXXX ) - to sign cloudfront URLs","title":"Apply the CloudFront Key Pair"},{"location":"deploy-manage/deploy/amazon_web_services/deploy-eks/","text":"Deploy Pachyderm on Amazon EKS \u00b6 Amazon EKS provides an easy way to deploy, configure, and manage Kubernetes clusters. If you want to avoid managing your Kubernetes infrastructure, EKS might be the right choice for your organization. Pachyderm seamlessly deploys on Amazon EKS. Prerequisites \u00b6 Before you can deploy Pachyderm on an EKS cluster, verify that you have the following prerequisites installed and configured: kubectl AWS CLI eksctl aws-iam-authenticator . pachctl Deploy an EKS cluster by using eksctl \u00b6 Use the eksctl tool to deploy an EKS cluster in your Amazon AWS environment. The eksctl create cluster command creates a virtual private cloud (VPC), a security group, and an IAM role for Kubernetes to create resources. For detailed instructions, see Amazon documentation . To deploy an EKS cluster, complete the following steps: Deploy an EKS cluster: eksctl create cluster --name <name> --version <version> \\ --nodegroup-name <name> --node-type <vm-flavor> \\ --nodes <number-of-nodes> --nodes-min <min-number-nodes> \\ --nodes-max <max-number-nodes> --node-ami auto Example output: [ \u2139 ] using region us-east-1 [ \u2139 ] setting availability zones to [ us-east-1a us-east-1f ] [ \u2139 ] subnets for us-east-1a - public:192.168.0.0/19 private:192.168.64.0/19 [ \u2139 ] subnets for us-east-1f - public:192.168.32.0/19 private:192.168.96.0/19 [ \u2139 ] nodegroup \"pachyderm-test-workers\" will use \"ami-0f2e8e5663e16b436\" [ AmazonLinux2/1.13 ] [ \u2139 ] using Kubernetes version 1 .13 [ \u2139 ] creating EKS cluster \"pachyderm-test-eks\" in \"us-east-1\" region [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=pachyderm-test-eks' [ \u2139 ] 2 sequential tasks: { create cluster control plane \"svetkars-eks\" , create nodegroup \"pachyderm-test-workers\" } [ \u2139 ] building cluster stack \"eksctl-pachyderm-test-eks-cluster\" [ \u2139 ] deploying stack \"eksctl-pachyderm-test-eks-cluster\" ... [ \u2714 ] EKS cluster \"pachyderm-test\" in \"us-east-1\" region is ready Verify the deployment: $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .100.0.1 <none> 443 /TCP 7m9s Deploy Pachyderm as described in Deploy Pachyderm on AWS .","title":"Deploy Pachyderm on Amazon EKS"},{"location":"deploy-manage/deploy/amazon_web_services/deploy-eks/#deploy-pachyderm-on-amazon-eks","text":"Amazon EKS provides an easy way to deploy, configure, and manage Kubernetes clusters. If you want to avoid managing your Kubernetes infrastructure, EKS might be the right choice for your organization. Pachyderm seamlessly deploys on Amazon EKS.","title":"Deploy Pachyderm on Amazon EKS"},{"location":"deploy-manage/deploy/amazon_web_services/deploy-eks/#prerequisites","text":"Before you can deploy Pachyderm on an EKS cluster, verify that you have the following prerequisites installed and configured: kubectl AWS CLI eksctl aws-iam-authenticator . pachctl","title":"Prerequisites"},{"location":"deploy-manage/deploy/amazon_web_services/deploy-eks/#deploy-an-eks-cluster-by-using-eksctl","text":"Use the eksctl tool to deploy an EKS cluster in your Amazon AWS environment. The eksctl create cluster command creates a virtual private cloud (VPC), a security group, and an IAM role for Kubernetes to create resources. For detailed instructions, see Amazon documentation . To deploy an EKS cluster, complete the following steps: Deploy an EKS cluster: eksctl create cluster --name <name> --version <version> \\ --nodegroup-name <name> --node-type <vm-flavor> \\ --nodes <number-of-nodes> --nodes-min <min-number-nodes> \\ --nodes-max <max-number-nodes> --node-ami auto Example output: [ \u2139 ] using region us-east-1 [ \u2139 ] setting availability zones to [ us-east-1a us-east-1f ] [ \u2139 ] subnets for us-east-1a - public:192.168.0.0/19 private:192.168.64.0/19 [ \u2139 ] subnets for us-east-1f - public:192.168.32.0/19 private:192.168.96.0/19 [ \u2139 ] nodegroup \"pachyderm-test-workers\" will use \"ami-0f2e8e5663e16b436\" [ AmazonLinux2/1.13 ] [ \u2139 ] using Kubernetes version 1 .13 [ \u2139 ] creating EKS cluster \"pachyderm-test-eks\" in \"us-east-1\" region [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=pachyderm-test-eks' [ \u2139 ] 2 sequential tasks: { create cluster control plane \"svetkars-eks\" , create nodegroup \"pachyderm-test-workers\" } [ \u2139 ] building cluster stack \"eksctl-pachyderm-test-eks-cluster\" [ \u2139 ] deploying stack \"eksctl-pachyderm-test-eks-cluster\" ... [ \u2714 ] EKS cluster \"pachyderm-test\" in \"us-east-1\" region is ready Verify the deployment: $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .100.0.1 <none> 443 /TCP 7m9s Deploy Pachyderm as described in Deploy Pachyderm on AWS .","title":"Deploy an EKS cluster by using eksctl"},{"location":"deploy-manage/deploy/deploy_custom/","text":"Create a Custom Pachyderm Deployment \u00b6 Pachyderm provides the pachctl deploy custom command for creating customized deployments for cloud providers or on-premises use. This section describes how to use pachctl deploy custom to create a manifest for a custom, on-premises deployment. Although deployment automation is out of the scope of this section, Pachyderm strongly encourages you to treat your infrastructure as code Deploy On-Premises . The topics in this section walk you through the process of using the available flags to create the following components of your Pachyderm infrastructure: A Pachyderm deployment using StatefulSets. An on-premises Kubernetes cluster with StatefulSets configured. It has the standard etcd StorageClass, along with access controls that limit the deployment to namespace-local roles only. An on-premises MinIO object store with the following parameters: SSL is enabled. Authentication requests are signed with the S3v4 signatures. The endpoint is minio:9000 . The access key is OBSIJRBE0PP2NO4QOA27 . The secret key is tfteSlswRu7BJ86wekitnifILbZam1KYY3TG . The S3 bucket name is pachyderm-bucket . After configuring these parameters, you save the output of the invocation to a configuration file that you can later use to deploy and configure your environment. For the purposes of our example, all scripts in that hypothetical infrastructure work with YAML manifests. Complete the steps described in the following topics to deploy your custom environment: Before You Begin Pachyderm Deployment Manifest Configuring Persistent Disk Parameters Configuring Object Store Create a Complete Configuration Additional Flags","title":"Overview"},{"location":"deploy-manage/deploy/deploy_custom/#create-a-custom-pachyderm-deployment","text":"Pachyderm provides the pachctl deploy custom command for creating customized deployments for cloud providers or on-premises use. This section describes how to use pachctl deploy custom to create a manifest for a custom, on-premises deployment. Although deployment automation is out of the scope of this section, Pachyderm strongly encourages you to treat your infrastructure as code Deploy On-Premises . The topics in this section walk you through the process of using the available flags to create the following components of your Pachyderm infrastructure: A Pachyderm deployment using StatefulSets. An on-premises Kubernetes cluster with StatefulSets configured. It has the standard etcd StorageClass, along with access controls that limit the deployment to namespace-local roles only. An on-premises MinIO object store with the following parameters: SSL is enabled. Authentication requests are signed with the S3v4 signatures. The endpoint is minio:9000 . The access key is OBSIJRBE0PP2NO4QOA27 . The secret key is tfteSlswRu7BJ86wekitnifILbZam1KYY3TG . The S3 bucket name is pachyderm-bucket . After configuring these parameters, you save the output of the invocation to a configuration file that you can later use to deploy and configure your environment. For the purposes of our example, all scripts in that hypothetical infrastructure work with YAML manifests. Complete the steps described in the following topics to deploy your custom environment: Before You Begin Pachyderm Deployment Manifest Configuring Persistent Disk Parameters Configuring Object Store Create a Complete Configuration Additional Flags","title":"Create a Custom Pachyderm Deployment"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_additional_flags/","text":"Additional Flags \u00b6 This section describes all the additional flags that you can use to configure your custom deployment: Access to Kubernetes resources flags: --local-roles : You can use the --local-roles flag to change the kind of role the pachyderm service account uses from cluster-wide ( ClusterRole ) to namespace-specific ( Role ). Using --local-roles inhibits your ability to use the coefficient parallelism feature. After you set the --local-roles flag, you might see a message similar to this in the pachd pod Kubernetes logs: ERROR unable to access kubernetes nodeslist, Pachyderm will continue to work but it will not be possible to use COEFFICIENT parallelism. error: nodes is forbidden: User \"system:serviceaccount:pachyderm-test-1:pachyderm\" cannot list nodes at the cluster scope Resource requests and limits flags: Larger deployments might require you to configure more resources for pachd and etcd or set higher limits for transient workloads. The following flags set attributes which are passed on to Kubernetes directly through the produced manifest. --etcd-cpu-request : The number of CPU cores that Kubernetes allocates to etcd . Fractions are allowed. --etcd-memory-request : The amount of memory that Kubernetes allocates to etcd . The SI suffixes are accepted as possible values. --no-guaranteed : Turn off QoS for etcd and pachd . Do not use this flag in production environments. --pachd-cpu-request : The number of CPU cores that Kubernetes allocates to pachd . Fractions are allowed. --pachd-memory-request : The amount of memory that Kubernetes allocates to pachd . This flag accepts the SI suffixes. --shards : The maximum number of pachd nodes allowed in the cluster. Increasing this number from the default value of 16 might result in degraded performance. Note: Do not modify the default values of these flags for production deployments without consulting with Pachyderm support. Enterprise Edition flags: --dash-image : The Docker image for the Pachyderm Enterprise Edition dashboard. --image-pull-secret : The name of a Kubernetes secret that Pachyderm uses to pull from a private Docker registry. --no-dashboard : Skip the creation of a manifest for the Enterprise Edition dashboard. --registry : The registry for Docker images. --tls : A string in the \"<cert path>,<key path>\" format with the signed TLS certificate that is used for encrypting pachd communications. Output formats flags: --dry-run : Create a manifest and send it to standard output, but do not deploy to Kubernetes. -o or --output : An output format. You can choose from JSON (default) or YAML. Logging flags: log-level : The pachd verbosity level, from most verbose to least. You can set this parameter to debug , info , or error . -v or --verbose : Controls the verbosity of the pachctl invocation.","title":"Additional flags"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_additional_flags/#additional-flags","text":"This section describes all the additional flags that you can use to configure your custom deployment: Access to Kubernetes resources flags: --local-roles : You can use the --local-roles flag to change the kind of role the pachyderm service account uses from cluster-wide ( ClusterRole ) to namespace-specific ( Role ). Using --local-roles inhibits your ability to use the coefficient parallelism feature. After you set the --local-roles flag, you might see a message similar to this in the pachd pod Kubernetes logs: ERROR unable to access kubernetes nodeslist, Pachyderm will continue to work but it will not be possible to use COEFFICIENT parallelism. error: nodes is forbidden: User \"system:serviceaccount:pachyderm-test-1:pachyderm\" cannot list nodes at the cluster scope Resource requests and limits flags: Larger deployments might require you to configure more resources for pachd and etcd or set higher limits for transient workloads. The following flags set attributes which are passed on to Kubernetes directly through the produced manifest. --etcd-cpu-request : The number of CPU cores that Kubernetes allocates to etcd . Fractions are allowed. --etcd-memory-request : The amount of memory that Kubernetes allocates to etcd . The SI suffixes are accepted as possible values. --no-guaranteed : Turn off QoS for etcd and pachd . Do not use this flag in production environments. --pachd-cpu-request : The number of CPU cores that Kubernetes allocates to pachd . Fractions are allowed. --pachd-memory-request : The amount of memory that Kubernetes allocates to pachd . This flag accepts the SI suffixes. --shards : The maximum number of pachd nodes allowed in the cluster. Increasing this number from the default value of 16 might result in degraded performance. Note: Do not modify the default values of these flags for production deployments without consulting with Pachyderm support. Enterprise Edition flags: --dash-image : The Docker image for the Pachyderm Enterprise Edition dashboard. --image-pull-secret : The name of a Kubernetes secret that Pachyderm uses to pull from a private Docker registry. --no-dashboard : Skip the creation of a manifest for the Enterprise Edition dashboard. --registry : The registry for Docker images. --tls : A string in the \"<cert path>,<key path>\" format with the signed TLS certificate that is used for encrypting pachd communications. Output formats flags: --dry-run : Create a manifest and send it to standard output, but do not deploy to Kubernetes. -o or --output : An output format. You can choose from JSON (default) or YAML. Logging flags: log-level : The pachd verbosity level, from most verbose to least. You can set this parameter to debug , info , or error . -v or --verbose : Controls the verbosity of the pachctl invocation.","title":"Additional Flags"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_before_you_begin/","text":"Before You Begin \u00b6 Before you start creating a custom deployment, verify that you have completed the following steps: Read and complete the steps described in the Introduction in the On-Premises section. This section explains the differences between static persistent volumes, StatefulSets, and StatefulSets with StorageClasses. Also, it explains the meanings of the variables, such as PVC_STORAGE_SIZE and OS_ENDPOINT that are used in the examples below. Install kubectl . Install pachctl . Proceed to Pachyderm Deployment Manifest .","title":"Before You Begin"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_before_you_begin/#before-you-begin","text":"Before you start creating a custom deployment, verify that you have completed the following steps: Read and complete the steps described in the Introduction in the On-Premises section. This section explains the differences between static persistent volumes, StatefulSets, and StatefulSets with StorageClasses. Also, it explains the meanings of the variables, such as PVC_STORAGE_SIZE and OS_ENDPOINT that are used in the examples below. Install kubectl . Install pachctl . Proceed to Pachyderm Deployment Manifest .","title":"Before You Begin"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_complete_example_invocation/","text":"Create a Complete Configuration \u00b6 Before reading this section, complete the steps in Configuring Object Store . The following is a complete deploy command example of a custom deployment. The command generates the manifest and saves it as a YAML configuration file. Also, the command includes the local-roles flag to scope the deployment to the pachyderm service account access permissions. Run the following command to deploy your example cluster: pachctl deploy custom --persistent-disk aws --object-store s3 \\ foobar 10 \\ pachyderm-bucket 'OBSIJRBE0PP2NO4QOA27' 'tfteSlswRu7BJ86wekitnifILbZam1KYY3TG' 'minio:9000' \\ --dynamic-etcd-nodes 1 --local-roles --output yaml --dry-run > custom_deploy.yaml For more information about the contents of the custom_deploy.yaml file, see Pachyderm Deployment Manifest . Deploy Your Cluster \u00b6 You can either deploy manifests that you have created above or edit them to customize them further, before deploying. If you decide to edit your manifest, you must consult with an experienced Kubernetes administrator. If you are attempting a highly customized deployment, use one of the Pachyderm support resources listed below. To deploy your configuration, run the following command: $ kubectl apply -f ./custom_deploy.yaml","title":"Create a Complete Configuration"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_complete_example_invocation/#create-a-complete-configuration","text":"Before reading this section, complete the steps in Configuring Object Store . The following is a complete deploy command example of a custom deployment. The command generates the manifest and saves it as a YAML configuration file. Also, the command includes the local-roles flag to scope the deployment to the pachyderm service account access permissions. Run the following command to deploy your example cluster: pachctl deploy custom --persistent-disk aws --object-store s3 \\ foobar 10 \\ pachyderm-bucket 'OBSIJRBE0PP2NO4QOA27' 'tfteSlswRu7BJ86wekitnifILbZam1KYY3TG' 'minio:9000' \\ --dynamic-etcd-nodes 1 --local-roles --output yaml --dry-run > custom_deploy.yaml For more information about the contents of the custom_deploy.yaml file, see Pachyderm Deployment Manifest .","title":"Create a Complete Configuration"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_complete_example_invocation/#deploy-your-cluster","text":"You can either deploy manifests that you have created above or edit them to customize them further, before deploying. If you decide to edit your manifest, you must consult with an experienced Kubernetes administrator. If you are attempting a highly customized deployment, use one of the Pachyderm support resources listed below. To deploy your configuration, run the following command: $ kubectl apply -f ./custom_deploy.yaml","title":"Deploy Your Cluster"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_configuring_object_store/","text":"Configuring Object Store \u00b6 Before reading this section, complete the steps in Configuring Persistent Disk Parameters . You can use the --object-store flag to configure Pachyderm to use an s3 storage protocol to access the configured object store. This configuration uses the Amazon S3 driver to access your on-premises object store, regardless of the vendor, since the Amazon S3 API is the standard with which every object store is designed to work. The S3 API has two different extant versions of signature styles , which are how the object store validates client requests. S3v4 is the most current version, but many S3v2 object-store servers are still in use. Because support for S3v2 is scheduled to be deprecated, Pachyderm recommends that you use S3v4 in all deployments. If you need to access an object store that uses S3v2 signatures, you can specify the --isS3V2 flag. This parameter configures Pachyderm to use the MinIO driver, which allows the use of the older signature. This --isS3V2 flag disables SSL for connections to the object store with the minio driver. You can re-enable it with the -s or --secure flag. You may also manually edit the pachyderm-storage-secret Kubernetes manifest. The --object-store flag takes four required configuration arguments. Place these arguments immediately after the persistent disk parameters : bucket-name : The name of the bucket, without the s3:// prefix or a trailing forward slash ( / ). access-key : The user access ID that is used to access the object store. secret-key : The associated password that is used with the user access ID to access the object store. endpoint : The hostname and port that are used to access the object store, in <hostname>:<port> format. Example Invocation with a PV and Object Store \u00b6 This example on-premises cluster uses an on-premises MinIO object store with the following configuration parameters: An on-premises MinIO object store with the following parameters: SSL is enabled S3v4 signatures The endpoint is minio:9000 The access key is OBSIJRBE0PP2NO4QOA27 The secret key is tfteSlswRu7BJ86wekitnifILbZam1KYY3TG A bucket named pachyderm-bucket The deployment command uses the following flags: pachctl deploy custom --persistent-disk aws --object-store s3 \\ any-string 10 \\ pachyderm-bucket 'OBSIJRBE0PP2NO4QOA27' 'tfteSlswRu7BJ86wekitnifILbZam1KYY3TG' 'minio:9000' \\ --dynamic-etcd-nodes 1 [optional flags] In the example command above, some of the arguments might contain characters that the shell could interpret. Those are enclosed in single-quotes. Note : Because the deploy custom command ignores the first configuration argument for the --persistent-disk flag, you can specify any string. For more information, see Configuring Persistent Disk Parameters After completing the steps described in this section, proceed to Create a Complete Configuration .","title":"Configuring Object Store"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_configuring_object_store/#configuring-object-store","text":"Before reading this section, complete the steps in Configuring Persistent Disk Parameters . You can use the --object-store flag to configure Pachyderm to use an s3 storage protocol to access the configured object store. This configuration uses the Amazon S3 driver to access your on-premises object store, regardless of the vendor, since the Amazon S3 API is the standard with which every object store is designed to work. The S3 API has two different extant versions of signature styles , which are how the object store validates client requests. S3v4 is the most current version, but many S3v2 object-store servers are still in use. Because support for S3v2 is scheduled to be deprecated, Pachyderm recommends that you use S3v4 in all deployments. If you need to access an object store that uses S3v2 signatures, you can specify the --isS3V2 flag. This parameter configures Pachyderm to use the MinIO driver, which allows the use of the older signature. This --isS3V2 flag disables SSL for connections to the object store with the minio driver. You can re-enable it with the -s or --secure flag. You may also manually edit the pachyderm-storage-secret Kubernetes manifest. The --object-store flag takes four required configuration arguments. Place these arguments immediately after the persistent disk parameters : bucket-name : The name of the bucket, without the s3:// prefix or a trailing forward slash ( / ). access-key : The user access ID that is used to access the object store. secret-key : The associated password that is used with the user access ID to access the object store. endpoint : The hostname and port that are used to access the object store, in <hostname>:<port> format.","title":"Configuring Object Store"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_configuring_object_store/#example-invocation-with-a-pv-and-object-store","text":"This example on-premises cluster uses an on-premises MinIO object store with the following configuration parameters: An on-premises MinIO object store with the following parameters: SSL is enabled S3v4 signatures The endpoint is minio:9000 The access key is OBSIJRBE0PP2NO4QOA27 The secret key is tfteSlswRu7BJ86wekitnifILbZam1KYY3TG A bucket named pachyderm-bucket The deployment command uses the following flags: pachctl deploy custom --persistent-disk aws --object-store s3 \\ any-string 10 \\ pachyderm-bucket 'OBSIJRBE0PP2NO4QOA27' 'tfteSlswRu7BJ86wekitnifILbZam1KYY3TG' 'minio:9000' \\ --dynamic-etcd-nodes 1 [optional flags] In the example command above, some of the arguments might contain characters that the shell could interpret. Those are enclosed in single-quotes. Note : Because the deploy custom command ignores the first configuration argument for the --persistent-disk flag, you can specify any string. For more information, see Configuring Persistent Disk Parameters After completing the steps described in this section, proceed to Create a Complete Configuration .","title":"Example Invocation with a PV and Object Store"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_configuring_persistent_disk_parameters/","text":"Configuring Persistent Disk Parameters \u00b6 Before reading this section, complete the steps in Before You Begin . To create a custom deployment, you need to configure persistent storage that Pachyderm will use to store metadata. You can do so by using the --persistent-disk flag that creates a PersistentVolume (PV) backend on a supported provider. Pachyderm has automated configuration for styles of backend for the following major cloud providers: Amazon Web Services\u2122 (AWS) Google Cloud Platform\u2122 (GCP) Microsoft\u00ae Azure\u2122 Choosing one of these providers creates a configuration close to what you need. After carefully reading the section below, consult with your Kubernetes administrators on which provider to choose. You might need to then edit your manifest manually, based on configuration information they provide to you. For each of the providers above, the final configuration depends on which of the following flags you define: --dynamic-etcd-nodes . The --dynamic-etcd-nodes flag is used when your Kubernetes installation is configured to use StatefulSets . Many Kubernetes deployments use StatefulSets as a reliable solution that ensures the persistence of pod storage. Your on-premises Kubernetes installation might also be configured to use StatefulSets. The --dynamic-etcd-nodes flag specifies the number of etcd nodes that your deployment creates. Pachyderm recommends that you keep this number at 1 . If you want to change it, consult with your Pachyderm support team. This flag creates a VolumeClaimTemplate in the etcd StatefulSet that uses the standard etcd-storage-class . NOTE Consult with your Kubernetes administrator about the StorageClass that you should use for etcd in your Kubernetes deployment. If you need to use a different than the default setting, you can use the --etcd-storage-class flag to specify the StorageClass. --static-etc-volume . The --static-etc-volume flag is used when your Kubernetes installation has not been configured to use StatefulSets. When you specify --static-etc-volume flag, Pachyderm creates a static volume for etcd . Pachyderm creates a PV with a spec appropriate for each of the cloud providers: aws : awsElasticBlockStore for Amazon Web Services google : gcePersistentDisk for Google Cloud Storage azure : azureDisk for Microsoft Azure As stated above, the specifics of one of these choices might not match precisely what your on-premises deployment requires. To determine the closest correct choices for your on-prem infrastructure, consult with your Kubernetes administrators. You might have to then edit your manifest manually, based on configuration information they provide to you. Example invocation with persistent disk parameters \u00b6 This example on-premises cluster has StatefulSets enabled, with the standard etcd storage class configured. The deployment command uses the following flags: pachctl deploy custom --persistent-disk aws --object-store <object store backend> \\ any-string 10 \\ <object store arg 1> <object store arg 2> <object store arg 3> <object store arg 4> \\ --dynamic-etcd-nodes 1 [optional flags] The --persistent-disk flag takes two arguments that you specify right after the single argument to the --object-store flag. Although the first argument is required, Pachyderm ignores it. Therefore, you can set it to any text value, such as any-string in the example above. The second argument is the size, in gigabytes (GB), that Pachyderm requests for the etcd disk. A good value for most deployments is 10. After completing the steps described in this section, proceed to Configuring Object Store .","title":"Configuring Persistent Disk Parameters"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_configuring_persistent_disk_parameters/#configuring-persistent-disk-parameters","text":"Before reading this section, complete the steps in Before You Begin . To create a custom deployment, you need to configure persistent storage that Pachyderm will use to store metadata. You can do so by using the --persistent-disk flag that creates a PersistentVolume (PV) backend on a supported provider. Pachyderm has automated configuration for styles of backend for the following major cloud providers: Amazon Web Services\u2122 (AWS) Google Cloud Platform\u2122 (GCP) Microsoft\u00ae Azure\u2122 Choosing one of these providers creates a configuration close to what you need. After carefully reading the section below, consult with your Kubernetes administrators on which provider to choose. You might need to then edit your manifest manually, based on configuration information they provide to you. For each of the providers above, the final configuration depends on which of the following flags you define: --dynamic-etcd-nodes . The --dynamic-etcd-nodes flag is used when your Kubernetes installation is configured to use StatefulSets . Many Kubernetes deployments use StatefulSets as a reliable solution that ensures the persistence of pod storage. Your on-premises Kubernetes installation might also be configured to use StatefulSets. The --dynamic-etcd-nodes flag specifies the number of etcd nodes that your deployment creates. Pachyderm recommends that you keep this number at 1 . If you want to change it, consult with your Pachyderm support team. This flag creates a VolumeClaimTemplate in the etcd StatefulSet that uses the standard etcd-storage-class . NOTE Consult with your Kubernetes administrator about the StorageClass that you should use for etcd in your Kubernetes deployment. If you need to use a different than the default setting, you can use the --etcd-storage-class flag to specify the StorageClass. --static-etc-volume . The --static-etc-volume flag is used when your Kubernetes installation has not been configured to use StatefulSets. When you specify --static-etc-volume flag, Pachyderm creates a static volume for etcd . Pachyderm creates a PV with a spec appropriate for each of the cloud providers: aws : awsElasticBlockStore for Amazon Web Services google : gcePersistentDisk for Google Cloud Storage azure : azureDisk for Microsoft Azure As stated above, the specifics of one of these choices might not match precisely what your on-premises deployment requires. To determine the closest correct choices for your on-prem infrastructure, consult with your Kubernetes administrators. You might have to then edit your manifest manually, based on configuration information they provide to you.","title":"Configuring Persistent Disk Parameters"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_configuring_persistent_disk_parameters/#example-invocation-with-persistent-disk-parameters","text":"This example on-premises cluster has StatefulSets enabled, with the standard etcd storage class configured. The deployment command uses the following flags: pachctl deploy custom --persistent-disk aws --object-store <object store backend> \\ any-string 10 \\ <object store arg 1> <object store arg 2> <object store arg 3> <object store arg 4> \\ --dynamic-etcd-nodes 1 [optional flags] The --persistent-disk flag takes two arguments that you specify right after the single argument to the --object-store flag. Although the first argument is required, Pachyderm ignores it. Therefore, you can set it to any text value, such as any-string in the example above. The second argument is the size, in gigabytes (GB), that Pachyderm requests for the etcd disk. A good value for most deployments is 10. After completing the steps described in this section, proceed to Configuring Object Store .","title":"Example invocation with persistent disk parameters"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_pachyderm_deployment_manifest/","text":"Pachyderm Deployment Manifest \u00b6 This section provides an overview of the Kubernetes manifest that you use to deploy your Pachyderm cluster. This section is provided for your reference and does not include configuration steps. If you are familiar with Kubernetes or do not have immediate questions about the configuration parameters, you can skip this section and proceed to Configuring Persistent Disk Parameters . When you run the pachctl deploy command, Pachyderm generates a JSON-encoded Kubernetes manifest which consists of sections that describe a Pachyderm deployment. Pachyderm deploys the following sets of application components: pachd : The main Pachyderm pod. etcd : The administrative datastore for pachd . dash : The web-based UI for Pachyderm Enterprise Edition. Example: pachctl deploy custom --persistent-disk <persistent disk backend> --object-store <object store backend> \\ <persistent disk arg1> <persistent disk arg 2> \\ <object store arg 1> <object store arg 2> <object store arg 3> <object store arg 4> \\ [[--dynamic-etcd-nodes n] | [--static-etcd-volume <volume name>]] [optional flags] As you can see in the example command above, you can run the pachctl deploy custom command with different flags that generate an appropriate manifest for your infrastructure. The flags broadly fall into the following categories: Category Description --persistent-disk Configures the storage resource for etcd. Pachyderm uses etcd to manage administrative metadata. User data is stored in an object store, not in etcd. --object-store Configures the object store that Pachyderm uses for storing all user data that you want to be versioned and managed. Optional flags Optional flags that are not required to deploy Pachyderm but enable you to configure access, output format, logging verbosity, and other parameters. Kubernetes Manifest Parameters \u00b6 Your Kubernetes manifest includes sections that describe the configuration of your Pachyderm cluster. The manifest includes the following sections: Roles and permissions manifests Manifest Description ServiceAccount Typically at the top of the manifest file Pachyderm produces, a roles and permissions manifest has the kind key set to ServiceAccount . Kubernetes uses ServiceAccounts to assign namespace-specific privileges to applications in a lightweight way. Pachyderm's service account is called pachyderm . Role or ClusterRole Depending on whether you used the --local-roles flag or not, the next manifest kind is Role or ClusterRole. RoleBinding or ClusterRoleBinding This manifest binds the Role or ClusterRole to the ServiceAccount created above. Application-related manifests Manifest Description PersistentVolume If you used --static-etcd-volume to deploy Pachyderm, the value that you specify for --persistent-disk causes pachctl to write a manifest for creating a PersistentVolume that Pachyderm\u2019s etcd uses in its PersistentVolumeClaim . A common persistent volume that is used in enterprises is an NFS mount backed by a storage fabric. In this case, a StorageClass for an NFS mount is made available for consumption. Consult with your Kubernetes administrators to learn what resources are available for your deployment. PersistentVolumeClaim If you deployed Pachyderm by using --static-etc-volume , the Pachyderm's etcd store uses this PersistentVolumeClaim . See this manifest's name in the Deployment manifest for the etcd pod, described below in Pachyderm pods manifests. StorageClass If you used the --dynamic-etcd-nodes flag to deploy Pachyderm, this manifest specifies the kind of storage and provisioner that is appropriate for what you have specified in the --persistent-disk flag. Note: You will not see this manifest if you specified azure as the argument to --persistent-disk , since Azure has their own provisioner. Service In a typical Pachyderm deployment, you see three Service manifests. A Service is a Kubernetes abstraction that exposes Pods to the network. If you use StatefulSets to deploy Pachyderm, that is, you used the --dynamic-etcd-nodes flag, Pachyderm deploys one Service for etcd-headless , one for pachd , and one for dash . A static deployment has Services for etcd , pachd , and dash . If you use the --no-dashboard flag, Pachyderm does not create a Service and Deployment for the dashboard. Similarly, if --dashboard-only is specified, Pachyderm generates the manifests for the Pachyderm enterprise UI only. The most common items that you can edit in Service manifests are the NodePort values for various services, and the containerPort values for Deployment manifests. To make your containerPort values work properly, you will need to add environment variables to a Deployment or StatefulSet object. You can see how what environment variables to add in the OpenShift example. Pachyderm pods manifests Manifest Description Deployment Declares the desired state of application pods to Kubernetes. If you configure a static deployment, Pachyderm deploys Deployment manifests for etcd , pachd , and dash . If you specify --dynamic-etcd-nodes , Pachyderm deploys the pachd and dash as Deployment and etcd as a StatefulSet . If you run the deploy command with the --no-dashboard flag, Pachyderm omits the deployment of the dash Service and Deployment . StatefulSet For a --dynamic-etcd-nodes deployment, Pachyderm replaces the etcd Deployment manifest with a StatefulSet . Pachyderm Kubernetes secrets manifests Manifest Description Secret Pachyderm uses the Kubernetes Secret manifest to store the credentials that are necessary to access object storage. The final manifest uses the command-line arguments that you submit to the pachctl deploy command to store such parameters as region, secret, token, and endpoint, that are used to access an object store. The exact values in the secret depend on the kind of object store you configure for your deployment. You can update the values after the deployment either by using kubectl to deploy a new Secret or the pachctl deploy storage command.","title":"Pachyderm Deployment Manifest"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_pachyderm_deployment_manifest/#pachyderm-deployment-manifest","text":"This section provides an overview of the Kubernetes manifest that you use to deploy your Pachyderm cluster. This section is provided for your reference and does not include configuration steps. If you are familiar with Kubernetes or do not have immediate questions about the configuration parameters, you can skip this section and proceed to Configuring Persistent Disk Parameters . When you run the pachctl deploy command, Pachyderm generates a JSON-encoded Kubernetes manifest which consists of sections that describe a Pachyderm deployment. Pachyderm deploys the following sets of application components: pachd : The main Pachyderm pod. etcd : The administrative datastore for pachd . dash : The web-based UI for Pachyderm Enterprise Edition. Example: pachctl deploy custom --persistent-disk <persistent disk backend> --object-store <object store backend> \\ <persistent disk arg1> <persistent disk arg 2> \\ <object store arg 1> <object store arg 2> <object store arg 3> <object store arg 4> \\ [[--dynamic-etcd-nodes n] | [--static-etcd-volume <volume name>]] [optional flags] As you can see in the example command above, you can run the pachctl deploy custom command with different flags that generate an appropriate manifest for your infrastructure. The flags broadly fall into the following categories: Category Description --persistent-disk Configures the storage resource for etcd. Pachyderm uses etcd to manage administrative metadata. User data is stored in an object store, not in etcd. --object-store Configures the object store that Pachyderm uses for storing all user data that you want to be versioned and managed. Optional flags Optional flags that are not required to deploy Pachyderm but enable you to configure access, output format, logging verbosity, and other parameters.","title":"Pachyderm Deployment Manifest"},{"location":"deploy-manage/deploy/deploy_custom/deploy_custom_pachyderm_deployment_manifest/#kubernetes-manifest-parameters","text":"Your Kubernetes manifest includes sections that describe the configuration of your Pachyderm cluster. The manifest includes the following sections: Roles and permissions manifests Manifest Description ServiceAccount Typically at the top of the manifest file Pachyderm produces, a roles and permissions manifest has the kind key set to ServiceAccount . Kubernetes uses ServiceAccounts to assign namespace-specific privileges to applications in a lightweight way. Pachyderm's service account is called pachyderm . Role or ClusterRole Depending on whether you used the --local-roles flag or not, the next manifest kind is Role or ClusterRole. RoleBinding or ClusterRoleBinding This manifest binds the Role or ClusterRole to the ServiceAccount created above. Application-related manifests Manifest Description PersistentVolume If you used --static-etcd-volume to deploy Pachyderm, the value that you specify for --persistent-disk causes pachctl to write a manifest for creating a PersistentVolume that Pachyderm\u2019s etcd uses in its PersistentVolumeClaim . A common persistent volume that is used in enterprises is an NFS mount backed by a storage fabric. In this case, a StorageClass for an NFS mount is made available for consumption. Consult with your Kubernetes administrators to learn what resources are available for your deployment. PersistentVolumeClaim If you deployed Pachyderm by using --static-etc-volume , the Pachyderm's etcd store uses this PersistentVolumeClaim . See this manifest's name in the Deployment manifest for the etcd pod, described below in Pachyderm pods manifests. StorageClass If you used the --dynamic-etcd-nodes flag to deploy Pachyderm, this manifest specifies the kind of storage and provisioner that is appropriate for what you have specified in the --persistent-disk flag. Note: You will not see this manifest if you specified azure as the argument to --persistent-disk , since Azure has their own provisioner. Service In a typical Pachyderm deployment, you see three Service manifests. A Service is a Kubernetes abstraction that exposes Pods to the network. If you use StatefulSets to deploy Pachyderm, that is, you used the --dynamic-etcd-nodes flag, Pachyderm deploys one Service for etcd-headless , one for pachd , and one for dash . A static deployment has Services for etcd , pachd , and dash . If you use the --no-dashboard flag, Pachyderm does not create a Service and Deployment for the dashboard. Similarly, if --dashboard-only is specified, Pachyderm generates the manifests for the Pachyderm enterprise UI only. The most common items that you can edit in Service manifests are the NodePort values for various services, and the containerPort values for Deployment manifests. To make your containerPort values work properly, you will need to add environment variables to a Deployment or StatefulSet object. You can see how what environment variables to add in the OpenShift example. Pachyderm pods manifests Manifest Description Deployment Declares the desired state of application pods to Kubernetes. If you configure a static deployment, Pachyderm deploys Deployment manifests for etcd , pachd , and dash . If you specify --dynamic-etcd-nodes , Pachyderm deploys the pachd and dash as Deployment and etcd as a StatefulSet . If you run the deploy command with the --no-dashboard flag, Pachyderm omits the deployment of the dash Service and Deployment . StatefulSet For a --dynamic-etcd-nodes deployment, Pachyderm replaces the etcd Deployment manifest with a StatefulSet . Pachyderm Kubernetes secrets manifests Manifest Description Secret Pachyderm uses the Kubernetes Secret manifest to store the credentials that are necessary to access object storage. The final manifest uses the command-line arguments that you submit to the pachctl deploy command to store such parameters as region, secret, token, and endpoint, that are used to access an object store. The exact values in the secret depend on the kind of object store you configure for your deployment. You can update the values after the deployment either by using kubectl to deploy a new Secret or the pachctl deploy storage command.","title":"Kubernetes Manifest Parameters"},{"location":"deploy-manage/manage/autoscaling/","text":"Autoscaling a Pachyderm Cluster \u00b6 There are 2 levels of autoscaling in Pachyderm: Pachyderm can scale down workers when they're not in use. Cloud providers can scale workers down/up based on resource utilization (most often CPU). Pachyderm Autoscaling of Workers \u00b6 You can configure autoscaling for workers by setting the standby field in the pipeline specification. This parameter enables you suspend your pipeline pods when no data is available to process. If new inputs come in on the pipeline corresponding to the suspended workers, Kubernetes automatically resumes them. Cloud Provider Autoscaling \u00b6 Out of the box, autoscaling at the cloud provider layer doesn't work well with Pachyderm. However, if configure it properly, cloud provider autoscaling can complement Pachyderm autoscaling of workers. Default Behavior with Cloud Autoscaling \u00b6 Normally when you create a pipeline, Pachyderm asks the k8s cluster how many nodes are available. Pachyderm then uses that number as the default value for the pipeline's parallelism. (To read more about parallelism, refer to the distributed processing docs ). If you have cloud provider autoscaling activated, it is possible that your number of nodes will be scaled down to a few or maybe even a single node. A pipeline created on this cluster would have a default parallelism will be set to this low value (e.g., 1 or 2). Then, once the autoscale group notices that more nodes are needed, the parallelism of the pipeline won't increase, and you won't actually make effective use of those new nodes. Configuration of Pipelines to Complement Cloud Autoscaling \u00b6 The goal of Cloud autoscaling is to: To schedule nodes only as the processing demand necessitates it. The goals of Pachyderm worker autoscaling are: To make sure your job uses a maximum amount of parallelism. To ensure that you process the job efficiently. Thus, to accomplish both of these goals, we recommend: Setting a constant , high level of parallelism. Specifically, setting the constant parallelism to the number of workers you will need when your pipeline is active. Setting the cpu and/or mem resource requirements in the resource_requests field on your pipeline . To determine the right values for cpu / mem , first set these values rather high. Then use the monitoring tools that come with your cloud provider (or try out our monitoring deployment ) so you can see the actual CPU/mem utilization per pod. Example Scenario \u00b6 Let's say you have a certain pipeline with a constant parallelism set to 16. Let's also assume that you've set cpu to 1.0 and your instance type has 4 cores. When a commit of data is made to the input of the pipeline, your cluster might be in a scaled down state (e.g., 2 nodes running). After accounting for the pachyderm services ( pachd and etcd ), ~6 cores are available with 2 nodes. K8s then schedules 6 of your workers. That accounts for all 8 of the CPUs across the nodes in your instance group. Your autoscale group then notices that all instances are being heavily utilized, and subsequently scales up to 5 nodes total. Now the rest of your workers get spun up (k8s can now schedule them), and your job proceeds. This type of setup is best suited for long running jobs, or jobs that take a lot of CPU time. Such jobs give the cloud autoscaling mechanisms time to scale up, while still having data that needs to be processed when the new nodes are up and running.","title":"Autoscale your Cluster"},{"location":"deploy-manage/manage/autoscaling/#autoscaling-a-pachyderm-cluster","text":"There are 2 levels of autoscaling in Pachyderm: Pachyderm can scale down workers when they're not in use. Cloud providers can scale workers down/up based on resource utilization (most often CPU).","title":"Autoscaling a Pachyderm Cluster"},{"location":"deploy-manage/manage/autoscaling/#pachyderm-autoscaling-of-workers","text":"You can configure autoscaling for workers by setting the standby field in the pipeline specification. This parameter enables you suspend your pipeline pods when no data is available to process. If new inputs come in on the pipeline corresponding to the suspended workers, Kubernetes automatically resumes them.","title":"Pachyderm Autoscaling of Workers"},{"location":"deploy-manage/manage/autoscaling/#cloud-provider-autoscaling","text":"Out of the box, autoscaling at the cloud provider layer doesn't work well with Pachyderm. However, if configure it properly, cloud provider autoscaling can complement Pachyderm autoscaling of workers.","title":"Cloud Provider Autoscaling"},{"location":"deploy-manage/manage/autoscaling/#default-behavior-with-cloud-autoscaling","text":"Normally when you create a pipeline, Pachyderm asks the k8s cluster how many nodes are available. Pachyderm then uses that number as the default value for the pipeline's parallelism. (To read more about parallelism, refer to the distributed processing docs ). If you have cloud provider autoscaling activated, it is possible that your number of nodes will be scaled down to a few or maybe even a single node. A pipeline created on this cluster would have a default parallelism will be set to this low value (e.g., 1 or 2). Then, once the autoscale group notices that more nodes are needed, the parallelism of the pipeline won't increase, and you won't actually make effective use of those new nodes.","title":"Default Behavior with Cloud Autoscaling"},{"location":"deploy-manage/manage/autoscaling/#configuration-of-pipelines-to-complement-cloud-autoscaling","text":"The goal of Cloud autoscaling is to: To schedule nodes only as the processing demand necessitates it. The goals of Pachyderm worker autoscaling are: To make sure your job uses a maximum amount of parallelism. To ensure that you process the job efficiently. Thus, to accomplish both of these goals, we recommend: Setting a constant , high level of parallelism. Specifically, setting the constant parallelism to the number of workers you will need when your pipeline is active. Setting the cpu and/or mem resource requirements in the resource_requests field on your pipeline . To determine the right values for cpu / mem , first set these values rather high. Then use the monitoring tools that come with your cloud provider (or try out our monitoring deployment ) so you can see the actual CPU/mem utilization per pod.","title":"Configuration of Pipelines to Complement Cloud Autoscaling"},{"location":"deploy-manage/manage/autoscaling/#example-scenario","text":"Let's say you have a certain pipeline with a constant parallelism set to 16. Let's also assume that you've set cpu to 1.0 and your instance type has 4 cores. When a commit of data is made to the input of the pipeline, your cluster might be in a scaled down state (e.g., 2 nodes running). After accounting for the pachyderm services ( pachd and etcd ), ~6 cores are available with 2 nodes. K8s then schedules 6 of your workers. That accounts for all 8 of the CPUs across the nodes in your instance group. Your autoscale group then notices that all instances are being heavily utilized, and subsequently scales up to 5 nodes total. Now the rest of your workers get spun up (k8s can now schedule them), and your job proceeds. This type of setup is best suited for long running jobs, or jobs that take a lot of CPU time. Such jobs give the cloud autoscaling mechanisms time to scale up, while still having data that needs to be processed when the new nodes are up and running.","title":"Example Scenario"},{"location":"deploy-manage/manage/backup_restore/","text":"Backup and Restore \u00b6 Pachyderm provides the pachctl extract and pachctl restore commands to backup and restore the state of a Pachyderm cluster. The pachctl extract command requires that all pipeline and data loading activity into Pachyderm stop before the extract occurs. This enables Pachyderm to create a consistent, point-in-time backup. In this document, we'll talk about how to create such a backup and restore it to another Pachyderm instance. Extract and restore commands are currently used to migrate between minor and major releases of Pachyderm, so it's important to understand how to perform them properly. In addition, there are a few design points and operational techniques that data engineers should take into consideration when creating complex pachyderm deployments to minimize disruptions to production pipelines. In this document, we'll take you through the steps to backup and restore a cluster, migrate an existing cluster to a newer minor or major release, and elaborate on some of those design and operations considerations. Backup and restore concepts \u00b6 Backing up Pachyderm involves the persistent volume (PV) that etcd uses for administrative data and the object store bucket that holds Pachyderm's actual data. Restoring involves populating that PV and object store with data to recreate a Pachyderm cluster. General backup procedure \u00b6 1. Pause all pipeline and data loading/unloading operations \u00b6 Before you begin, you need to pause all pipelines and data operations. Pausing pipelines \u00b6 From the directed acyclic graphs (DAG) that define your pachyderm cluster, stop each pipeline. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the pachctl stop pipeline command. pachctl stop pipeline <pipeline-name> You can confirm each pipeline is paused using the pachctl list pipeline command pachctl list pipeline Alternatively, a useful shell script for running stop pipeline on all pipelines is included below. It may be necessary to install the utilities used in the script, like jq and xargs , on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl stop pipeline {} It's also a useful practice, for simple to moderately complex deployments, to keep a terminal window up showing the state of all running kubernetes pods. watch -n 5 kubectl get pods You may need to install the watch and kubectl commands on your system, and configure kubectl to point at the cluster that Pachyderm is running in. Pausing data loading operations \u00b6 Input repositories or input repos in pachyderm are repositories created with the pachctl create repo command. They're designed to be the repos at the top of a directed acyclic graph of pipelines. Pipelines have their own output repos associated with them, and are not considered input repos. If there are any processes external to pachyderm that put data into input repos using any method (the Pachyderm APIs, pachctl put file , etc.), they need to be paused. See Loading data from other sources into pachyderm below for design considerations for those processes that will minimize downtime during a restore or migration. Alternatively, you can use the following commands to stop all data loading into Pachyderm from outside processes. # Once you have stopped all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl stop pipeline {} # all pipelines in your cluster should be suspended. To stop all # data loading processes, we're going to modify the pachd Kubernetes service so that # it only accepts traffic on port 30649 (instead of the usual 30650). This way, # any background users and services that send requests to your Pachyderm cluster # while 'extract' is running will not interfere with the process # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pachd -o json >pach_service_backup_30650.json # Modify the service to accept traffic on port 30649 # Note that you'll likely also need to modify your cloud provider's firewall # rules to allow traffic on this port $ kubectl get svc/pachd -o json | sed 's/30650/30649/g' | kubectl apply -f - # Modify your environment so that *you* can talk to pachd on this new port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30649 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pachctl version COMPONENT VERSION pachctl 1.9.5 pachd 1.9.5 2. Extract a pachyderm backup \u00b6 You can use pachctl extract alone or in combination with cloning/snapshotting services. Using pachctl extract \u00b6 Using the pachctl extract command, create the backup you need. pachctl extract > path/to/your/backup/file You can also use the -u or --url flag to put the backup directly into an object store. pachctl extract --url s3://... If you are planning on backing up the object store using its own built-in clone operation, be sure to add the --no-objects flag to the pachctl extract command. Using your cloud provider's clone and snapshot services \u00b6 You should follow your cloud provider's recommendation for backing up these resources. Here are some pointers to the relevant documentation. Snapshotting persistent volumes \u00b6 For example, here are official guides on creating snapshots of persistent volumes on Google Cloud Platform, Amazon Web Services (AWS) and Microsoft Azure, respectively: Creating snapshots of GCE persistent volumes Creating snapshots of Elastic Block Store (EBS) volumes Creating snapshots of Azure Virtual Hard Disk volumes For on-premises Kubernetes deployments, check the vendor documentation for your PV implementation on backing up and restoring. Cloning object stores \u00b6 Below, we give an example using the Amazon Web Services CLI to clone one bucket to another, taken from the documentation for that command . Similar commands are available for Google Cloud and Azure blob storage . aws s3 sync s3://mybucket s3://mybucket2 For on-premises Kubernetes deployments, check the vendor documentation for your on-premises object store for details on backing up and restoring a bucket. Combining cloning, snapshots and extract/restore \u00b6 You can use pachctl extract command with the --no-objects flag to exclude the object store, and use an object store snapshot or clone command to back up the object store. You can run the two commands at the same time. For example, on Amazon Web Services, the following commands can be run simultaneously. aws s3 sync s3://mybucket s3://mybucket2 pachctl extract --no-objects --url s3://anotherbucket Use case: minimizing downtime during a migration \u00b6 The above cloning/snapshotting technique is recommended when doing a migration where minimizing downtime is desirable, as it allows the duplicated object store to be the basis of the upgraded, new cluster instead of requiring Pachyderm to extract the data from object store. 3. Restart all pipeline and data loading operations \u00b6 Once the backup is complete, restart all paused pipelines and data loading operations. From the directed acyclic graphs (DAG) that define your pachyderm cluster, start each pipeline. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the pachctl start pipeline command. pachctl start pipeline <pipeline-name> You can confirm each pipeline is started using the list pipeline command pachctl list pipeline A useful shell script for running start pipeline on all pipelines is included below. It may be necessary to install the utilities used in the script, like jq and xargs , on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl start pipeline {} If you used the port-changing technique, above , to stop all data loading into Pachyderm from outside processes, you should change the ports back. # Once you have restarted all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl start pipeline {} # all pipelines in your cluster should be restarted. To restart all data loading # processes, we're going to change the pachd Kubernetes service so that # it only accepts traffic on port 30650 again (from 30649). # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pachd -o json >pach_service_backup_30649.json # Modify the service to accept traffic on port 30650, again $ kubectl get svc/pachd -o json | sed 's/30649/30650/g' | kubectl apply -f - # Modify your environment so that *you* can talk to pachd on the old port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30650 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pachctl version COMPONENT VERSION pachctl 1.9.5 pachd 1.9.5 General restore procedure \u00b6 Restore your backup to a pachyderm cluster, same version \u00b6 Spin up a Pachyderm cluster and run pachctl restore with the backup you created earlier. pachctl restore < path/to/your/backup/file You can also use the -u or --url flag to get the backup directly from the object store you placed it in pachctl restore --url s3://... Loading data from other sources into Pachyderm \u00b6 When writing systems that place data into Pachyderm input repos (see above for a definition of 'input repo'), it is important to provide ways of 'pausing' output while queueing any data output requests to be output when the systems are 'resumed'. This allows all Pachyderm processing to be stopped while the extract takes place. In addition, it is desirable for systems that load data into Pachyderm have a mechanism for replaying a queue from any checkpoint in time. This is useful when doing migrations from one release to another, where you would like to minimize downtime of a production Pachyderm system. After an extract, the old system is kept running with the checkpoint established while a duplicate, upgraded pachyderm cluster is migrated with duplicated data. Transactions that occur while the migrated, upgraded cluster is being brought up are not lost,","title":"Backup and Restore"},{"location":"deploy-manage/manage/backup_restore/#backup-and-restore","text":"Pachyderm provides the pachctl extract and pachctl restore commands to backup and restore the state of a Pachyderm cluster. The pachctl extract command requires that all pipeline and data loading activity into Pachyderm stop before the extract occurs. This enables Pachyderm to create a consistent, point-in-time backup. In this document, we'll talk about how to create such a backup and restore it to another Pachyderm instance. Extract and restore commands are currently used to migrate between minor and major releases of Pachyderm, so it's important to understand how to perform them properly. In addition, there are a few design points and operational techniques that data engineers should take into consideration when creating complex pachyderm deployments to minimize disruptions to production pipelines. In this document, we'll take you through the steps to backup and restore a cluster, migrate an existing cluster to a newer minor or major release, and elaborate on some of those design and operations considerations.","title":"Backup and Restore"},{"location":"deploy-manage/manage/backup_restore/#backup-and-restore-concepts","text":"Backing up Pachyderm involves the persistent volume (PV) that etcd uses for administrative data and the object store bucket that holds Pachyderm's actual data. Restoring involves populating that PV and object store with data to recreate a Pachyderm cluster.","title":"Backup and restore concepts"},{"location":"deploy-manage/manage/backup_restore/#general-backup-procedure","text":"","title":"General backup procedure"},{"location":"deploy-manage/manage/backup_restore/#1-pause-all-pipeline-and-data-loadingunloading-operations","text":"Before you begin, you need to pause all pipelines and data operations.","title":"1. Pause all pipeline and data loading/unloading operations"},{"location":"deploy-manage/manage/backup_restore/#pausing-pipelines","text":"From the directed acyclic graphs (DAG) that define your pachyderm cluster, stop each pipeline. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the pachctl stop pipeline command. pachctl stop pipeline <pipeline-name> You can confirm each pipeline is paused using the pachctl list pipeline command pachctl list pipeline Alternatively, a useful shell script for running stop pipeline on all pipelines is included below. It may be necessary to install the utilities used in the script, like jq and xargs , on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl stop pipeline {} It's also a useful practice, for simple to moderately complex deployments, to keep a terminal window up showing the state of all running kubernetes pods. watch -n 5 kubectl get pods You may need to install the watch and kubectl commands on your system, and configure kubectl to point at the cluster that Pachyderm is running in.","title":"Pausing pipelines"},{"location":"deploy-manage/manage/backup_restore/#pausing-data-loading-operations","text":"Input repositories or input repos in pachyderm are repositories created with the pachctl create repo command. They're designed to be the repos at the top of a directed acyclic graph of pipelines. Pipelines have their own output repos associated with them, and are not considered input repos. If there are any processes external to pachyderm that put data into input repos using any method (the Pachyderm APIs, pachctl put file , etc.), they need to be paused. See Loading data from other sources into pachyderm below for design considerations for those processes that will minimize downtime during a restore or migration. Alternatively, you can use the following commands to stop all data loading into Pachyderm from outside processes. # Once you have stopped all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl stop pipeline {} # all pipelines in your cluster should be suspended. To stop all # data loading processes, we're going to modify the pachd Kubernetes service so that # it only accepts traffic on port 30649 (instead of the usual 30650). This way, # any background users and services that send requests to your Pachyderm cluster # while 'extract' is running will not interfere with the process # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pachd -o json >pach_service_backup_30650.json # Modify the service to accept traffic on port 30649 # Note that you'll likely also need to modify your cloud provider's firewall # rules to allow traffic on this port $ kubectl get svc/pachd -o json | sed 's/30650/30649/g' | kubectl apply -f - # Modify your environment so that *you* can talk to pachd on this new port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30649 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pachctl version COMPONENT VERSION pachctl 1.9.5 pachd 1.9.5","title":"Pausing data loading operations"},{"location":"deploy-manage/manage/backup_restore/#2-extract-a-pachyderm-backup","text":"You can use pachctl extract alone or in combination with cloning/snapshotting services.","title":"2. Extract a pachyderm backup"},{"location":"deploy-manage/manage/backup_restore/#using-pachctl-extract","text":"Using the pachctl extract command, create the backup you need. pachctl extract > path/to/your/backup/file You can also use the -u or --url flag to put the backup directly into an object store. pachctl extract --url s3://... If you are planning on backing up the object store using its own built-in clone operation, be sure to add the --no-objects flag to the pachctl extract command.","title":"Using pachctl extract"},{"location":"deploy-manage/manage/backup_restore/#using-your-cloud-providers-clone-and-snapshot-services","text":"You should follow your cloud provider's recommendation for backing up these resources. Here are some pointers to the relevant documentation.","title":"Using your cloud provider's clone and snapshot services"},{"location":"deploy-manage/manage/backup_restore/#snapshotting-persistent-volumes","text":"For example, here are official guides on creating snapshots of persistent volumes on Google Cloud Platform, Amazon Web Services (AWS) and Microsoft Azure, respectively: Creating snapshots of GCE persistent volumes Creating snapshots of Elastic Block Store (EBS) volumes Creating snapshots of Azure Virtual Hard Disk volumes For on-premises Kubernetes deployments, check the vendor documentation for your PV implementation on backing up and restoring.","title":"Snapshotting persistent volumes"},{"location":"deploy-manage/manage/backup_restore/#cloning-object-stores","text":"Below, we give an example using the Amazon Web Services CLI to clone one bucket to another, taken from the documentation for that command . Similar commands are available for Google Cloud and Azure blob storage . aws s3 sync s3://mybucket s3://mybucket2 For on-premises Kubernetes deployments, check the vendor documentation for your on-premises object store for details on backing up and restoring a bucket.","title":"Cloning object stores"},{"location":"deploy-manage/manage/backup_restore/#combining-cloning-snapshots-and-extractrestore","text":"You can use pachctl extract command with the --no-objects flag to exclude the object store, and use an object store snapshot or clone command to back up the object store. You can run the two commands at the same time. For example, on Amazon Web Services, the following commands can be run simultaneously. aws s3 sync s3://mybucket s3://mybucket2 pachctl extract --no-objects --url s3://anotherbucket","title":"Combining cloning, snapshots and extract/restore"},{"location":"deploy-manage/manage/backup_restore/#use-case-minimizing-downtime-during-a-migration","text":"The above cloning/snapshotting technique is recommended when doing a migration where minimizing downtime is desirable, as it allows the duplicated object store to be the basis of the upgraded, new cluster instead of requiring Pachyderm to extract the data from object store.","title":"Use case: minimizing downtime during a migration"},{"location":"deploy-manage/manage/backup_restore/#3-restart-all-pipeline-and-data-loading-operations","text":"Once the backup is complete, restart all paused pipelines and data loading operations. From the directed acyclic graphs (DAG) that define your pachyderm cluster, start each pipeline. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the pachctl start pipeline command. pachctl start pipeline <pipeline-name> You can confirm each pipeline is started using the list pipeline command pachctl list pipeline A useful shell script for running start pipeline on all pipelines is included below. It may be necessary to install the utilities used in the script, like jq and xargs , on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl start pipeline {} If you used the port-changing technique, above , to stop all data loading into Pachyderm from outside processes, you should change the ports back. # Once you have restarted all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl start pipeline {} # all pipelines in your cluster should be restarted. To restart all data loading # processes, we're going to change the pachd Kubernetes service so that # it only accepts traffic on port 30650 again (from 30649). # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pachd -o json >pach_service_backup_30649.json # Modify the service to accept traffic on port 30650, again $ kubectl get svc/pachd -o json | sed 's/30649/30650/g' | kubectl apply -f - # Modify your environment so that *you* can talk to pachd on the old port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30650 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pachctl version COMPONENT VERSION pachctl 1.9.5 pachd 1.9.5","title":"3. Restart all pipeline and data loading operations"},{"location":"deploy-manage/manage/backup_restore/#general-restore-procedure","text":"","title":"General restore procedure"},{"location":"deploy-manage/manage/backup_restore/#restore-your-backup-to-a-pachyderm-cluster-same-version","text":"Spin up a Pachyderm cluster and run pachctl restore with the backup you created earlier. pachctl restore < path/to/your/backup/file You can also use the -u or --url flag to get the backup directly from the object store you placed it in pachctl restore --url s3://...","title":"Restore your backup to a pachyderm cluster, same version"},{"location":"deploy-manage/manage/backup_restore/#loading-data-from-other-sources-into-pachyderm","text":"When writing systems that place data into Pachyderm input repos (see above for a definition of 'input repo'), it is important to provide ways of 'pausing' output while queueing any data output requests to be output when the systems are 'resumed'. This allows all Pachyderm processing to be stopped while the extract takes place. In addition, it is desirable for systems that load data into Pachyderm have a mechanism for replaying a queue from any checkpoint in time. This is useful when doing migrations from one release to another, where you would like to minimize downtime of a production Pachyderm system. After an extract, the old system is kept running with the checkpoint established while a duplicate, upgraded pachyderm cluster is migrated with duplicated data. Transactions that occur while the migrated, upgraded cluster is being brought up are not lost,","title":"Loading data from other sources into Pachyderm"},{"location":"deploy-manage/manage/batching_pachyderm_with_transactions/","text":"Batching Pachyderm with Transactions \u00b6 Transactions were added to Pachyderm as a way to make multiple changes to the state of Pachyderm while only triggering jobs once. This is done by constructing a batch of operations to perform on the cluster state, then running the set of operations in a single ETCD transaction. The transaction framework provides a method for batching together commit propagation such that changed branches are collected over the course of the transaction and all propagated in one batch at the end. This allows Pachyderm to dedupe changed branches and branches provenant on the changed branches so that the minimum number of new commits are issued. This is useful in particular for pipelines with multiple inputs. If you need to update two or more input repos, you might not want pipeline jobs for each state change. You can issue a transaction to start commits in each of the input repos, which will create a single downstream commit in the pipeline repo. After the transaction, you can put files and finish the commits at will, and the pipeline job will run once all the input commits have been finished. Pachctl \u00b6 In pachctl, a transaction can be initiated through the start transaction command. This will generate a transaction object in the cluster and save its ID into the local pachyderm config ( ~/.pachyderm/config.json by default). While there is a transaction object in the config file, all transactionally-supported API requests will append the request to the transaction instead of running directly. These commands include: Example create repo delete repo start commit finish commit delete commit create branch delete branch Each time a command is added to a transaction, the transaction is dry-run against the current state of the cluster metadata to make sure it is still valid and to obtain any return values (important for commands like start commit ). If the dry-run fails for any reason, the operation will not be added to the transaction. If the transaction has been invalidated by changing cluster state, the transaction will need to be deleted and started over, taking into account the new state of the cluster. From a command-line perspective, these commands should work identically within a transaction as without with the exception that the changes will not be committed until finish transaction is run, and a message will be logged to stderr to indicate that the command was placed in a transaction rather than run directly. There are several other supporting commands for transactions: list transaction - list all unfinished transactions available in the pachyderm cluster stop transaction - remove the currently active transaction from the local pachyderm config file - it remains in the pachyderm cluster and may be resumed later resume transaction - set an already-existing transaction as the active transaction in the local pachyderm config file delete transaction - deletes a transaction from the pachyderm cluster inspect transaction - provide detailed information about an existing transaction, including which operations it will perform","title":"Batching Pachyderm with Transactions"},{"location":"deploy-manage/manage/batching_pachyderm_with_transactions/#batching-pachyderm-with-transactions","text":"Transactions were added to Pachyderm as a way to make multiple changes to the state of Pachyderm while only triggering jobs once. This is done by constructing a batch of operations to perform on the cluster state, then running the set of operations in a single ETCD transaction. The transaction framework provides a method for batching together commit propagation such that changed branches are collected over the course of the transaction and all propagated in one batch at the end. This allows Pachyderm to dedupe changed branches and branches provenant on the changed branches so that the minimum number of new commits are issued. This is useful in particular for pipelines with multiple inputs. If you need to update two or more input repos, you might not want pipeline jobs for each state change. You can issue a transaction to start commits in each of the input repos, which will create a single downstream commit in the pipeline repo. After the transaction, you can put files and finish the commits at will, and the pipeline job will run once all the input commits have been finished.","title":"Batching Pachyderm with Transactions"},{"location":"deploy-manage/manage/batching_pachyderm_with_transactions/#pachctl","text":"In pachctl, a transaction can be initiated through the start transaction command. This will generate a transaction object in the cluster and save its ID into the local pachyderm config ( ~/.pachyderm/config.json by default). While there is a transaction object in the config file, all transactionally-supported API requests will append the request to the transaction instead of running directly. These commands include: Example create repo delete repo start commit finish commit delete commit create branch delete branch Each time a command is added to a transaction, the transaction is dry-run against the current state of the cluster metadata to make sure it is still valid and to obtain any return values (important for commands like start commit ). If the dry-run fails for any reason, the operation will not be added to the transaction. If the transaction has been invalidated by changing cluster state, the transaction will need to be deleted and started over, taking into account the new state of the cluster. From a command-line perspective, these commands should work identically within a transaction as without with the exception that the changes will not be committed until finish transaction is run, and a message will be logged to stderr to indicate that the command was placed in a transaction rather than run directly. There are several other supporting commands for transactions: list transaction - list all unfinished transactions available in the pachyderm cluster stop transaction - remove the currently active transaction from the local pachyderm config file - it remains in the pachyderm cluster and may be resumed later resume transaction - set an already-existing transaction as the active transaction in the local pachyderm config file delete transaction - deletes a transaction from the pachyderm cluster inspect transaction - provide detailed information about an existing transaction, including which operations it will perform","title":"Pachctl"},{"location":"deploy-manage/manage/cluster-access/","text":"Manage Cluster Access \u00b6 Pachyderm contexts enable you to store configuration parameters for multiple Pachyderm clusters in a single configuration file saved at ~/.pachyderm/config.json . This file stores the information about all Pachyderm clusters that you have deployed from your machine locally or on a remote server. For example, if you have a cluster that is deployed locally in minikube and another one deployed on Amazon EKS, configurations for these clusters are stored in that config.json file. By default, all local cluster configurations have the local prefix. If you have multiple local clusters, Pachyderm adds a consecutive number to the local prefix of each cluster. The following text is an example of a Pachyderm config.json file: { \"user_id\" : \"b4fe4317-be21-4836-824f-6661c68b8fba\" , \"v2\" : { \"active_context\" : \"local-1\" , \"contexts\" : { \"default\" : {} , \"local\" : {} , \"local-1\" : {} , } , \"metrics\" : true } } View the Active Context \u00b6 When you have multiple Pachyderm clusters, you can switch between them by setting the current context. The active context is the cluster that you interact with when you run pachctl commands. To view active context, type: View the active context: $ pachctl config get active-context local-1 List all contexts and view the current context: $ pachctl config list context ACTIVE NAME default local * local-1 The active context is marked with an asterisk. Change the Active Context \u00b6 To change the active context, type pachctl config set active-context <name> . Also, you can set the PACH_CONTEXT environmental variable that overrides the active context. Example: export PACH_CONTEXT = local1 Create a New Context \u00b6 When you deploy a new Pachyderm cluster, a new context that points to the new cluster is created automatically. In addition, you can create a new context by providing your parameters through the standard input stream ( stdin ) in your terminal. Specify the parameters as a comma-separated list enclosed in curly brackets. Note By default, the pachd port is 30650 . To create a new context with specific parameters, complete the following steps: Create a new Pachyderm context with a specific pachd IP address and a client certificate: $ echo '{\"pachd_address\":\"10.10.10.130:650\", \"server_cas\":\"key.pem\"}' | pachctl config set context new-local Reading from stdin Verify your configuration by running the following command: $ pachctl config get context new-local { \"pachd_address\" : \"10.10.10.130:650\" , \"server_cas\" : \"key.pem\" } Update an Existing Context \u00b6 You can update an existing context with new parameters, such as a Pachyderm IP address, certificate authority (CA), and others. For the list of parameters, see Pachyderm Config Specification . To update the Active Context, run the following commands: Update the context with a new pachd address: $ pachctl config update context local-1 --pachd-address 10 .10.10.131 The pachctl config update command supports the --pachd-address flag only. Verify that the context has been updated: $ pachctl config get context local-1 { \"pachd_address\" : \"10.10.10.131\" } Alternatively, you can update multiple properties by using an echo script: $ echo '{\"pachd_address\":\"10.10.10.132\", \"server_cas\":\"key.pem\"}' | pachctl config set context local-1 --overwrite Reading from stdin. Verify that the changes were applied: $ pachctl config get context local-1 { \"pachd_address\" : \"10.10.10.132\" , \"server_cas\" : \"key.pem\" }","title":"Manage Cluster Access"},{"location":"deploy-manage/manage/cluster-access/#manage-cluster-access","text":"Pachyderm contexts enable you to store configuration parameters for multiple Pachyderm clusters in a single configuration file saved at ~/.pachyderm/config.json . This file stores the information about all Pachyderm clusters that you have deployed from your machine locally or on a remote server. For example, if you have a cluster that is deployed locally in minikube and another one deployed on Amazon EKS, configurations for these clusters are stored in that config.json file. By default, all local cluster configurations have the local prefix. If you have multiple local clusters, Pachyderm adds a consecutive number to the local prefix of each cluster. The following text is an example of a Pachyderm config.json file: { \"user_id\" : \"b4fe4317-be21-4836-824f-6661c68b8fba\" , \"v2\" : { \"active_context\" : \"local-1\" , \"contexts\" : { \"default\" : {} , \"local\" : {} , \"local-1\" : {} , } , \"metrics\" : true } }","title":"Manage Cluster Access"},{"location":"deploy-manage/manage/cluster-access/#view-the-active-context","text":"When you have multiple Pachyderm clusters, you can switch between them by setting the current context. The active context is the cluster that you interact with when you run pachctl commands. To view active context, type: View the active context: $ pachctl config get active-context local-1 List all contexts and view the current context: $ pachctl config list context ACTIVE NAME default local * local-1 The active context is marked with an asterisk.","title":"View the Active Context"},{"location":"deploy-manage/manage/cluster-access/#change-the-active-context","text":"To change the active context, type pachctl config set active-context <name> . Also, you can set the PACH_CONTEXT environmental variable that overrides the active context. Example: export PACH_CONTEXT = local1","title":"Change the Active Context"},{"location":"deploy-manage/manage/cluster-access/#create-a-new-context","text":"When you deploy a new Pachyderm cluster, a new context that points to the new cluster is created automatically. In addition, you can create a new context by providing your parameters through the standard input stream ( stdin ) in your terminal. Specify the parameters as a comma-separated list enclosed in curly brackets. Note By default, the pachd port is 30650 . To create a new context with specific parameters, complete the following steps: Create a new Pachyderm context with a specific pachd IP address and a client certificate: $ echo '{\"pachd_address\":\"10.10.10.130:650\", \"server_cas\":\"key.pem\"}' | pachctl config set context new-local Reading from stdin Verify your configuration by running the following command: $ pachctl config get context new-local { \"pachd_address\" : \"10.10.10.130:650\" , \"server_cas\" : \"key.pem\" }","title":"Create a New Context"},{"location":"deploy-manage/manage/cluster-access/#update-an-existing-context","text":"You can update an existing context with new parameters, such as a Pachyderm IP address, certificate authority (CA), and others. For the list of parameters, see Pachyderm Config Specification . To update the Active Context, run the following commands: Update the context with a new pachd address: $ pachctl config update context local-1 --pachd-address 10 .10.10.131 The pachctl config update command supports the --pachd-address flag only. Verify that the context has been updated: $ pachctl config get context local-1 { \"pachd_address\" : \"10.10.10.131\" } Alternatively, you can update multiple properties by using an echo script: $ echo '{\"pachd_address\":\"10.10.10.132\", \"server_cas\":\"key.pem\"}' | pachctl config set context local-1 --overwrite Reading from stdin. Verify that the changes were applied: $ pachctl config get context local-1 { \"pachd_address\" : \"10.10.10.132\" , \"server_cas\" : \"key.pem\" }","title":"Update an Existing Context"},{"location":"deploy-manage/manage/data_management/","text":"Storage Use Optimization \u00b6 This section discusses best practices for minimizing the space needed to store your Pachyderm data, increasing the performance of your data processing as related to data organization, and general good ideas when you are using Pachyderm to version/process your data. Garbage collection \u00b6 When a file, commit, repo is deleted, the data is not immediately removed from the underlying storage system, such as S3, for performance and architectural reasons. This is similar to how when you delete a file on your computer, the file is not necessarily wiped from disk immediately. To actually remove the data, you may need to manually invoke garbage collection. The easiest way to do it is through pachctl garbage-collect . You can start pachctl garbage-collect only when no active jobs are running. Also, you need to ensure that all pachctl put file operations have been completed. Garbage collection puts the cluster into a read-only mode where no new jobs can be created and no data can be added. Setting a root volume size \u00b6 When planning and configuring your Pachyderm deployment, you need to make sure that each node's root volume is big enough to accommodate your total processing bandwidth. Specifically, you should calculate the bandwidth for your expected running jobs as follows: ( storage needed per datum ) x ( number of datums being processed simultaneously ) / ( number of nodes ) Here, the storage needed per datum must be the storage needed for the largest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum. If your root volume size is not large enough, pipelines might fail when downloading the input. The pod would get evicted and rescheduled to a different node, where the same thing might happen (assuming that node had a similar volume). See also: Troubleshoot a pipeline","title":"Storage Use Optimization"},{"location":"deploy-manage/manage/data_management/#storage-use-optimization","text":"This section discusses best practices for minimizing the space needed to store your Pachyderm data, increasing the performance of your data processing as related to data organization, and general good ideas when you are using Pachyderm to version/process your data.","title":"Storage Use Optimization"},{"location":"deploy-manage/manage/data_management/#garbage-collection","text":"When a file, commit, repo is deleted, the data is not immediately removed from the underlying storage system, such as S3, for performance and architectural reasons. This is similar to how when you delete a file on your computer, the file is not necessarily wiped from disk immediately. To actually remove the data, you may need to manually invoke garbage collection. The easiest way to do it is through pachctl garbage-collect . You can start pachctl garbage-collect only when no active jobs are running. Also, you need to ensure that all pachctl put file operations have been completed. Garbage collection puts the cluster into a read-only mode where no new jobs can be created and no data can be added.","title":"Garbage collection"},{"location":"deploy-manage/manage/data_management/#setting-a-root-volume-size","text":"When planning and configuring your Pachyderm deployment, you need to make sure that each node's root volume is big enough to accommodate your total processing bandwidth. Specifically, you should calculate the bandwidth for your expected running jobs as follows: ( storage needed per datum ) x ( number of datums being processed simultaneously ) / ( number of nodes ) Here, the storage needed per datum must be the storage needed for the largest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum. If your root volume size is not large enough, pipelines might fail when downloading the input. The pod would get evicted and rescheduled to a different node, where the same thing might happen (assuming that node had a similar volume). See also: Troubleshoot a pipeline","title":"Setting a root volume size"},{"location":"deploy-manage/manage/disable-metrics/","text":"Disable Usage Metrics \u00b6 Pachyderm automatically collects and reports anonymous usage metrics. These metrics help the Pachyderm team understand how people use Pachyderm to make it better. If you want opt out of anonymous metrics collection, disable them by setting the METRICS environment variable to false in the pachd container.","title":"Disable Usage Metrics"},{"location":"deploy-manage/manage/disable-metrics/#disable-usage-metrics","text":"Pachyderm automatically collects and reports anonymous usage metrics. These metrics help the Pachyderm team understand how people use Pachyderm to make it better. If you want opt out of anonymous metrics collection, disable them by setting the METRICS environment variable to false in the pachd container.","title":"Disable Usage Metrics"},{"location":"deploy-manage/manage/gpus/","text":"Use GPUs \u00b6 Pachyderm currently supports GPUs through Kubernetes device plugins. If you already have a GPU enabled Kubernetes cluster through device plugins, skip to Configure GPUs in Pipelines . Set up a GPU-enabled Kubernetes Cluster \u00b6 For instructions on how to set up a GPU-enabled Kubernetes cluster through device plugins, see the Kubernetes documentation . Depending your hardware and applications, setting up a GPU-enabled Kubernetes cluster might require significant effort. If you are run into issues, verify that the following issues are addressed: The correct software is installed on the GPU machines so that applications that run in Docker containers can use the GPUs. This is highly dependent on the manufacturer of the GPUs and how you use them. The most straightforward approach is to get a VM image with this pre-installed and use management software such as kops nvidia-device-plugin . Kubernetes exposes the GPU resources. You can check this by describing the GPU nodes with kubectl describe node . If the GPU resources available configured correctly, you should see them as available for scheduling. Your application can access and use the GPUs. This may be as simple as making shared libraries accessible by the application that runs in your container. You can configure this by injecting environment variables into the Docker image or passing environment variables through the pipeline spec. Configure GPUs in Pipelines \u00b6 If you already have a GPU-enabled Kubernetes cluster through device plugins, then using GPUs in your pipelines is as simple as setting up a GPU resource limit with the type and number of GPUs. The following text is an example of a pipeline spec for a GPU-enabled pipeline: Example { \"pipeline\" : { \"name\" : \"train\" }, \"transform\" : { \"image\" : \"acme/your-gpu-image\" , \"cmd\" : [ \"python\" , \"train.py\" ], }, \"resource_limits\" : { \"memory\" : \"1024M\" , \"gpu\" : { \"type\" : \"nvidia.com/gpu\" , \"number\" : 1 } }, \"inputs\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } ] }","title":"Use GPUs"},{"location":"deploy-manage/manage/gpus/#use-gpus","text":"Pachyderm currently supports GPUs through Kubernetes device plugins. If you already have a GPU enabled Kubernetes cluster through device plugins, skip to Configure GPUs in Pipelines .","title":"Use GPUs"},{"location":"deploy-manage/manage/gpus/#set-up-a-gpu-enabled-kubernetes-cluster","text":"For instructions on how to set up a GPU-enabled Kubernetes cluster through device plugins, see the Kubernetes documentation . Depending your hardware and applications, setting up a GPU-enabled Kubernetes cluster might require significant effort. If you are run into issues, verify that the following issues are addressed: The correct software is installed on the GPU machines so that applications that run in Docker containers can use the GPUs. This is highly dependent on the manufacturer of the GPUs and how you use them. The most straightforward approach is to get a VM image with this pre-installed and use management software such as kops nvidia-device-plugin . Kubernetes exposes the GPU resources. You can check this by describing the GPU nodes with kubectl describe node . If the GPU resources available configured correctly, you should see them as available for scheduling. Your application can access and use the GPUs. This may be as simple as making shared libraries accessible by the application that runs in your container. You can configure this by injecting environment variables into the Docker image or passing environment variables through the pipeline spec.","title":"Set up a GPU-enabled Kubernetes Cluster"},{"location":"deploy-manage/manage/gpus/#configure-gpus-in-pipelines","text":"If you already have a GPU-enabled Kubernetes cluster through device plugins, then using GPUs in your pipelines is as simple as setting up a GPU resource limit with the type and number of GPUs. The following text is an example of a pipeline spec for a GPU-enabled pipeline: Example { \"pipeline\" : { \"name\" : \"train\" }, \"transform\" : { \"image\" : \"acme/your-gpu-image\" , \"cmd\" : [ \"python\" , \"train.py\" ], }, \"resource_limits\" : { \"memory\" : \"1024M\" , \"gpu\" : { \"type\" : \"nvidia.com/gpu\" , \"number\" : 1 } }, \"inputs\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } ] }","title":"Configure GPUs in Pipelines"},{"location":"deploy-manage/manage/migrations/","text":"Migration \u00b6 Info If you need to upgrade Pachyderm from one minor version to another, such as from 1.9.4 to 1.9.5, see Upgrade Pachyderm . Introduction [Note about 1.7 to 1.8 migrations](#note-about-1-7-to-1-8-migrations] General migration procedure Before you start: backups Migration steps 1. Pause all pipeline and data loading operations 2. Extract a pachyderm backup with the --no-objects flag 3. Clone your object store bucket 4. Restart all pipeline and data loading ops 5. Deploy a 1.X Pachyderm cluster with cloned bucket 6. Restore the new 1.X Pachyderm cluster from your backup 7. Load transactional data from checkpoint into new cluster 8. Disable the old cluster 9. Reconfigure new cluster as necessary Introduction \u00b6 As new versions of Pachyderm are released, you may need to update your cluster to get access to bug fixes and new features. These updates fall into two categories, upgrades and migrations. An upgrade is moving between point releases within the same major release, like 1.7.2 to 1.7.3. Upgrades are typically a simple process that require little to no downtime. Migrations involve moving between major releases, like 1.8.6 to 1.9.0. Migration is covered in this document. In general, Pachyderm stores all of its state in two places: etcd (which in turn stores its state in one or more persistent volumes, which were created when the Pachyderm cluster was deployed) and an object store bucket (something like AWS S3, MinIO, or Azure Blob Storage). In a migration, the data structures stored in those locations need to be read, transformed, and rewritten, so the process involves: bringing up a new Pachyderm cluster adjacent to the old pachyderm cluster exporting the old Pachdyerm cluster's repos, pipelines, and input commits importing the old cluster's repos, commits, and pipelines into the new cluster. You must perform a migration to move between major releases , such as 1.8.7 to 1.9.0. Whether you're doing an upgrade or migration, it is recommended you backup Pachyderm prior. That will guarantee you can restore your cluster to its previous, good state. Note about 1.7 to 1.8 migrations \u00b6 In Pachyderm 1.8, we rearchitected core parts of the platform to improve speed and scalability . Migrating from 1.7.x to 1.8.x using the procedure below can a fairly lengthy process. If your requirements fit, it may be easier to create a new 1.8 or greater cluster and reload your latest source data into your input repositories. You may wish to keep your original 1.7 cluster around in a suspended state, reactivating it in case you need access to that provenance data. General migration procedure \u00b6 Before you start: backups \u00b6 Please refer to the documentation on backing up your cluster . Migration steps \u00b6 1. Pause all pipeline and data loading operations \u00b6 From the directed acyclic graphs (DAG) that define your pachyderm cluster, stop each pipeline step. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the stop pipeline command. pachctl stop pipeline <pipeline-name> You can confirm each pipeline is paused using the list pipeline command pachctl list pipeline Alternatively, a useful shell script for running stop pipeline on all pipelines is included below. It may be necessary to install the utilities used in the script, like jq and xargs , on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl stop pipeline {} It's also a useful practice, for simple to moderately complex deployments, to keep a terminal window up showing the state of all running kubernetes pods. watch -n 5 kubectl get pods You may need to install the watch and kubectl commands on your system, and configure kubectl to point at the cluster that Pachyderm is running in. Pausing data loading operations \u00b6 Input repositories or input repos in Pachyderm are repositories created with the pachctl create repo command. They're designed to be the repos at the top of a directed acyclic graph of pipelines. Pipelines have their own output repos associated with them, and are not considered input repos. If there are any processes external to pachyderm that put data into input repos using any method (the Pachyderm APIs, pachctl put file , etc.), they need to be paused. See Loading data from other sources into pachyderm below for design considerations for those processes that will minimize downtime during a restore or migration. Alternatively, you can use the following commands to stop all data loading into Pachyderm from outside processes. # Once you have stopped all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl stop pipeline {} # all pipelines in your cluster should be suspended. To stop all # data loading processes, we're going to modify the pachd Kubernetes service so that # it only accepts traffic on port 30649 (instead of the usual 30650). This way, # any background users and services that send requests to your Pachyderm cluster # while 'extract' is running will not interfere with the process # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pach -o json >pach_service_backup_30650.json # Modify the service to accept traffic on port 30649 # Note that you'll likely also need to modify your cloud provider's firewall # rules to allow traffic on this port $ kubectl get svc/pachd -o json | sed 's/30650/30649/g' | kc apply -f - # Modify your environment so that *you* can talk to pachd on this new port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30649 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pc version COMPONENT VERSION pachctl 1.7.11 pachd 1.7.11 2. Extract a pachyderm backup with the --no-objects flag \u00b6 This step and the following step, 3. Clone your object store bucket , can be run simultaneously. Using the pachctl extract command, create the backup you need. pachctl extract --no-objects > path/to/your/backup/file You can also use the -u or --url flag to put the backup directly into an object store. pachctl extract --no-objects --url s3://... Note that this s3 bucket is different than the s3 bucket will create to clone your object store. This is merely a bucket you allocated to hold the pachyderm backup without objects. 3. Clone your object store bucket \u00b6 This step and the prior step, 2. Extract a pachyderm backup with the --no-objects flag , can be run simultaneously. Run the command that will clone a bucket in your object store. Below, we give an example using the Amazon Web Services CLI to clone one bucket to another, taken from the documentation for that command . Similar commands are available for Google Cloud and Azure blob storage . aws s3 sync s3://mybucket s3://mybucket2 4. Restart all pipeline and data loading ops \u00b6 Once the backup and clone operations are complete, restart all paused pipelines and data loading operations, setting a checkpoint for the started operations that you can use in step 7. Load transactional data from checkpoint into new cluster , below. See Loading data from other sources into pachyderm to understand why designing this checkpoint into your data loading systems is important. From the directed acyclic graphs (DAG) that define your pachyderm cluster, start each pipeline. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the 'start pipeline' command. pachctl start pipeline <pipeline-name> You can confirm each pipeline is started using the list pipeline command pachctl list pipeline A useful shell script for running start pipeline on all pipelines is included below. It may be necessary to install several of the utlilies used in the script, like jq, on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl start pipeline {} If you used the port-changing technique, above , to stop all data loading into Pachyderm from outside processes, you should change the ports back. # Once you have restarted all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl start pipeline {} # all pipelines in your cluster should be restarted. To restart all data loading # processes, we're going to change the pachd Kubernetes service so that # it only accepts traffic on port 30650 again (from 30649). # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pach -o json >pach_service_backup_30649.json # Modify the service to accept traffic on port 30650, again $ kubectl get svc/pachd -o json | sed 's/30649/30650/g' | kc apply -f - # Modify your environment so that *you* can talk to pachd on the old port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30650 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pc version COMPONENT VERSION pachctl 1.7.11 pachd 1.7.11 Your old pachyderm cluster can operate while you're creating a migrated one. It's important that your data loading operations are designed to use the \" Loading data from other sources into pachyderm \" design criteria below for this to work. 5. Deploy a 1.X Pachyderm cluster with cloned bucket \u00b6 Create a pachyderm cluster using the bucket you cloned in 3. Clone your object store bucket . You'll want to bring up this new pachyderm cluster in a different namespace. You'll check at the steps below to see if there was some kind of problem with the extracted data and steps 2 and 3 need to be run again. Once your new cluster is up and you're connected to it, go on to the next step. Note that there may be modifications needed to Kubernetes ingress to Pachyderm deployment in the new namespace to avoid port conflicts in the same cluster. Please consult with your Kubernetes administrator for information on avoiding ingress conflicts, or check with us in your Pachyderm support channel if you need help. Important: Use the kubectl config current-config command to confirm you're talking to the correct kubernetes cluster configuration for the new cluster. 6. Restore the new 1.X Pachyderm cluster from your backup \u00b6 Using the Pachyderm cluster you deployed in the previous step, 5. Deploy a 1.X pachyderm cluster with cloned bucket , run pachctl restore with the backup you created in 2. Extract a pachyderm backup with the --no-objects flag . Important Use the_ kubectl config current-config command to confirm you're talking to the correct kubernetes cluster configuration . pachctl restore < path/to/your/backup/file You can also use the -u or --url flag to get the backup directly from the object store you placed it in pachctl restore --url s3://... Note that this s3 bucket is different than the s3 bucket you cloned, above. This is merely a bucket you allocated to hold the Pachyderm backup without objects. 7. Load transactional data from checkpoint into new cluster \u00b6 Configure an instance of your data loading systems to point at the new, upgraded pachyderm cluster and play back transactions from the checkpoint you established in 4. Restart all pipeline and data loading operations . Perform any reconfiguration to data loading or unloading operations. Confirm that the data output is as expected and the new cluster is operating as expected. 8. Disable the old cluster \u00b6 Once you've confirmed that the new cluster is operating, you can disable the old cluster. 9. Reconfigure new cluster as necessary \u00b6 You may also need to reconfigure data loading operations from Pachyderm to processes outside of it to work as expected Kubernetes ingress and port changes taken to avoid conflicts with the old cluster","title":"Migrate to a Major Version"},{"location":"deploy-manage/manage/migrations/#migration","text":"Info If you need to upgrade Pachyderm from one minor version to another, such as from 1.9.4 to 1.9.5, see Upgrade Pachyderm . Introduction [Note about 1.7 to 1.8 migrations](#note-about-1-7-to-1-8-migrations] General migration procedure Before you start: backups Migration steps 1. Pause all pipeline and data loading operations 2. Extract a pachyderm backup with the --no-objects flag 3. Clone your object store bucket 4. Restart all pipeline and data loading ops 5. Deploy a 1.X Pachyderm cluster with cloned bucket 6. Restore the new 1.X Pachyderm cluster from your backup 7. Load transactional data from checkpoint into new cluster 8. Disable the old cluster 9. Reconfigure new cluster as necessary","title":"Migration"},{"location":"deploy-manage/manage/migrations/#introduction","text":"As new versions of Pachyderm are released, you may need to update your cluster to get access to bug fixes and new features. These updates fall into two categories, upgrades and migrations. An upgrade is moving between point releases within the same major release, like 1.7.2 to 1.7.3. Upgrades are typically a simple process that require little to no downtime. Migrations involve moving between major releases, like 1.8.6 to 1.9.0. Migration is covered in this document. In general, Pachyderm stores all of its state in two places: etcd (which in turn stores its state in one or more persistent volumes, which were created when the Pachyderm cluster was deployed) and an object store bucket (something like AWS S3, MinIO, or Azure Blob Storage). In a migration, the data structures stored in those locations need to be read, transformed, and rewritten, so the process involves: bringing up a new Pachyderm cluster adjacent to the old pachyderm cluster exporting the old Pachdyerm cluster's repos, pipelines, and input commits importing the old cluster's repos, commits, and pipelines into the new cluster. You must perform a migration to move between major releases , such as 1.8.7 to 1.9.0. Whether you're doing an upgrade or migration, it is recommended you backup Pachyderm prior. That will guarantee you can restore your cluster to its previous, good state.","title":"Introduction"},{"location":"deploy-manage/manage/migrations/#note-about-17-to-18-migrations","text":"In Pachyderm 1.8, we rearchitected core parts of the platform to improve speed and scalability . Migrating from 1.7.x to 1.8.x using the procedure below can a fairly lengthy process. If your requirements fit, it may be easier to create a new 1.8 or greater cluster and reload your latest source data into your input repositories. You may wish to keep your original 1.7 cluster around in a suspended state, reactivating it in case you need access to that provenance data.","title":"Note about 1.7 to 1.8 migrations"},{"location":"deploy-manage/manage/migrations/#general-migration-procedure","text":"","title":"General migration procedure"},{"location":"deploy-manage/manage/migrations/#before-you-start-backups","text":"Please refer to the documentation on backing up your cluster .","title":"Before you start: backups"},{"location":"deploy-manage/manage/migrations/#migration-steps","text":"","title":"Migration steps"},{"location":"deploy-manage/manage/migrations/#1-pause-all-pipeline-and-data-loading-operations","text":"From the directed acyclic graphs (DAG) that define your pachyderm cluster, stop each pipeline step. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the stop pipeline command. pachctl stop pipeline <pipeline-name> You can confirm each pipeline is paused using the list pipeline command pachctl list pipeline Alternatively, a useful shell script for running stop pipeline on all pipelines is included below. It may be necessary to install the utilities used in the script, like jq and xargs , on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl stop pipeline {} It's also a useful practice, for simple to moderately complex deployments, to keep a terminal window up showing the state of all running kubernetes pods. watch -n 5 kubectl get pods You may need to install the watch and kubectl commands on your system, and configure kubectl to point at the cluster that Pachyderm is running in.","title":"1. Pause all pipeline and data loading operations"},{"location":"deploy-manage/manage/migrations/#pausing-data-loading-operations","text":"Input repositories or input repos in Pachyderm are repositories created with the pachctl create repo command. They're designed to be the repos at the top of a directed acyclic graph of pipelines. Pipelines have their own output repos associated with them, and are not considered input repos. If there are any processes external to pachyderm that put data into input repos using any method (the Pachyderm APIs, pachctl put file , etc.), they need to be paused. See Loading data from other sources into pachyderm below for design considerations for those processes that will minimize downtime during a restore or migration. Alternatively, you can use the following commands to stop all data loading into Pachyderm from outside processes. # Once you have stopped all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl stop pipeline {} # all pipelines in your cluster should be suspended. To stop all # data loading processes, we're going to modify the pachd Kubernetes service so that # it only accepts traffic on port 30649 (instead of the usual 30650). This way, # any background users and services that send requests to your Pachyderm cluster # while 'extract' is running will not interfere with the process # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pach -o json >pach_service_backup_30650.json # Modify the service to accept traffic on port 30649 # Note that you'll likely also need to modify your cloud provider's firewall # rules to allow traffic on this port $ kubectl get svc/pachd -o json | sed 's/30650/30649/g' | kc apply -f - # Modify your environment so that *you* can talk to pachd on this new port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30649 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pc version COMPONENT VERSION pachctl 1.7.11 pachd 1.7.11","title":"Pausing data loading operations"},{"location":"deploy-manage/manage/migrations/#2-extract-a-pachyderm-backup-with-the-no-objects-flag","text":"This step and the following step, 3. Clone your object store bucket , can be run simultaneously. Using the pachctl extract command, create the backup you need. pachctl extract --no-objects > path/to/your/backup/file You can also use the -u or --url flag to put the backup directly into an object store. pachctl extract --no-objects --url s3://... Note that this s3 bucket is different than the s3 bucket will create to clone your object store. This is merely a bucket you allocated to hold the pachyderm backup without objects.","title":"2. Extract a pachyderm backup with the --no-objects flag"},{"location":"deploy-manage/manage/migrations/#3-clone-your-object-store-bucket","text":"This step and the prior step, 2. Extract a pachyderm backup with the --no-objects flag , can be run simultaneously. Run the command that will clone a bucket in your object store. Below, we give an example using the Amazon Web Services CLI to clone one bucket to another, taken from the documentation for that command . Similar commands are available for Google Cloud and Azure blob storage . aws s3 sync s3://mybucket s3://mybucket2","title":"3. Clone your object store bucket"},{"location":"deploy-manage/manage/migrations/#4-restart-all-pipeline-and-data-loading-ops","text":"Once the backup and clone operations are complete, restart all paused pipelines and data loading operations, setting a checkpoint for the started operations that you can use in step 7. Load transactional data from checkpoint into new cluster , below. See Loading data from other sources into pachyderm to understand why designing this checkpoint into your data loading systems is important. From the directed acyclic graphs (DAG) that define your pachyderm cluster, start each pipeline. You can either run a multiline shell command, shown below, or you must, for each pipeline, manually run the 'start pipeline' command. pachctl start pipeline <pipeline-name> You can confirm each pipeline is started using the list pipeline command pachctl list pipeline A useful shell script for running start pipeline on all pipelines is included below. It may be necessary to install several of the utlilies used in the script, like jq, on your system. pachctl list pipeline --raw \\ | jq -r '.pipeline.name' \\ | xargs -P3 -n1 -I{} pachctl start pipeline {} If you used the port-changing technique, above , to stop all data loading into Pachyderm from outside processes, you should change the ports back. # Once you have restarted all running pachyderm pipelines, such as with this command, # $ pachctl list pipeline --raw \\ # | jq -r '.pipeline.name' \\ # | xargs -P3 -n1 -I{} pachctl start pipeline {} # all pipelines in your cluster should be restarted. To restart all data loading # processes, we're going to change the pachd Kubernetes service so that # it only accepts traffic on port 30650 again (from 30649). # # Backup the Pachyderm service spec, in case you need to restore it quickly $ kubectl get svc/pach -o json >pach_service_backup_30649.json # Modify the service to accept traffic on port 30650, again $ kubectl get svc/pachd -o json | sed 's/30649/30650/g' | kc apply -f - # Modify your environment so that *you* can talk to pachd on the old port $ pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30650 # Make sure you can talk to pachd (if not, firewall rules are a common culprit) $ pc version COMPONENT VERSION pachctl 1.7.11 pachd 1.7.11 Your old pachyderm cluster can operate while you're creating a migrated one. It's important that your data loading operations are designed to use the \" Loading data from other sources into pachyderm \" design criteria below for this to work.","title":"4. Restart all pipeline and data loading ops"},{"location":"deploy-manage/manage/migrations/#5-deploy-a-1x-pachyderm-cluster-with-cloned-bucket","text":"Create a pachyderm cluster using the bucket you cloned in 3. Clone your object store bucket . You'll want to bring up this new pachyderm cluster in a different namespace. You'll check at the steps below to see if there was some kind of problem with the extracted data and steps 2 and 3 need to be run again. Once your new cluster is up and you're connected to it, go on to the next step. Note that there may be modifications needed to Kubernetes ingress to Pachyderm deployment in the new namespace to avoid port conflicts in the same cluster. Please consult with your Kubernetes administrator for information on avoiding ingress conflicts, or check with us in your Pachyderm support channel if you need help. Important: Use the kubectl config current-config command to confirm you're talking to the correct kubernetes cluster configuration for the new cluster.","title":"5. Deploy a 1.X Pachyderm cluster with cloned bucket"},{"location":"deploy-manage/manage/migrations/#6-restore-the-new-1x-pachyderm-cluster-from-your-backup","text":"Using the Pachyderm cluster you deployed in the previous step, 5. Deploy a 1.X pachyderm cluster with cloned bucket , run pachctl restore with the backup you created in 2. Extract a pachyderm backup with the --no-objects flag . Important Use the_ kubectl config current-config command to confirm you're talking to the correct kubernetes cluster configuration . pachctl restore < path/to/your/backup/file You can also use the -u or --url flag to get the backup directly from the object store you placed it in pachctl restore --url s3://... Note that this s3 bucket is different than the s3 bucket you cloned, above. This is merely a bucket you allocated to hold the Pachyderm backup without objects.","title":"6. Restore the new 1.X Pachyderm cluster from your backup"},{"location":"deploy-manage/manage/migrations/#7-load-transactional-data-from-checkpoint-into-new-cluster","text":"Configure an instance of your data loading systems to point at the new, upgraded pachyderm cluster and play back transactions from the checkpoint you established in 4. Restart all pipeline and data loading operations . Perform any reconfiguration to data loading or unloading operations. Confirm that the data output is as expected and the new cluster is operating as expected.","title":"7. Load transactional data from checkpoint into new cluster"},{"location":"deploy-manage/manage/migrations/#8-disable-the-old-cluster","text":"Once you've confirmed that the new cluster is operating, you can disable the old cluster.","title":"8. Disable the old cluster"},{"location":"deploy-manage/manage/migrations/#9-reconfigure-new-cluster-as-necessary","text":"You may also need to reconfigure data loading operations from Pachyderm to processes outside of it to work as expected Kubernetes ingress and port changes taken to avoid conflicts with the old cluster","title":"9. Reconfigure new cluster as necessary"},{"location":"deploy-manage/manage/sharing_gpu_resources/","text":"Sharing GPU Resources \u00b6 Often times, teams are running big ML models on instances with GPU resources. GPU instances are expensive! You want to make sure that you're utilizing the GPUs you're paying for! Without configuration \u00b6 To deploy a pipeline that relies on GPU , you'll already have set the gpu resource requirement in the pipeline specification. But Pachyderm workers by default are long lived ... the worker is spun up and waits for new input. That works great for pipelines that are processing a lot of new incoming commits. For ML workflows, especially during the development cycle, you probably will see lower volume of input commits. Which means that you could have your pipeline workers 'taking' the GPU resource as far as k8s is concerned, but 'idling' as far as you're concerned. Let's use an example. Let's say your cluster has a single GPU node with 2 GPUs. Let's say you have a pipeline running that requires 1 GPU. You've trained some models, and found the results were surprising. You suspect your feature extraction code, and are delving into debugging that stage of your pipeline. Meanwhile, the worker you've spun up for your GPU training job is sitting idle, but telling k8s it's using the GPU instance. Now your coworker is actively trying to develop their GPU model with their pipeline. Their model requires 2 GPUs. But your pipeline is still marked as using 1 GPU, so their pipeline can't run! Configuring your pipelines to share GPUs \u00b6 Whenever you have a limited amount of a resource on your cluster (in this case GPU), you want to make sure you've specified how much of that resource you need via the resource_requests as part of your pipeline specification . But, you also need to make sure you set the standby field to true so that if your pipeline is not getting used, the worker pods get spun down and you free the GPU resource.","title":"Sharing GPU Resources"},{"location":"deploy-manage/manage/sharing_gpu_resources/#sharing-gpu-resources","text":"Often times, teams are running big ML models on instances with GPU resources. GPU instances are expensive! You want to make sure that you're utilizing the GPUs you're paying for!","title":"Sharing GPU Resources"},{"location":"deploy-manage/manage/sharing_gpu_resources/#without-configuration","text":"To deploy a pipeline that relies on GPU , you'll already have set the gpu resource requirement in the pipeline specification. But Pachyderm workers by default are long lived ... the worker is spun up and waits for new input. That works great for pipelines that are processing a lot of new incoming commits. For ML workflows, especially during the development cycle, you probably will see lower volume of input commits. Which means that you could have your pipeline workers 'taking' the GPU resource as far as k8s is concerned, but 'idling' as far as you're concerned. Let's use an example. Let's say your cluster has a single GPU node with 2 GPUs. Let's say you have a pipeline running that requires 1 GPU. You've trained some models, and found the results were surprising. You suspect your feature extraction code, and are delving into debugging that stage of your pipeline. Meanwhile, the worker you've spun up for your GPU training job is sitting idle, but telling k8s it's using the GPU instance. Now your coworker is actively trying to develop their GPU model with their pipeline. Their model requires 2 GPUs. But your pipeline is still marked as using 1 GPU, so their pipeline can't run!","title":"Without configuration"},{"location":"deploy-manage/manage/sharing_gpu_resources/#configuring-your-pipelines-to-share-gpus","text":"Whenever you have a limited amount of a resource on your cluster (in this case GPU), you want to make sure you've specified how much of that resource you need via the resource_requests as part of your pipeline specification . But, you also need to make sure you set the standby field to true so that if your pipeline is not getting used, the worker pods get spun down and you free the GPU resource.","title":"Configuring your pipelines to share GPUs"},{"location":"deploy-manage/manage/upgrades/","text":"Upgrade Pachyderm \u00b6 If you need to upgrade Pachyderm from one major version to another, such as from 1.8.x to 1.9.x , follow the instructions in the Migrate between major versions . Upgrades from one minor version to another, such as from version 1.9.0 to version 1.9.2 do not introduce breaking changes. Therefore, the upgrade procedure is simple and requires little to no downtime. Warning Do not use these steps to upgrade between major versions because it might result in data corruption. To upgrade Pachyderm to a minor version, complete the following steps: Back up your cluster as described in the Backup and Restore section. Destroy your Pachyderm cluster: pachctl undeploy Upgrade pachctl by using brew for macOS or apt for Linux: Example: $ brew upgrade pachyderm/tap/pachctl@1.9 == > Upgrading 1 outdated package: pachyderm/tap/pachctl@1.9 == > Upgrading pachyderm/tap/pachctl@1.9 ... Note: You need to specify the version of pachctl to which you want to upgrade. For example, if you want to upgrade 1.9.0 to 1.9.2 , add @1.9 at the end of the upgrade path. Confirm that the new version has been successfully installed by running the following command: $ pachctl version --client-only COMPONENT VERSION pachctl 1 .9.2 Redeploy Pachyderm by running the pachctl deploy command with the same arguments, fields, and storage resources that you specified when you deployed the previous version of Pachyderm: $ pachctl deploy <args> serviceaccount \"pachyderm\" created storageclass \"etcd-storage-class\" created service \"etcd-headless\" created statefulset \"etcd\" created service \"etcd\" created service \"pachd\" created deployment \"pachd\" created service \"dash\" created deployment \"dash\" created secret \"pachyderm-storage-secret\" created Pachyderm is launching. Check its status with \"kubectl get all\" Once launched, access the dashboard by running \"pachctl port-forward\" The deployment takes some time. You can run kubectl get pods periodically to check the status of the deployment. When Pachyderm is deployed, the command shows all pods as READY : $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-482120938-np8cc 2 /2 Running 0 4m etcd-0 1 /1 Running 0 4m pachd-3677268306-9sqm0 1 /1 Running 0 4m Verify that the new version has been deployed: pachctl version COMPONENT VERSION pachctl 1 .9.2 pachd 1 .9.2 The pachd and pachctl versions must both match the new version. Troubleshooting Minor Upgrades \u00b6 This section describes issues that you might run into when upgrading Pachyderm and provides guidelines on how to resolve them. StatefulSets vs static persistent volumes \u00b6 StatefulSets are a mechanism provided in Kubernetes 1.9 and newer to manage the deployment and scaling of applications. It can use Persistent Volume Provisioning or pre-provisioned PV\u2019s, both of which are dynamically allocated from Pachyderm's point of view. Thus, the --dynamic-etcd-nodes flag to pachctl deploy is used to deploy Pachyderm using StatefulSets. It is recommended that you deploy Pachyderm using StatefulSets when possible. All of the instructions for cloud provider deployments do this by default. We also provide instructions for on-premises deployments using StatefulSets . If you have deployed Pachyderm using StatefulSets, you can still use the same deploy command to re-deploy Pachyderm. Kubernetes is smart enough to see the previously utilized volumes and re-use them. etcd re-deploy problems \u00b6 Depending on the cloud you are deploying to and the previous deployment configuration, we have seen certain cases in which volumes don't get attached to the right nodes on re-deploy (especially when using AWS). In these scenarios, you may see the etcd pod stuck in a Pending , CrashLoopBackoff , or other failed state. Most often, deleting the corresponding etcd pod(s) or nodes to redeploy them or re-deploying all of Pachyderm again will fix the issue. AlreadyExists errors on re-deploy \u00b6 Occasionally, you might see errors similar to the following: Error from server (AlreadyExists): error when creating \"STDIN\": secrets \"pachyderm-storage-secret\" already exists This might happen when re-deploying the enterprise dashboard, for example. These warning are benign. pachctl connnection problems \u00b6 When you upgrade Pachyderm versions, you may lose your local port-forward to connect pachctl to your cluster. If you are not using port-forward and you are instead setting pachd address config value to connect pachctl to your cluster, the IP address for Pachyderm may have changed. To fix problems with connections to pachd after upgrading, you can perform the appropriate remedy for your situation: Re-run pachctl port-forward & , or Set the pachd address config value to the updated value, e.g.: pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30650","title":"Upgrade your Cluster"},{"location":"deploy-manage/manage/upgrades/#upgrade-pachyderm","text":"If you need to upgrade Pachyderm from one major version to another, such as from 1.8.x to 1.9.x , follow the instructions in the Migrate between major versions . Upgrades from one minor version to another, such as from version 1.9.0 to version 1.9.2 do not introduce breaking changes. Therefore, the upgrade procedure is simple and requires little to no downtime. Warning Do not use these steps to upgrade between major versions because it might result in data corruption. To upgrade Pachyderm to a minor version, complete the following steps: Back up your cluster as described in the Backup and Restore section. Destroy your Pachyderm cluster: pachctl undeploy Upgrade pachctl by using brew for macOS or apt for Linux: Example: $ brew upgrade pachyderm/tap/pachctl@1.9 == > Upgrading 1 outdated package: pachyderm/tap/pachctl@1.9 == > Upgrading pachyderm/tap/pachctl@1.9 ... Note: You need to specify the version of pachctl to which you want to upgrade. For example, if you want to upgrade 1.9.0 to 1.9.2 , add @1.9 at the end of the upgrade path. Confirm that the new version has been successfully installed by running the following command: $ pachctl version --client-only COMPONENT VERSION pachctl 1 .9.2 Redeploy Pachyderm by running the pachctl deploy command with the same arguments, fields, and storage resources that you specified when you deployed the previous version of Pachyderm: $ pachctl deploy <args> serviceaccount \"pachyderm\" created storageclass \"etcd-storage-class\" created service \"etcd-headless\" created statefulset \"etcd\" created service \"etcd\" created service \"pachd\" created deployment \"pachd\" created service \"dash\" created deployment \"dash\" created secret \"pachyderm-storage-secret\" created Pachyderm is launching. Check its status with \"kubectl get all\" Once launched, access the dashboard by running \"pachctl port-forward\" The deployment takes some time. You can run kubectl get pods periodically to check the status of the deployment. When Pachyderm is deployed, the command shows all pods as READY : $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-482120938-np8cc 2 /2 Running 0 4m etcd-0 1 /1 Running 0 4m pachd-3677268306-9sqm0 1 /1 Running 0 4m Verify that the new version has been deployed: pachctl version COMPONENT VERSION pachctl 1 .9.2 pachd 1 .9.2 The pachd and pachctl versions must both match the new version.","title":"Upgrade Pachyderm"},{"location":"deploy-manage/manage/upgrades/#troubleshooting-minor-upgrades","text":"This section describes issues that you might run into when upgrading Pachyderm and provides guidelines on how to resolve them.","title":"Troubleshooting Minor Upgrades"},{"location":"deploy-manage/manage/upgrades/#statefulsets-vs-static-persistent-volumes","text":"StatefulSets are a mechanism provided in Kubernetes 1.9 and newer to manage the deployment and scaling of applications. It can use Persistent Volume Provisioning or pre-provisioned PV\u2019s, both of which are dynamically allocated from Pachyderm's point of view. Thus, the --dynamic-etcd-nodes flag to pachctl deploy is used to deploy Pachyderm using StatefulSets. It is recommended that you deploy Pachyderm using StatefulSets when possible. All of the instructions for cloud provider deployments do this by default. We also provide instructions for on-premises deployments using StatefulSets . If you have deployed Pachyderm using StatefulSets, you can still use the same deploy command to re-deploy Pachyderm. Kubernetes is smart enough to see the previously utilized volumes and re-use them.","title":"StatefulSets vs static persistent volumes"},{"location":"deploy-manage/manage/upgrades/#etcd-re-deploy-problems","text":"Depending on the cloud you are deploying to and the previous deployment configuration, we have seen certain cases in which volumes don't get attached to the right nodes on re-deploy (especially when using AWS). In these scenarios, you may see the etcd pod stuck in a Pending , CrashLoopBackoff , or other failed state. Most often, deleting the corresponding etcd pod(s) or nodes to redeploy them or re-deploying all of Pachyderm again will fix the issue.","title":"etcd re-deploy problems"},{"location":"deploy-manage/manage/upgrades/#alreadyexists-errors-on-re-deploy","text":"Occasionally, you might see errors similar to the following: Error from server (AlreadyExists): error when creating \"STDIN\": secrets \"pachyderm-storage-secret\" already exists This might happen when re-deploying the enterprise dashboard, for example. These warning are benign.","title":"AlreadyExists errors on re-deploy"},{"location":"deploy-manage/manage/upgrades/#pachctl-connnection-problems","text":"When you upgrade Pachyderm versions, you may lose your local port-forward to connect pachctl to your cluster. If you are not using port-forward and you are instead setting pachd address config value to connect pachctl to your cluster, the IP address for Pachyderm may have changed. To fix problems with connections to pachd after upgrading, you can perform the appropriate remedy for your situation: Re-run pachctl port-forward & , or Set the pachd address config value to the updated value, e.g.: pachctl config update context `pachctl config get active-context` --pachd-address=<cluster ip>:30650","title":"pachctl connnection problems"},{"location":"deploy-manage/manage/upgrades_migrations/","text":"Upgrades and Migrations \u00b6 As new versions of Pachyderm are released, you might need to update your cluster to get access to bug fixes and new features. These updates fall into the following categories: Upgrades \u2014 An upgrade is moving between point releases within the same major release. For example, between version 1.9.5 and 1.9.7. Upgrades are typically a simple process that require little to no downtime. Migrations \u2014 A migration that you must perform to move between major releases, such as between version 1.8.7 and 1.9.7. Important Performing an upgrade between major releases might lead to corrupted data. You must perform a migration when going between major releases! Whether you upgrade or migrate your cluster, Pachyderm recommendeds that you back up Pachyderm . A backup guarantees that you can restore your cluster to its previous, stable state.","title":"Overview"},{"location":"deploy-manage/manage/upgrades_migrations/#upgrades-and-migrations","text":"As new versions of Pachyderm are released, you might need to update your cluster to get access to bug fixes and new features. These updates fall into the following categories: Upgrades \u2014 An upgrade is moving between point releases within the same major release. For example, between version 1.9.5 and 1.9.7. Upgrades are typically a simple process that require little to no downtime. Migrations \u2014 A migration that you must perform to move between major releases, such as between version 1.8.7 and 1.9.7. Important Performing an upgrade between major releases might lead to corrupted data. You must perform a migration when going between major releases! Whether you upgrade or migrate your cluster, Pachyderm recommendeds that you back up Pachyderm . A backup guarantees that you can restore your cluster to its previous, stable state.","title":"Upgrades and Migrations"},{"location":"enterprise/deployment/","text":"Deploy Enterprise Edition \u00b6 To deploy and use Pachyderm's Enterprise Edition, follow the deployment instructions for your platform and then activate the Enterprise Edition . Pachyderm provides a FREE evaluation token for the Enterprise Edition on the landing page of the Enterprise dashboard. Note Pachyderm automatically deploys the Enterprise dashboard. If you want to deploy without the dashboard, run pachctl deploy [command] --no-dashboard . Activate Pachyderm Enterprise Edition \u00b6 There are two ways to activate Pachyderm's enterprise features:: Activate Pachyderm Enterprise by Using the pachctl CLI Activate Pachyderm Enterprise by Using the Dashboard For either method, you need to have your Pachyderm Enterprise activation code available. You should have received this from the Pachyderm sales team when registering for the Enterprise Edition. If you are a new user evaluating Pachyderm, you can request a FREE evaluation code on the landing page of the dashboard. If you are having trouble locating your activation code, contact support@pachyderm.io . Activate by Using the pachctl CLI \u00b6 When you have Pachyderm up and running, the kubectl get pods must return a similar output: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-vb972 2/2 Running 0 6m etcd-7dbb489f44-9v5jj 1/1 Running 0 6m pachd-6c878bbc4c-f2h2c 1/1 Running 0 6m You should also be able to connect to the Pachyderm cluster via the pachctl CLI: $ pachctl version COMPONENT VERSION pachctl 1.9.5 pachd 1.9.5 To activate the Pachyderm Enterprise Edition, complete the following steps:: Activate the Enterprise Edition by running: $ pachctl enterprise activate <activation-code> If this command does not return any error, then the activation was successful. Verify the status of the enterprise activation: $ pachctl enterprise get-state ACTIVE Activate by Using the Dashboard \u00b6 You can activate Enterprise Edition directly in the dashboard. To active Enterprise Edition in the Dashboard, complete the following steps: Connect to the dashboard by using one of the following methods: If you can connect directly, point your browser to port 30080 on your Kubernetes cluster's IP address. Enable port forwarding by running pachctl port-forward in a separate terminal window and then, point your browser to localhost:30080 . When you first access the dashboard, you are prompted to enter your activation code. Enter the promo code: After you enter your activation code, you have full access to the Enterprise dashboard, and your cluster has an active Enterprise Edition license. Confirm that your cluster has an active Enterprise Edition license: $ pachctl enterprise get-state ACTIVE","title":"Deploy Enterprise Edition"},{"location":"enterprise/deployment/#deploy-enterprise-edition","text":"To deploy and use Pachyderm's Enterprise Edition, follow the deployment instructions for your platform and then activate the Enterprise Edition . Pachyderm provides a FREE evaluation token for the Enterprise Edition on the landing page of the Enterprise dashboard. Note Pachyderm automatically deploys the Enterprise dashboard. If you want to deploy without the dashboard, run pachctl deploy [command] --no-dashboard .","title":"Deploy Enterprise Edition"},{"location":"enterprise/deployment/#activate-pachyderm-enterprise-edition","text":"There are two ways to activate Pachyderm's enterprise features:: Activate Pachyderm Enterprise by Using the pachctl CLI Activate Pachyderm Enterprise by Using the Dashboard For either method, you need to have your Pachyderm Enterprise activation code available. You should have received this from the Pachyderm sales team when registering for the Enterprise Edition. If you are a new user evaluating Pachyderm, you can request a FREE evaluation code on the landing page of the dashboard. If you are having trouble locating your activation code, contact support@pachyderm.io .","title":"Activate Pachyderm Enterprise Edition"},{"location":"enterprise/deployment/#activate-by-using-the-pachctl-cli","text":"When you have Pachyderm up and running, the kubectl get pods must return a similar output: $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-vb972 2/2 Running 0 6m etcd-7dbb489f44-9v5jj 1/1 Running 0 6m pachd-6c878bbc4c-f2h2c 1/1 Running 0 6m You should also be able to connect to the Pachyderm cluster via the pachctl CLI: $ pachctl version COMPONENT VERSION pachctl 1.9.5 pachd 1.9.5 To activate the Pachyderm Enterprise Edition, complete the following steps:: Activate the Enterprise Edition by running: $ pachctl enterprise activate <activation-code> If this command does not return any error, then the activation was successful. Verify the status of the enterprise activation: $ pachctl enterprise get-state ACTIVE","title":"Activate by Using the pachctl CLI"},{"location":"enterprise/deployment/#activate-by-using-the-dashboard","text":"You can activate Enterprise Edition directly in the dashboard. To active Enterprise Edition in the Dashboard, complete the following steps: Connect to the dashboard by using one of the following methods: If you can connect directly, point your browser to port 30080 on your Kubernetes cluster's IP address. Enable port forwarding by running pachctl port-forward in a separate terminal window and then, point your browser to localhost:30080 . When you first access the dashboard, you are prompted to enter your activation code. Enter the promo code: After you enter your activation code, you have full access to the Enterprise dashboard, and your cluster has an active Enterprise Edition license. Confirm that your cluster has an active Enterprise Edition license: $ pachctl enterprise get-state ACTIVE","title":"Activate by Using the Dashboard"},{"location":"enterprise/overview/","text":"Overview \u00b6 Pachyderm Enterprise Edition includes everything you need to scale and manage Pachyderm data pipelines in an enterprise setting. It delivers the most recent version of Pachyderm along with: Administrative and security features needed for enterprise-scale implementations of Pachyderm Visual and interactive interfaces to Pachyderm Detailed job and data statistics for faster development and data insight Pachyderm Enterprise Edition can be deployed easily on top of an existing or new deployment of Pachyderm, and we have engineers available to help enterprise customers get up and running very quickly. To get more information about Pachyderm Enterprise Edition, to ask questions, or to get access for evaluation, please contact us at sales@pachyderm.io or on our Slack . Pipeline Visualization and Data Exploration \u00b6 Pachyderm Enterprise Edition includes a full UI for visualizing pipelines and exploring data. Pachyderm Enterprise will automatically infer the structure of data scientists' DAG pipelines and display them visually. Data scientists and cluster admins can even click on individual segments of the pipelines to see what data is being processed, how many jobs have run, what images and commands are being run, and much more! Data scientists can also explore the versioned data in Pachyderm data repositories and see how the state of data has changed over time. Access Controls \u00b6 Enterprise-scale deployments require access controls and multitenancy. Pachyderm Enterprise Edition gives teams the ability to control access to production pipelines, data, and configuration. Administrators can silo data, prevent unintended modifications to production pipelines, and support multiple data scientists or even multiple data science groups. Advanced Statistics \u00b6 Pachyderm Enterprise Edition gives data scientists advanced insights into their data, jobs, and results. For example, data scientists can see how much time jobs spend downloading/uploading data, what data was processed or skipped, and which workers were given particular datums. This information can be explored programmatically or via a number of charts and plots that help users parse the information quickly. Administrative Controls, Interactive Pipeline Configuration \u00b6 With Pachyderm Enterprise, cluster admins don't have to rely solely on command line tools and language libraries to configure and control Pachyderm. With new versions of our UI you can control, scale, and configure Pachyderm interactively. S3Gateway \u00b6 Pachyderm Enterprise Edition includes the s3gateway, an S3-like API for interacting with PFS content. With it, you can interact with PFS content with tools and libraries built to work with S3.","title":"Overview"},{"location":"enterprise/overview/#overview","text":"Pachyderm Enterprise Edition includes everything you need to scale and manage Pachyderm data pipelines in an enterprise setting. It delivers the most recent version of Pachyderm along with: Administrative and security features needed for enterprise-scale implementations of Pachyderm Visual and interactive interfaces to Pachyderm Detailed job and data statistics for faster development and data insight Pachyderm Enterprise Edition can be deployed easily on top of an existing or new deployment of Pachyderm, and we have engineers available to help enterprise customers get up and running very quickly. To get more information about Pachyderm Enterprise Edition, to ask questions, or to get access for evaluation, please contact us at sales@pachyderm.io or on our Slack .","title":"Overview"},{"location":"enterprise/overview/#pipeline-visualization-and-data-exploration","text":"Pachyderm Enterprise Edition includes a full UI for visualizing pipelines and exploring data. Pachyderm Enterprise will automatically infer the structure of data scientists' DAG pipelines and display them visually. Data scientists and cluster admins can even click on individual segments of the pipelines to see what data is being processed, how many jobs have run, what images and commands are being run, and much more! Data scientists can also explore the versioned data in Pachyderm data repositories and see how the state of data has changed over time.","title":"Pipeline Visualization and Data Exploration"},{"location":"enterprise/overview/#access-controls","text":"Enterprise-scale deployments require access controls and multitenancy. Pachyderm Enterprise Edition gives teams the ability to control access to production pipelines, data, and configuration. Administrators can silo data, prevent unintended modifications to production pipelines, and support multiple data scientists or even multiple data science groups.","title":"Access Controls"},{"location":"enterprise/overview/#advanced-statistics","text":"Pachyderm Enterprise Edition gives data scientists advanced insights into their data, jobs, and results. For example, data scientists can see how much time jobs spend downloading/uploading data, what data was processed or skipped, and which workers were given particular datums. This information can be explored programmatically or via a number of charts and plots that help users parse the information quickly.","title":"Advanced Statistics"},{"location":"enterprise/overview/#administrative-controls-interactive-pipeline-configuration","text":"With Pachyderm Enterprise, cluster admins don't have to rely solely on command line tools and language libraries to configure and control Pachyderm. With new versions of our UI you can control, scale, and configure Pachyderm interactively.","title":"Administrative Controls, Interactive Pipeline Configuration"},{"location":"enterprise/overview/#s3gateway","text":"Pachyderm Enterprise Edition includes the s3gateway, an S3-like API for interacting with PFS content. With it, you can interact with PFS content with tools and libraries built to work with S3.","title":"S3Gateway"},{"location":"enterprise/s3gateway/","text":"S3Gateway \u00b6 Pachyderm Enterprise includes s3gateway, which allows you to interact with PFS storage through an HTTP API that imitates a subset of AWS S3's API. With this, you can reuse a number of tools and libraries built to work with object stores such as minio or Boto to interact with Pachyderm. You can only interact with the HEAD commit of non-authorization-gated PFS branches through the gateway. If you need richer access, you'll need to work with PFS through its gRPC interface instead. Connecting to the s3gateway \u00b6 The s3gateway runs in your cluster. You can access it by pointing your browser to http://<cluster ip>:30600 . Alternatively, you can use port forwarding to connect to the cluster. However, we do not recommend it, as Kubernetes' port forwarder incurs overhead, and does not recover well from broken connections. Supported operations \u00b6 These operations are supported by the gateway: Creating buckets: creates a repo and/or branch. Deleting buckets: Deletings a branch and/or repo. Listing buckets: Lists all branches on all repos as s3 buckets. Writing objects: Atomically overwrites a file on the HEAD of a branch. Removing objects: Atomically removes a file on the HEAD of a branch. Listing objects: Lists the files in the HEAD of a branch. Getting objects: Gets file contents on the HEAD of a branch. For details on what's going on under the hood and current peculiarities, see the s3gateway API . Unsupported operations \u00b6 These S3 operations are not supported by the gateway: Accelerate Analytics Object copying. PFS supports this through gRPC. You can use the gRPC API directly. CORS configuration Encryption HTML form uploads Inventory Legal holds Lifecycles Logging Metrics Multipart uploads. See writing object documentation above for a workaround. Notifications Object locks Payment requests Policies Public access blocks Regions Replication Retention policies Tagging Torrents Website configuration Attempting any of these operations returns a standard NotImplemented error. Additionally, there are a few general differences: There is no support for authentication or ACLs. As per PFS rules, you cannot write to an output repo. If you try to write to an output repository, Pachyderm returns a 500 error. Minio \u00b6 If you have the option of what S3 client library to use for interfacing with the s3gateway, Pachyderm recommends minio , as integration with its Golang client SDK is thoroughly tested. Pachyderm supports the following Minio operations: Bucket operations MakeBucket ListBuckets BucketExists RemoveBucket ListObjects Object operations GetObject PutObject StatObject RemoveObject FPutObject FGetObject","title":"Using the S3 Gateway"},{"location":"enterprise/s3gateway/#s3gateway","text":"Pachyderm Enterprise includes s3gateway, which allows you to interact with PFS storage through an HTTP API that imitates a subset of AWS S3's API. With this, you can reuse a number of tools and libraries built to work with object stores such as minio or Boto to interact with Pachyderm. You can only interact with the HEAD commit of non-authorization-gated PFS branches through the gateway. If you need richer access, you'll need to work with PFS through its gRPC interface instead.","title":"S3Gateway"},{"location":"enterprise/s3gateway/#connecting-to-the-s3gateway","text":"The s3gateway runs in your cluster. You can access it by pointing your browser to http://<cluster ip>:30600 . Alternatively, you can use port forwarding to connect to the cluster. However, we do not recommend it, as Kubernetes' port forwarder incurs overhead, and does not recover well from broken connections.","title":"Connecting to the s3gateway"},{"location":"enterprise/s3gateway/#supported-operations","text":"These operations are supported by the gateway: Creating buckets: creates a repo and/or branch. Deleting buckets: Deletings a branch and/or repo. Listing buckets: Lists all branches on all repos as s3 buckets. Writing objects: Atomically overwrites a file on the HEAD of a branch. Removing objects: Atomically removes a file on the HEAD of a branch. Listing objects: Lists the files in the HEAD of a branch. Getting objects: Gets file contents on the HEAD of a branch. For details on what's going on under the hood and current peculiarities, see the s3gateway API .","title":"Supported operations"},{"location":"enterprise/s3gateway/#unsupported-operations","text":"These S3 operations are not supported by the gateway: Accelerate Analytics Object copying. PFS supports this through gRPC. You can use the gRPC API directly. CORS configuration Encryption HTML form uploads Inventory Legal holds Lifecycles Logging Metrics Multipart uploads. See writing object documentation above for a workaround. Notifications Object locks Payment requests Policies Public access blocks Regions Replication Retention policies Tagging Torrents Website configuration Attempting any of these operations returns a standard NotImplemented error. Additionally, there are a few general differences: There is no support for authentication or ACLs. As per PFS rules, you cannot write to an output repo. If you try to write to an output repository, Pachyderm returns a 500 error.","title":"Unsupported operations"},{"location":"enterprise/s3gateway/#minio","text":"If you have the option of what S3 client library to use for interfacing with the s3gateway, Pachyderm recommends minio , as integration with its Golang client SDK is thoroughly tested. Pachyderm supports the following Minio operations: Bucket operations MakeBucket ListBuckets BucketExists RemoveBucket ListObjects Object operations GetObject PutObject StatObject RemoveObject FPutObject FGetObject","title":"Minio"},{"location":"enterprise/saml/","text":"Overview \u00b6 This guide walks you through an example of using Pachyderm's SAML support, including the following: Activate Pachyderm enterprise and Pachyderm auth. Configure Pachyderm's auth system to enable its SAML ACS, receive SAML assertions, and allow you to log in by using the Okta\u00ae access management software. Log in to both the dash and CLI. Activation \u00b6 When starting out, we highly recommend running Pachyderm in Minikube, as mistakes in this configuration could lock you out of your cluster. To activate Pachyderm enterprise and Pachyderm auth: pachctl enterprise activate <enterprise code> pachctl auth activate --initial-admin=robot:admin At this point, Pachyderm is ready to authenticate & authorize users. What the --initial-admin flag does is this: 1. Pachyderm requires there to be at least one cluster admin if auth is activated 2. Pachyderm's authentication is built around GitHub by default. Without this flag, Pachyderm asks the caller to go through an OAuth flow with GitHub, and then at the conclusion, makes the caller the cluster admin. Then whoever activated Pachyderm's auth system can modify it by re-authenticating via GitHub and performing any necessary actions 3. To avoid the OAuth flow, though, it's also possible to make the initial cluster admin a \"robot user\". This is what --initial-admin=robot:<something> does. 4. Pachyderm will print out a Pachyderm token that authenticates the holder as this robot user. At any point, you can authenticate as this robot user by running $ pachctl auth use-auth-token Please paste your Pachyderm auth token: <paste robot token emitted by \"pachctl auth activate --initial-admin=robot:admin\"> $ # you are now robot:admin, cluster administrator The rest of this example assumes that your Pachyderm cluster is running in minikube, and you're accessing it via pachctl 's port forwarding. Many of the SAML service provider URLs below are set to some variation of localhost , which will only work if you're using port forwarding and your browser is able to access Pachyderm via localhost on the port forwarder's usual ports. Create IdP test app \u00b6 The ID provider (IdP) that this example uses is Okta. Here is an example configuration for an Okta test app that authenticates Okta users with Pachyderm: Once created, you can get the IdP Metadata URL associated with the test Okta app here: Write Pachyderm config \u00b6 Broadly, setting an auth config is what enables SAML in Pachyderm (specifically, it enables Pachyderm's ACS). Below is an example config that will allow users to authenticate in your Pachyderm cluster using the Okta app above. Note that this example assumes # Lookup current config version--pachyderm config has a barrier to prevent # read-modify-write conflicts between admins live_config_version=\"$(pachctl auth get-config | jq .live_config_version)\" live_config_version=\"${live_config_version:-0}\" # Set the Pachyderm config pachctl auth set-config <<EOF { # prevent read-modify-write conflicts by explicitly specifying live version \"live_config_version\": ${live_config_version}, \"id_providers\": [ { \"name\": \"okta\", \"description\": \"Okta test app\", \"saml\": { \"metadata_url\": <okta app metadata URL>, \"group_attribute\": \"memberOf\" # optional: enable group support } } ], \"saml_svc_options\": { # These URLs work if using pachctl port-forward \"acs_url\": \"http://localhost:30654/saml/acs\", \"metadata_url\": \"http://localhost:30654/saml/metadata\", \"dash_url\": \"http://localhost:30080/auth/autologin\", } } EOF Logging In \u00b6 Currently Pachyderm only supports IdP-initiated authentication. To proceed, configure your Okta app to point to the Pachyderm ACS ( http://localhost:30654/saml/acs if using pachctl 's port forwarding), then sign in via the new Okta app in your Okta dashboard. After clicking on the test Okta app, your browser will do a SAML authentication handshake with your pachyderm cluster, and you will arrive at your Pachyderm dashboard fully authenticated. To log in with the Pachyderm CLI, get a One-Time Password from the Pachyderm dash, and then run pachctl auth login --code=<one-time password> in your terminal. Groups \u00b6 If your SAML ID provider supports setting group attributes, you can use groups to manage access in Pachyderm with the \"group_attribute\" in the IDProvider field of the auth config: pachctl auth set-config <<EOF { ... \"id_providers\": [ { ... \"saml\": { \"group_attribute\": \"memberOf\" } } ], } EOF Then, try: pachctl create repo group-test pachctl put file group-test@master -f some-data.txt pachctl auth set group/saml:\"Test Group\" reader group-test Elsewhere: pachctl auth login --code=<auth code> pachctl get file group-test@master:some-data.txt # should work for members of \"Test Group\"","title":"Overview"},{"location":"enterprise/saml/#overview","text":"This guide walks you through an example of using Pachyderm's SAML support, including the following: Activate Pachyderm enterprise and Pachyderm auth. Configure Pachyderm's auth system to enable its SAML ACS, receive SAML assertions, and allow you to log in by using the Okta\u00ae access management software. Log in to both the dash and CLI.","title":"Overview"},{"location":"enterprise/saml/#activation","text":"When starting out, we highly recommend running Pachyderm in Minikube, as mistakes in this configuration could lock you out of your cluster. To activate Pachyderm enterprise and Pachyderm auth: pachctl enterprise activate <enterprise code> pachctl auth activate --initial-admin=robot:admin At this point, Pachyderm is ready to authenticate & authorize users. What the --initial-admin flag does is this: 1. Pachyderm requires there to be at least one cluster admin if auth is activated 2. Pachyderm's authentication is built around GitHub by default. Without this flag, Pachyderm asks the caller to go through an OAuth flow with GitHub, and then at the conclusion, makes the caller the cluster admin. Then whoever activated Pachyderm's auth system can modify it by re-authenticating via GitHub and performing any necessary actions 3. To avoid the OAuth flow, though, it's also possible to make the initial cluster admin a \"robot user\". This is what --initial-admin=robot:<something> does. 4. Pachyderm will print out a Pachyderm token that authenticates the holder as this robot user. At any point, you can authenticate as this robot user by running $ pachctl auth use-auth-token Please paste your Pachyderm auth token: <paste robot token emitted by \"pachctl auth activate --initial-admin=robot:admin\"> $ # you are now robot:admin, cluster administrator The rest of this example assumes that your Pachyderm cluster is running in minikube, and you're accessing it via pachctl 's port forwarding. Many of the SAML service provider URLs below are set to some variation of localhost , which will only work if you're using port forwarding and your browser is able to access Pachyderm via localhost on the port forwarder's usual ports.","title":"Activation"},{"location":"enterprise/saml/#create-idp-test-app","text":"The ID provider (IdP) that this example uses is Okta. Here is an example configuration for an Okta test app that authenticates Okta users with Pachyderm: Once created, you can get the IdP Metadata URL associated with the test Okta app here:","title":"Create IdP test app"},{"location":"enterprise/saml/#write-pachyderm-config","text":"Broadly, setting an auth config is what enables SAML in Pachyderm (specifically, it enables Pachyderm's ACS). Below is an example config that will allow users to authenticate in your Pachyderm cluster using the Okta app above. Note that this example assumes # Lookup current config version--pachyderm config has a barrier to prevent # read-modify-write conflicts between admins live_config_version=\"$(pachctl auth get-config | jq .live_config_version)\" live_config_version=\"${live_config_version:-0}\" # Set the Pachyderm config pachctl auth set-config <<EOF { # prevent read-modify-write conflicts by explicitly specifying live version \"live_config_version\": ${live_config_version}, \"id_providers\": [ { \"name\": \"okta\", \"description\": \"Okta test app\", \"saml\": { \"metadata_url\": <okta app metadata URL>, \"group_attribute\": \"memberOf\" # optional: enable group support } } ], \"saml_svc_options\": { # These URLs work if using pachctl port-forward \"acs_url\": \"http://localhost:30654/saml/acs\", \"metadata_url\": \"http://localhost:30654/saml/metadata\", \"dash_url\": \"http://localhost:30080/auth/autologin\", } } EOF","title":"Write Pachyderm config"},{"location":"enterprise/saml/#logging-in","text":"Currently Pachyderm only supports IdP-initiated authentication. To proceed, configure your Okta app to point to the Pachyderm ACS ( http://localhost:30654/saml/acs if using pachctl 's port forwarding), then sign in via the new Okta app in your Okta dashboard. After clicking on the test Okta app, your browser will do a SAML authentication handshake with your pachyderm cluster, and you will arrive at your Pachyderm dashboard fully authenticated. To log in with the Pachyderm CLI, get a One-Time Password from the Pachyderm dash, and then run pachctl auth login --code=<one-time password> in your terminal.","title":"Logging In"},{"location":"enterprise/saml/#groups","text":"If your SAML ID provider supports setting group attributes, you can use groups to manage access in Pachyderm with the \"group_attribute\" in the IDProvider field of the auth config: pachctl auth set-config <<EOF { ... \"id_providers\": [ { ... \"saml\": { \"group_attribute\": \"memberOf\" } } ], } EOF Then, try: pachctl create repo group-test pachctl put file group-test@master -f some-data.txt pachctl auth set group/saml:\"Test Group\" reader group-test Elsewhere: pachctl auth login --code=<auth code> pachctl get file group-test@master:some-data.txt # should work for members of \"Test Group\"","title":"Groups"},{"location":"enterprise/saml_setup/","text":"Configure SAML \u00b6 This guide will walk through testing Pachyderm's experimental SAML support. These features aren't integrated into mainline Pachyderm yet and aren't available in any official releases. This will describe the process of: Activating Pachyderm enterprise and Pachyderm auth Configuring Pachyderm's auth system and enabling its SAML ACS (Assertion Consumer Service\u2014the HTTP endpoint to which users will forward SAML assertions). Logging in to both the dash and CLI Enabling debug logging in case anything goes wrong Activation \u00b6 For testing, we highly recommend running Pachyderm in Minikube, in case any early bugs make it necessary to restart the cluster. To activate Pachyderm enterprise and Pachyderm auth: pachctl enterprise activate <enterprise code> pachctl auth activate --initial-admin=robot:admin These commands cause Pachyderm's auth system to start verifying attempts to read and write Pachyderm data and blocking unauthorized users. Whichever user ran this command automatically authenticates as robot:admin and has admin privileges in the cluster (run pachctl auth whoami , as shown below, to confirm) Users will either need to set the --initial-admin admin flag or have one GitHub-based user in the system. The reason: 1. Pachyderm requires there to be at least one cluster admin if auth is activated 1. Pachyderm uses GitHub for authentication by default. Without this flag, Pachyderm asks the caller to go through an OAuth flow with GitHub, and then at the conclusion, makes the caller the cluster admin. Then whoever activated Pachyderm's auth system can assume admin status by re-authenticating via GitHub and performing any necessary actions 1. To avoid the OAuth flow, though, it's also possible to make the initial cluster admin a \"robot user\". Setting --initial-admin=robot:<something> does this. 1. Pachyderm will print out a Pachyderm token that authenticates the holder as this robot user. At any point, you can authenticate as this robot user by running $ pachctl auth use-auth-token Please paste your Pachyderm auth token: <paste robot token emitted by \"pachctl auth activate --initial-admin=robot:admin\"> $ pachctl auth whoami You are \"robot:admin\" You are an administrator of this Pachyderm cluster Create IdP test app \u00b6 This image shows an example configuration for an Okta test app that authenticates Okta users with Pachyderm: Pachyderm also needs a URL where it can scrape SAML metadata from the ID provider. All SAML ID providers should provide such a URL; the Okta metadata URL, for example, can be retrieved here: Write Pachyderm config \u00b6 This enables the Pachyderm ACS. See inline comments: # Lookup current config version--pachyderm config has a barrier to prevent # read-modify-write conflicts between admins live_config_version = \" $( pachctl auth get-config | jq .live_config_version ) \" live_config_version = \" ${ live_config_version :- 0 } \" Set the Pachyderm config: pachctl auth set-config <<EOF { \"live_config_version\": ${live_config_version}, \"id_providers\": [ { \"name\": \"saml\", \"description\": \"Okta test app metadata\", \"saml\": { \"metadata_url\": <okta app metadata URL>, \"group_attribute\": \"memberOf\" } } ], \"saml_svc_options\": { \"acs_url\": \"http://localhost:30654/saml/acs\", \"metadata_url\": \"http://localhost:30654/saml/metadata\", \"dash_url\": \"http://localhost:30080/auth/autologin?lol=wut\", \"session_duration\": \"8h\", } } EOF Logging In \u00b6 Currently Pachyderm only supports IdP-initiated authentication. Configure an Okta app to point to the Pachyderm ACS ( http://localhost:30654/saml/acs if using pachctl 's port forwarding, then sign in via the new Okta app This should allow you to log in at the Pachyderm dash. To log in with the Pachyderm CLI, get a One-Time Password from the Pachyderm dash, and then run pachctl auth login --code=<one-time password> in your terminal. Other features \u00b6 Debug Logging \u00b6 If we run into issues while deploying this, it may be useful to enable a collection of debug logs that we added during development. To do so, add the option \"debug_logging\": true to \"saml_svc_options\" : pachctl auth set-config <<EOF { ... \"saml_svc_options\": { ... \"debug_logging\": true } } EOF Groups \u00b6 Pachyderm has very preliminary, experimental support for groups. While they won't appear in ACLs in the dash (and may have other issues), you can experiment using the CLI by setting \"group_attribute\" in the IDProvider field of the auth config: pachctl auth set-config <<EOF { ... \"id_providers\": [ { ... \"saml\": { \"group_attribute\": \"memberOf\" } } ], } EOF Then, try: pachctl create repo group-test pachctl put file group-test@master -f some-data.txt pachctl auth set group/saml: \"Test Group\" reader group-test Elsewhere: pachctl auth login --code = <auth code> pachctl get file group-test@master:some-data.txt # should work for members of \"Test Group\"","title":"Configure SAML"},{"location":"enterprise/saml_setup/#configure-saml","text":"This guide will walk through testing Pachyderm's experimental SAML support. These features aren't integrated into mainline Pachyderm yet and aren't available in any official releases. This will describe the process of: Activating Pachyderm enterprise and Pachyderm auth Configuring Pachyderm's auth system and enabling its SAML ACS (Assertion Consumer Service\u2014the HTTP endpoint to which users will forward SAML assertions). Logging in to both the dash and CLI Enabling debug logging in case anything goes wrong","title":"Configure SAML"},{"location":"enterprise/saml_setup/#activation","text":"For testing, we highly recommend running Pachyderm in Minikube, in case any early bugs make it necessary to restart the cluster. To activate Pachyderm enterprise and Pachyderm auth: pachctl enterprise activate <enterprise code> pachctl auth activate --initial-admin=robot:admin These commands cause Pachyderm's auth system to start verifying attempts to read and write Pachyderm data and blocking unauthorized users. Whichever user ran this command automatically authenticates as robot:admin and has admin privileges in the cluster (run pachctl auth whoami , as shown below, to confirm) Users will either need to set the --initial-admin admin flag or have one GitHub-based user in the system. The reason: 1. Pachyderm requires there to be at least one cluster admin if auth is activated 1. Pachyderm uses GitHub for authentication by default. Without this flag, Pachyderm asks the caller to go through an OAuth flow with GitHub, and then at the conclusion, makes the caller the cluster admin. Then whoever activated Pachyderm's auth system can assume admin status by re-authenticating via GitHub and performing any necessary actions 1. To avoid the OAuth flow, though, it's also possible to make the initial cluster admin a \"robot user\". Setting --initial-admin=robot:<something> does this. 1. Pachyderm will print out a Pachyderm token that authenticates the holder as this robot user. At any point, you can authenticate as this robot user by running $ pachctl auth use-auth-token Please paste your Pachyderm auth token: <paste robot token emitted by \"pachctl auth activate --initial-admin=robot:admin\"> $ pachctl auth whoami You are \"robot:admin\" You are an administrator of this Pachyderm cluster","title":"Activation"},{"location":"enterprise/saml_setup/#create-idp-test-app","text":"This image shows an example configuration for an Okta test app that authenticates Okta users with Pachyderm: Pachyderm also needs a URL where it can scrape SAML metadata from the ID provider. All SAML ID providers should provide such a URL; the Okta metadata URL, for example, can be retrieved here:","title":"Create IdP test app"},{"location":"enterprise/saml_setup/#write-pachyderm-config","text":"This enables the Pachyderm ACS. See inline comments: # Lookup current config version--pachyderm config has a barrier to prevent # read-modify-write conflicts between admins live_config_version = \" $( pachctl auth get-config | jq .live_config_version ) \" live_config_version = \" ${ live_config_version :- 0 } \" Set the Pachyderm config: pachctl auth set-config <<EOF { \"live_config_version\": ${live_config_version}, \"id_providers\": [ { \"name\": \"saml\", \"description\": \"Okta test app metadata\", \"saml\": { \"metadata_url\": <okta app metadata URL>, \"group_attribute\": \"memberOf\" } } ], \"saml_svc_options\": { \"acs_url\": \"http://localhost:30654/saml/acs\", \"metadata_url\": \"http://localhost:30654/saml/metadata\", \"dash_url\": \"http://localhost:30080/auth/autologin?lol=wut\", \"session_duration\": \"8h\", } } EOF","title":"Write Pachyderm config"},{"location":"enterprise/saml_setup/#logging-in","text":"Currently Pachyderm only supports IdP-initiated authentication. Configure an Okta app to point to the Pachyderm ACS ( http://localhost:30654/saml/acs if using pachctl 's port forwarding, then sign in via the new Okta app This should allow you to log in at the Pachyderm dash. To log in with the Pachyderm CLI, get a One-Time Password from the Pachyderm dash, and then run pachctl auth login --code=<one-time password> in your terminal.","title":"Logging In"},{"location":"enterprise/saml_setup/#other-features","text":"","title":"Other features"},{"location":"enterprise/saml_setup/#debug-logging","text":"If we run into issues while deploying this, it may be useful to enable a collection of debug logs that we added during development. To do so, add the option \"debug_logging\": true to \"saml_svc_options\" : pachctl auth set-config <<EOF { ... \"saml_svc_options\": { ... \"debug_logging\": true } } EOF","title":"Debug Logging"},{"location":"enterprise/saml_setup/#groups","text":"Pachyderm has very preliminary, experimental support for groups. While they won't appear in ACLs in the dash (and may have other issues), you can experiment using the CLI by setting \"group_attribute\" in the IDProvider field of the auth config: pachctl auth set-config <<EOF { ... \"id_providers\": [ { ... \"saml\": { \"group_attribute\": \"memberOf\" } } ], } EOF Then, try: pachctl create repo group-test pachctl put file group-test@master -f some-data.txt pachctl auth set group/saml: \"Test Group\" reader group-test Elsewhere: pachctl auth login --code = <auth code> pachctl get file group-test@master:some-data.txt # should work for members of \"Test Group\"","title":"Groups"},{"location":"enterprise/saml_usage/","text":"Use SAML \u00b6 This section walks you through an example of using Pachyderm's experimental SAML support. We'll describe: Authenticating via a SAML ID Provider Authenticating in the CLI Authorizing a user or group to access data Setup \u00b6 Follow the instructions in Configure SAML to enable auth in a Pachyderm cluster and connect it to a SAML ID provider. Then, we'll authenticate as a cluster admin in one console and set up our open CV demo . In the CLI, that would look like: (admin)$ pachctl auth use-auth-token Please paste your Pachyderm auth token: <auth token> (admin)$ pachctl auth whoami You are \"robot:admin\" You are an administrator of this Pachyderm cluster (admin)$ pachctl create repo images (admin)$ pachctl create pipeline -f examples/opencv/edges.json (admin)$ pachctl create pipeline -f examples/opencv/montage.json (admin)$ pachctl put file images@master -i examples/opencv/images.txt (admin)$ pachctl put file images@master -i examples/opencv/images2.txt (admin)$ pachctl list repo NAME CREATED SIZE (MASTER) ACCESS LEVEL montage 2 minutes ago 1.653MiB OWNER edges 2 minutes ago 133.6KiB OWNER images 2 minutes ago 238.3KiB OWNER (admin)$ pachctl list job ID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE 023a478b16e849b4996c19632fee6782 montage/e3dd7e9cacc5450c92e0e62ab844bd26 2 minutes ago 8 seconds 0 1 + 0 / 1 371.9KiB 1.283MiB success fe8b409e0db54f96bbb757d4d0679186 edges/9cc634a63f794a14a78e931bea47fa73 2 minutes ago 5 seconds 0 2 + 1 / 3 181.1KiB 111.4KiB success 152cb8a0b0854d44affb4bf4bd57228f montage/82a49260595246fe8f6a7d381e092650 2 minutes ago 5 seconds 0 1 + 0 / 1 79.49KiB 378.6KiB success 86e6eb4ae1e74745b993c2e47eba05e9 edges/ee7ebdddd31d46d1af10cee25f17870b 2 minutes ago 4 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success Authenticating via a SAML ID Provider (in the dashboard) \u00b6 Before authenticating, navigating to the dash will yield a blank screen: Even through the dash suggests logging in via GitHub, we will log in using a SAML IdP (which has hopefully already been configured). To see your Pachyderm DAG, navigate to your SAML ID provider and sign in to your Pachyderm cluster there (currently Pachyderm only supports IdP-initiate SAML authentication). Once you've authenticated, you'll be redirected to the Pachyderm dash (the redirect URL is configured in the Pachyderm auth system). You'll be given the opportunity to generate a one-time password (OTP), though you can always do this later from the settings panel. After closing the OTP panel, you'll be able to see the Pachyderm DAG, but you may not have access to any of the repos inside (a repo that you cannot read is indicated by a lock symbol): Authenticating in the CLI \u00b6 After authenticating in the dash, you'll be given the opportunity to generate a one-time password (OTP) and sign in on the CLI. You can also generate an OTP from the settings panel: (user)$ pachctl auth login --code auth_code:73db4686e3e142508fa74aae920cc58b (user)$ pachctl auth whoami You are \"saml:msteffen@pachyderm.io\" session expires: 14 Sep 18 20:55 PDT Note that this session expires after 8 hours. The duration of sessions is configurable in the Pachyderm auth config, but it's important that they be relatively short, as SAML group memberships are only updated when users sign in. If a user is removed from a group, they'll still be able to access the group's resources until their session expires. Authorizing a user or group to access data \u00b6 First, we'll give the example of an admin granting a user access. This can be accomplished on the CLI like so: (admin)$ pachctl auth set saml:msteffen@pachyderm.io reader images Now, the images repo is no longer locked when that user views the DAG: At this point, you can click on the images repo and preview data inside: Likewise, you can grant access to repos via groups. You'll need a SAML ID provider that supports group attributes, and you'll need to put the name of that attribute in the Pachyderm auth config. Here, we'll grant access to the Everyone group: (admin)$ pachctl auth set group/saml:Everyone owner edges Now, the edges repo is also not locked: Also, becase msteffen@pachyderm.io has OWNER provileges in the edges repo (via the Everyone group), the ACL for edges can be edited. msteffen@pachyderm.io will use OWNER privileges gained via the Everyone group to add msteffen@pachyderm.io (the user principal) directly to that ACL: this change is reflected in the CLI as well: ( admin ) $ pachctl auth get edges pipeline:edges: WRITER pipeline:montage: READER group/saml:Everyone: OWNER saml:msteffen@pachyderm.io: READER robot:admin: OWNER Conclusion \u00b6 This is just an example of Pachyderm's auth system, meant to illustrate the general nature of available features. Hopefully, it clarifies whether Pachyderm can meet your requirements.","title":"Use SAML"},{"location":"enterprise/saml_usage/#use-saml","text":"This section walks you through an example of using Pachyderm's experimental SAML support. We'll describe: Authenticating via a SAML ID Provider Authenticating in the CLI Authorizing a user or group to access data","title":"Use SAML"},{"location":"enterprise/saml_usage/#setup","text":"Follow the instructions in Configure SAML to enable auth in a Pachyderm cluster and connect it to a SAML ID provider. Then, we'll authenticate as a cluster admin in one console and set up our open CV demo . In the CLI, that would look like: (admin)$ pachctl auth use-auth-token Please paste your Pachyderm auth token: <auth token> (admin)$ pachctl auth whoami You are \"robot:admin\" You are an administrator of this Pachyderm cluster (admin)$ pachctl create repo images (admin)$ pachctl create pipeline -f examples/opencv/edges.json (admin)$ pachctl create pipeline -f examples/opencv/montage.json (admin)$ pachctl put file images@master -i examples/opencv/images.txt (admin)$ pachctl put file images@master -i examples/opencv/images2.txt (admin)$ pachctl list repo NAME CREATED SIZE (MASTER) ACCESS LEVEL montage 2 minutes ago 1.653MiB OWNER edges 2 minutes ago 133.6KiB OWNER images 2 minutes ago 238.3KiB OWNER (admin)$ pachctl list job ID OUTPUT COMMIT STARTED DURATION RESTART PROGRESS DL UL STATE 023a478b16e849b4996c19632fee6782 montage/e3dd7e9cacc5450c92e0e62ab844bd26 2 minutes ago 8 seconds 0 1 + 0 / 1 371.9KiB 1.283MiB success fe8b409e0db54f96bbb757d4d0679186 edges/9cc634a63f794a14a78e931bea47fa73 2 minutes ago 5 seconds 0 2 + 1 / 3 181.1KiB 111.4KiB success 152cb8a0b0854d44affb4bf4bd57228f montage/82a49260595246fe8f6a7d381e092650 2 minutes ago 5 seconds 0 1 + 0 / 1 79.49KiB 378.6KiB success 86e6eb4ae1e74745b993c2e47eba05e9 edges/ee7ebdddd31d46d1af10cee25f17870b 2 minutes ago 4 seconds 0 1 + 0 / 1 57.27KiB 22.22KiB success","title":"Setup"},{"location":"enterprise/saml_usage/#authenticating-via-a-saml-id-provider-in-the-dashboard","text":"Before authenticating, navigating to the dash will yield a blank screen: Even through the dash suggests logging in via GitHub, we will log in using a SAML IdP (which has hopefully already been configured). To see your Pachyderm DAG, navigate to your SAML ID provider and sign in to your Pachyderm cluster there (currently Pachyderm only supports IdP-initiate SAML authentication). Once you've authenticated, you'll be redirected to the Pachyderm dash (the redirect URL is configured in the Pachyderm auth system). You'll be given the opportunity to generate a one-time password (OTP), though you can always do this later from the settings panel. After closing the OTP panel, you'll be able to see the Pachyderm DAG, but you may not have access to any of the repos inside (a repo that you cannot read is indicated by a lock symbol):","title":"Authenticating via a SAML ID Provider (in the dashboard)"},{"location":"enterprise/saml_usage/#authenticating-in-the-cli","text":"After authenticating in the dash, you'll be given the opportunity to generate a one-time password (OTP) and sign in on the CLI. You can also generate an OTP from the settings panel: (user)$ pachctl auth login --code auth_code:73db4686e3e142508fa74aae920cc58b (user)$ pachctl auth whoami You are \"saml:msteffen@pachyderm.io\" session expires: 14 Sep 18 20:55 PDT Note that this session expires after 8 hours. The duration of sessions is configurable in the Pachyderm auth config, but it's important that they be relatively short, as SAML group memberships are only updated when users sign in. If a user is removed from a group, they'll still be able to access the group's resources until their session expires.","title":"Authenticating in the CLI"},{"location":"enterprise/saml_usage/#authorizing-a-user-or-group-to-access-data","text":"First, we'll give the example of an admin granting a user access. This can be accomplished on the CLI like so: (admin)$ pachctl auth set saml:msteffen@pachyderm.io reader images Now, the images repo is no longer locked when that user views the DAG: At this point, you can click on the images repo and preview data inside: Likewise, you can grant access to repos via groups. You'll need a SAML ID provider that supports group attributes, and you'll need to put the name of that attribute in the Pachyderm auth config. Here, we'll grant access to the Everyone group: (admin)$ pachctl auth set group/saml:Everyone owner edges Now, the edges repo is also not locked: Also, becase msteffen@pachyderm.io has OWNER provileges in the edges repo (via the Everyone group), the ACL for edges can be edited. msteffen@pachyderm.io will use OWNER privileges gained via the Everyone group to add msteffen@pachyderm.io (the user principal) directly to that ACL: this change is reflected in the CLI as well: ( admin ) $ pachctl auth get edges pipeline:edges: WRITER pipeline:montage: READER group/saml:Everyone: OWNER saml:msteffen@pachyderm.io: READER robot:admin: OWNER","title":"Authorizing a user or group to access data"},{"location":"enterprise/saml_usage/#conclusion","text":"This is just an example of Pachyderm's auth system, meant to illustrate the general nature of available features. Hopefully, it clarifies whether Pachyderm can meet your requirements.","title":"Conclusion"},{"location":"enterprise/stats/","text":"Advanced Statistics \u00b6 To use the advanced statistics features in Pachyderm Enterprise Edition, you need to: Run your pipelines on a Pachyderm cluster that has activated Enterprise features. See Deploying Enterprise Edition . Enable stats collection in your pipelines by including \"enable_stats\": true in your pipeline specification . Advanced statistics provides the following information for any jobs corresponding to your pipelines: The amount of data that was uploaded and downloaded during the job and on a per-datum level. The time spend uploading and downloading data on a per-datum level. The amount of data uploaded and downloaded on a per-datum level. The total time spend processing on a per-datum level. Success/failure information on a per-datum level. The directory structure of input data that was seen by the job. The primary and recommended way to view this information is via the Pachyderm Enterprise dashboard. However, the same information is available through the pachctl inspect datum and pachctl list datum commands or through their language client equivalents. Note Pachyderm recommends that you enable stats for all of your pipelines and only disabling the feature for very stable, long-running pipelines. In most cases, the debugging and maintenance benefits of the stats data outweigh any disadvantages of storing the extra data associated with the stats. Also note, none of your data is duplicated in producing the stats. Enabling Stats for a Pipeline \u00b6 As mentioned above, enabling stats collection for a pipeline is as simple as adding the \"enable_stats\": true field to a pipeline specification. For example, to enable stats collection for the OpenCV demo pipeline , modify the pipeline specification as follows: Example { \"pipeline\" : { \"name\" : \"edges\" } , \"input\" : { \"pfs\" : { \"glob\" : \"/*\" , \"repo\" : \"images\" } } , \"transform\" : { \"cmd\" : [ \"python3\" , \"/edges.py\" ] , \"image\" : \"pachyderm/opencv\" } , \"enable_stats\" : true } Once the pipeline has been created and you have use it to process data, you can confirm that stats are being collected with list file . There should now be stats data in the output repo of the pipeline under a branch called stats : Example $ pachctl list file edges@stats NAME TYPE SIZE 002f991aa9db9f0c44a92a30dff8ab22e788f86cc851bec80d5a74e05ad12868 dir 342 .7KiB 0597f2df3f37f1bb5b9bcd6397841f30c62b2b009e79653f9a97f5f13432cf09 dir 1 .177MiB 068fac9c3165421b4e54b358630acd2c29f23ebf293e04be5aa52c6750d3374e dir 270 .3KiB 0909461500ce508c330ca643f3103f964a383479097319dbf4954de99f92f9d9 dir 109 .6KiB ... Accessing Stats Through the Sashboard \u00b6 If you have deployed and activated the Pachyderm Enterprise dashboard, you can explore advanced statistics. For example, if you navigate to the edges pipeline, you might see something similar to this: In this example case, you can see that the pipeline has 1 recent successful job and 2 recent job failures. Pachyderm advanced stats can be very helpful in debugging these job failures. When you click on one of the job failures, can see general stats about the failed job, such as total time, total data upload/download, and so on: To get more granular per-datum stats, click on the 41 datums total , to get the following information: You can identify the exact datums that caused the pipeline to fail, as well as the associated stats: Total time Time spent downloading data Time spent processing Time spent uploading data Amount of data downloaded Amount of data uploaded If we need to, you can even go a level deeper and explore the exact details of a failed datum. Clicking on one of the failed datums reveals the logs that corresponds to the datum processing failure along with the exact input files of the datum:","title":"Advanced Statistics"},{"location":"enterprise/stats/#advanced-statistics","text":"To use the advanced statistics features in Pachyderm Enterprise Edition, you need to: Run your pipelines on a Pachyderm cluster that has activated Enterprise features. See Deploying Enterprise Edition . Enable stats collection in your pipelines by including \"enable_stats\": true in your pipeline specification . Advanced statistics provides the following information for any jobs corresponding to your pipelines: The amount of data that was uploaded and downloaded during the job and on a per-datum level. The time spend uploading and downloading data on a per-datum level. The amount of data uploaded and downloaded on a per-datum level. The total time spend processing on a per-datum level. Success/failure information on a per-datum level. The directory structure of input data that was seen by the job. The primary and recommended way to view this information is via the Pachyderm Enterprise dashboard. However, the same information is available through the pachctl inspect datum and pachctl list datum commands or through their language client equivalents. Note Pachyderm recommends that you enable stats for all of your pipelines and only disabling the feature for very stable, long-running pipelines. In most cases, the debugging and maintenance benefits of the stats data outweigh any disadvantages of storing the extra data associated with the stats. Also note, none of your data is duplicated in producing the stats.","title":"Advanced Statistics"},{"location":"enterprise/stats/#enabling-stats-for-a-pipeline","text":"As mentioned above, enabling stats collection for a pipeline is as simple as adding the \"enable_stats\": true field to a pipeline specification. For example, to enable stats collection for the OpenCV demo pipeline , modify the pipeline specification as follows: Example { \"pipeline\" : { \"name\" : \"edges\" } , \"input\" : { \"pfs\" : { \"glob\" : \"/*\" , \"repo\" : \"images\" } } , \"transform\" : { \"cmd\" : [ \"python3\" , \"/edges.py\" ] , \"image\" : \"pachyderm/opencv\" } , \"enable_stats\" : true } Once the pipeline has been created and you have use it to process data, you can confirm that stats are being collected with list file . There should now be stats data in the output repo of the pipeline under a branch called stats : Example $ pachctl list file edges@stats NAME TYPE SIZE 002f991aa9db9f0c44a92a30dff8ab22e788f86cc851bec80d5a74e05ad12868 dir 342 .7KiB 0597f2df3f37f1bb5b9bcd6397841f30c62b2b009e79653f9a97f5f13432cf09 dir 1 .177MiB 068fac9c3165421b4e54b358630acd2c29f23ebf293e04be5aa52c6750d3374e dir 270 .3KiB 0909461500ce508c330ca643f3103f964a383479097319dbf4954de99f92f9d9 dir 109 .6KiB ...","title":"Enabling Stats for a Pipeline"},{"location":"enterprise/stats/#accessing-stats-through-the-sashboard","text":"If you have deployed and activated the Pachyderm Enterprise dashboard, you can explore advanced statistics. For example, if you navigate to the edges pipeline, you might see something similar to this: In this example case, you can see that the pipeline has 1 recent successful job and 2 recent job failures. Pachyderm advanced stats can be very helpful in debugging these job failures. When you click on one of the job failures, can see general stats about the failed job, such as total time, total data upload/download, and so on: To get more granular per-datum stats, click on the 41 datums total , to get the following information: You can identify the exact datums that caused the pipeline to fail, as well as the associated stats: Total time Time spent downloading data Time spent processing Time spent uploading data Amount of data downloaded Amount of data uploaded If we need to, you can even go a level deeper and explore the exact details of a failed datum. Clicking on one of the failed datums reveals the logs that corresponds to the datum processing failure along with the exact input files of the datum:","title":"Accessing Stats Through the Sashboard"},{"location":"enterprise/auth/auth/","text":"Configure Access Controls \u00b6 If access controls are activated, each data repository, or repo, in Pachyderm has an Access Control List (ACL) associated with it. The ACL includes: READERs - users who can read the data versioned in the repo. WRITERs - users with READER access who can also submit additions, deletions, or modifications of data into the repo. OWNERs - users with READER and WRITER access who can also modify the repo's ACL. Pachyderm defines the following account types: GitHub user is a user account that is associated with a GitHub account and logs in through the GitHub OAuth flow. If you do not use any third-party identity provider, you use this option. When a user tries to log in with a GitHub account, Pachyderm verifies the identity and sends a Pachyderm token for that account. Robot user is a user account that logs in with a pach-generated authentication token. Typically, you create a user in simplified workflow scenarios, such as initial SAML configuration. Pipeline is an account that Pachyderm creates for data pipelines. Pipelines inherit access control from its creator. SAML user is a user account that is associated with a Security Assertion Markup Language (SAML) identity provider. When a user tries to log in through a SAML ID provider, the system confirms the identity, associates that identity with a SAML identity provider account, and responds with the SAML identity provider token for that user. Pachyderm verifies the token, drops it, and creates a new internal token that encapsulates the information about the user. By default, Pachyderm defines one hardcoded group called admin . Users in the admin group can perform any action on the cluster including appointing other admins. Furthermore, only the cluster admins can manage a repository without ACLs. Enable Access Control \u00b6 Before you enable access controls, make sure that you have activated Pachyderm Enterprise Edition as described in Deploy Enterprise Edition . To enable access controls, complete the following steps: Verify the status of the Enterprise features by opening the Pachyderm dashboard in your browser or by running the following pachctl command: $ pachctl enterprise get-state ACTIVE Activate the Enterprise access control features by completing the steps in one of these sections: Activating Access Control with the Dashboard Activating Access Control with pachctl Activate Access Controls by Using the Dashboard \u00b6 To activate access controls in the Pachyderm dashboard, complete the following steps: Go to the Settings page. Click the Activate Access Controls button. After you click the button, Pachyderm enables you to add GitHub users as cluster admins and activate access control: After activating access controls, you should see the following screen that asks you to log in to Pachyderm: Activate Access Controls with pachctl \u00b6 To activate access controls with pachctl , choose one of these options: Activate access controls by specifying an initial admin user: $ pachctl auth activate --initial-admin = <prefix>:<user> You must prefix the username with the appropriate account type, either github:<user> or robot:<user> . If you select the latter, Pachyderm generates and returns a Pachyderm auth token that might be used to authenticate as the initial robot admin by using pachctl auth use-auth-token . You can use this option when you cannot use GitHub as an identity provider. Activate access controls with a GitHub account: $ pachctl auth activate Pachyderm prompts you to log in with your GitHub account. The GitHub account that you sign in with is the only admin until you add more by running pachctl auth modify-admins . Log in to Pachyderm \u00b6 After you activate access controls, log in to your cluster either through the dashboard or CLI. The CLI and the dashboard have independent login workflows: Log in to the dashboard . Log in to the CLI . Log in to the Dashboard \u00b6 After you have activated access controls for Pachyderm, you need to log in to use the Pachyderm dashboard as shown above in Activate Access Controls by Using the Dashboard . To log in to the dashboard, complete the following steps: Click the Get GitHub token button. If you have not previously authorized Pachyderm on GitHub, an option to Authorize Pachyderm appears. After you authorize Pachyderm, a Pachyderm user token appears: Copy and paste this token back into the Pachyderm login screen and press Enter . You are now logged in to Pachyderm, and you should see your GitHub avatar and an indication of your user in the upper left-hand corner of the dashboard: Log in to the CLI \u00b6 To log in to pachctl , complete the following steps: Type the following command: $ pachctl auth login When you run this command, pachctl provides you with a GitHub link to authenticate as a GitHub user. If you have not previously authorized Pachyderm on GitHub, an option to Authorize Pachyderm appears. After you authorize Pachyderm, a Pachyderm user token appears: Copy and paste this token back into the terminal and press enter. You are now logged in to Pachyderm! Alternatively, you can run the command: $ pachctl auth use-auth-token Paste an authentication token recieved from pachctl auth activate --initial-admin=robot:<user> or pachctl auth get-auth-token . Manage and update user access \u00b6 You can manage user access in the UI and CLI. For example, you are logged in to Pachyderm as the user dwhitena and have a repository called test . Because the user dwhitena created this repository, dwhitena has full OWNER -level access to the repo. You can confirm this in the dashboard by navigating to or clicking on the repo test : Alternatively, you can confirm your access by running the pachctl auth get ... command. Example $ pachctl auth get dwhitena test` OWNER An OWNER of test or a cluster admin can then set other user\u2019s level of access to the repo by using the pachctl auth set ... command or through the dashboard. For example, to give the GitHub users JoeyZwicker and msteffen READER , but not WRITER or OWNER , access to test and jdoliner WRITER , but not OWNER , access, click on Modify access controls under the repo details in the dashboard. This functionality allows you to add the users easily one by one: Behavior of Pipelines as Related to Access Control \u00b6 In Pachyderm, you do not explicitly grant users access to pipelines. Instead, pipelines infer access from their input and output repositories. To update a pipeline, you must have at least READER -level access to all pipeline inputs and at least WRITER -level access to the pipeline output. This is because pipelines read from their input repos and write to their output repos, and you cannot grant a pipeline more access than you have yourself. An OWNER , WRITER , or READER of a repo can subscribe a pipeline to that repo. When a user subscribes a pipeline to a repo, Pachyderm sets that user as an OWNER of that pipeline's output repo. If additional users need access to the output repository, the initial OWNER of a pipeline's output repo, or an admin, needs to configure these access rules. To update a pipeline, you must have WRITER access to the pipeline's output repos and READER access to the pipeline's input repos. Manage the Activation Code \u00b6 When an enterprise activation code expires, an auth-activated Pachyderm cluster goes into an admin-only state. In this state, only admins have access to data that is in Pachyderm. This safety measure keeps sensitive data protected, even when an enterprise subscription becomes stale. As soon as the enterprise activation code is updated by using the dashboard or CLI, the Pachyderm cluster returns to its previous state. When you deactivate access controls on a Pachyderm cluster by running pachctl auth deactivate , the cluster returns its original state that including the following changes: All ACLs are deleted. The cluster returns to being a blank slate in regards to access control. Everyone that can connect to Pachyderm can access and modify the data in all repos. No users are present in Pachyderm, and no one can log in to Pachyderm.","title":"Configure Access Controls"},{"location":"enterprise/auth/auth/#configure-access-controls","text":"If access controls are activated, each data repository, or repo, in Pachyderm has an Access Control List (ACL) associated with it. The ACL includes: READERs - users who can read the data versioned in the repo. WRITERs - users with READER access who can also submit additions, deletions, or modifications of data into the repo. OWNERs - users with READER and WRITER access who can also modify the repo's ACL. Pachyderm defines the following account types: GitHub user is a user account that is associated with a GitHub account and logs in through the GitHub OAuth flow. If you do not use any third-party identity provider, you use this option. When a user tries to log in with a GitHub account, Pachyderm verifies the identity and sends a Pachyderm token for that account. Robot user is a user account that logs in with a pach-generated authentication token. Typically, you create a user in simplified workflow scenarios, such as initial SAML configuration. Pipeline is an account that Pachyderm creates for data pipelines. Pipelines inherit access control from its creator. SAML user is a user account that is associated with a Security Assertion Markup Language (SAML) identity provider. When a user tries to log in through a SAML ID provider, the system confirms the identity, associates that identity with a SAML identity provider account, and responds with the SAML identity provider token for that user. Pachyderm verifies the token, drops it, and creates a new internal token that encapsulates the information about the user. By default, Pachyderm defines one hardcoded group called admin . Users in the admin group can perform any action on the cluster including appointing other admins. Furthermore, only the cluster admins can manage a repository without ACLs.","title":"Configure Access Controls"},{"location":"enterprise/auth/auth/#enable-access-control","text":"Before you enable access controls, make sure that you have activated Pachyderm Enterprise Edition as described in Deploy Enterprise Edition . To enable access controls, complete the following steps: Verify the status of the Enterprise features by opening the Pachyderm dashboard in your browser or by running the following pachctl command: $ pachctl enterprise get-state ACTIVE Activate the Enterprise access control features by completing the steps in one of these sections: Activating Access Control with the Dashboard Activating Access Control with pachctl","title":"Enable Access Control"},{"location":"enterprise/auth/auth/#activate-access-controls-by-using-the-dashboard","text":"To activate access controls in the Pachyderm dashboard, complete the following steps: Go to the Settings page. Click the Activate Access Controls button. After you click the button, Pachyderm enables you to add GitHub users as cluster admins and activate access control: After activating access controls, you should see the following screen that asks you to log in to Pachyderm:","title":"Activate Access Controls by Using the Dashboard"},{"location":"enterprise/auth/auth/#activate-access-controls-with-pachctl","text":"To activate access controls with pachctl , choose one of these options: Activate access controls by specifying an initial admin user: $ pachctl auth activate --initial-admin = <prefix>:<user> You must prefix the username with the appropriate account type, either github:<user> or robot:<user> . If you select the latter, Pachyderm generates and returns a Pachyderm auth token that might be used to authenticate as the initial robot admin by using pachctl auth use-auth-token . You can use this option when you cannot use GitHub as an identity provider. Activate access controls with a GitHub account: $ pachctl auth activate Pachyderm prompts you to log in with your GitHub account. The GitHub account that you sign in with is the only admin until you add more by running pachctl auth modify-admins .","title":"Activate Access Controls with pachctl"},{"location":"enterprise/auth/auth/#log-in-to-pachyderm","text":"After you activate access controls, log in to your cluster either through the dashboard or CLI. The CLI and the dashboard have independent login workflows: Log in to the dashboard . Log in to the CLI .","title":"Log in to Pachyderm"},{"location":"enterprise/auth/auth/#log-in-to-the-dashboard","text":"After you have activated access controls for Pachyderm, you need to log in to use the Pachyderm dashboard as shown above in Activate Access Controls by Using the Dashboard . To log in to the dashboard, complete the following steps: Click the Get GitHub token button. If you have not previously authorized Pachyderm on GitHub, an option to Authorize Pachyderm appears. After you authorize Pachyderm, a Pachyderm user token appears: Copy and paste this token back into the Pachyderm login screen and press Enter . You are now logged in to Pachyderm, and you should see your GitHub avatar and an indication of your user in the upper left-hand corner of the dashboard:","title":"Log in to the Dashboard"},{"location":"enterprise/auth/auth/#log-in-to-the-cli","text":"To log in to pachctl , complete the following steps: Type the following command: $ pachctl auth login When you run this command, pachctl provides you with a GitHub link to authenticate as a GitHub user. If you have not previously authorized Pachyderm on GitHub, an option to Authorize Pachyderm appears. After you authorize Pachyderm, a Pachyderm user token appears: Copy and paste this token back into the terminal and press enter. You are now logged in to Pachyderm! Alternatively, you can run the command: $ pachctl auth use-auth-token Paste an authentication token recieved from pachctl auth activate --initial-admin=robot:<user> or pachctl auth get-auth-token .","title":"Log in to the CLI"},{"location":"enterprise/auth/auth/#manage-and-update-user-access","text":"You can manage user access in the UI and CLI. For example, you are logged in to Pachyderm as the user dwhitena and have a repository called test . Because the user dwhitena created this repository, dwhitena has full OWNER -level access to the repo. You can confirm this in the dashboard by navigating to or clicking on the repo test : Alternatively, you can confirm your access by running the pachctl auth get ... command. Example $ pachctl auth get dwhitena test` OWNER An OWNER of test or a cluster admin can then set other user\u2019s level of access to the repo by using the pachctl auth set ... command or through the dashboard. For example, to give the GitHub users JoeyZwicker and msteffen READER , but not WRITER or OWNER , access to test and jdoliner WRITER , but not OWNER , access, click on Modify access controls under the repo details in the dashboard. This functionality allows you to add the users easily one by one:","title":"Manage and update user access"},{"location":"enterprise/auth/auth/#behavior-of-pipelines-as-related-to-access-control","text":"In Pachyderm, you do not explicitly grant users access to pipelines. Instead, pipelines infer access from their input and output repositories. To update a pipeline, you must have at least READER -level access to all pipeline inputs and at least WRITER -level access to the pipeline output. This is because pipelines read from their input repos and write to their output repos, and you cannot grant a pipeline more access than you have yourself. An OWNER , WRITER , or READER of a repo can subscribe a pipeline to that repo. When a user subscribes a pipeline to a repo, Pachyderm sets that user as an OWNER of that pipeline's output repo. If additional users need access to the output repository, the initial OWNER of a pipeline's output repo, or an admin, needs to configure these access rules. To update a pipeline, you must have WRITER access to the pipeline's output repos and READER access to the pipeline's input repos.","title":"Behavior of Pipelines as Related to Access Control"},{"location":"enterprise/auth/auth/#manage-the-activation-code","text":"When an enterprise activation code expires, an auth-activated Pachyderm cluster goes into an admin-only state. In this state, only admins have access to data that is in Pachyderm. This safety measure keeps sensitive data protected, even when an enterprise subscription becomes stale. As soon as the enterprise activation code is updated by using the dashboard or CLI, the Pachyderm cluster returns to its previous state. When you deactivate access controls on a Pachyderm cluster by running pachctl auth deactivate , the cluster returns its original state that including the following changes: All ACLs are deleted. The cluster returns to being a blank slate in regards to access control. Everyone that can connect to Pachyderm can access and modify the data in all repos. No users are present in Pachyderm, and no one can log in to Pachyderm.","title":"Manage the Activation Code"},{"location":"examples/examples/","text":"Examples \u00b6 OpenCV Edge Detection \u00b6 This example does edge detection using OpenCV. This is our canonical starter demo. If you haven't used Pachyderm before, start here. We'll get you started running Pachyderm locally in just a few minutes and processing sample log lines. Open CV Word Count (Map/Reduce) \u00b6 Word count is basically the \"hello world\" of distributed computation. This example is great for benchmarking in distributed deployments on large swaths of text data. Word Count Periodic Ingress from a Database \u00b6 This example pipeline executes a query periodically against a MongoDB database outside of Pachyderm. The results of the query are stored in a corresponding output repository. This repository could be used to drive additional pipeline stages periodically based on the results of the query. Periodic Ingress from MongoDB Lazy Shuffle pipeline \u00b6 This example demonstrates how lazy shuffle pipeline i.e. a pipeline that shuffles, combines files without downloading/uploading can be created. These types of pipelines are useful for intermediate processing step that aggregates or rearranges data from one or many sources. For more information see Lazy Shuffle pipeline Variant Calling and Joint Genotyping with GATK \u00b6 This example illustrates the use of GATK in Pachyderm for Germline variant calling and joint genotyping. Each stage of this GATK best practice pipeline can be scaled individually and is automatically triggered as data flows into the top of the pipeline. The example follows this tutorial from GATK, which includes more details about the various stages. GATK - Variant Calling Machine Learning \u00b6 Iris flower classification with R, Python, or Julia \u00b6 The \"hello world\" of machine learning implemented in Pachyderm. You can deploy this pipeline using R, Python, or Julia components, where the pipeline includes the training of a SVM, LDA, Decision Tree, or Random Forest model and the subsequent utilization of that model to perform inferences. R, Python, or Julia - Iris flower classification Sentiment analysis with Neon \u00b6 This example implements the machine learning template pipeline discussed in this blog post . It trains and utilizes a neural network (implemented in Python using Nervana Neon) to infer the sentiment of movie reviews based on data from IMDB. Neon - Sentiment Analysis pix2pix with TensorFlow \u00b6 If you haven't seen pix2pix, check out this great demo . In this example, we implement the training and image translation of the pix2pix model in Pachyderm, so you can generate cat images from edge drawings, day time photos from night time photos, etc. TensorFlow - pix2pix Recurrent Neural Network with Tensorflow \u00b6 Based on this Tensorflow example , this pipeline generates a new Game of Thrones script using a model trained on existing Game of Thrones scripts. Tensorflow - Recurrent Neural Network Distributed Hyperparameter Tuning \u00b6 This example demonstrates how you can evaluate a model or function in a distributed manner on multiple sets of parameters. In this particular case, we will evaluate many machine learning models, each configured uses different sets of parameters (aka hyperparameters), and we will output only the best performing model or models. Hyperparameter Tuning Spark Example \u00b6 This example demonstrates integration of Spark with Pachyderm by launching a Spark job on an existing cluster from within a Pachyderm Job. The job uses configuration info that is versioned within Pachyderm, and stores it's reduced result back into a Pachyderm output repo, maintaining full provenance and version history within Pachyderm, while taking advantage of Spark for computation. Spark Example","title":"Examples"},{"location":"examples/examples/#examples","text":"","title":"Examples"},{"location":"examples/examples/#opencv-edge-detection","text":"This example does edge detection using OpenCV. This is our canonical starter demo. If you haven't used Pachyderm before, start here. We'll get you started running Pachyderm locally in just a few minutes and processing sample log lines. Open CV","title":"OpenCV Edge Detection"},{"location":"examples/examples/#word-count-mapreduce","text":"Word count is basically the \"hello world\" of distributed computation. This example is great for benchmarking in distributed deployments on large swaths of text data. Word Count","title":"Word Count (Map/Reduce)"},{"location":"examples/examples/#periodic-ingress-from-a-database","text":"This example pipeline executes a query periodically against a MongoDB database outside of Pachyderm. The results of the query are stored in a corresponding output repository. This repository could be used to drive additional pipeline stages periodically based on the results of the query. Periodic Ingress from MongoDB","title":"Periodic Ingress from a Database"},{"location":"examples/examples/#lazy-shuffle-pipeline","text":"This example demonstrates how lazy shuffle pipeline i.e. a pipeline that shuffles, combines files without downloading/uploading can be created. These types of pipelines are useful for intermediate processing step that aggregates or rearranges data from one or many sources. For more information see Lazy Shuffle pipeline","title":"Lazy Shuffle pipeline"},{"location":"examples/examples/#variant-calling-and-joint-genotyping-with-gatk","text":"This example illustrates the use of GATK in Pachyderm for Germline variant calling and joint genotyping. Each stage of this GATK best practice pipeline can be scaled individually and is automatically triggered as data flows into the top of the pipeline. The example follows this tutorial from GATK, which includes more details about the various stages. GATK - Variant Calling","title":"Variant Calling and Joint Genotyping with GATK"},{"location":"examples/examples/#machine-learning","text":"","title":"Machine Learning"},{"location":"examples/examples/#iris-flower-classification-with-r-python-or-julia","text":"The \"hello world\" of machine learning implemented in Pachyderm. You can deploy this pipeline using R, Python, or Julia components, where the pipeline includes the training of a SVM, LDA, Decision Tree, or Random Forest model and the subsequent utilization of that model to perform inferences. R, Python, or Julia - Iris flower classification","title":"Iris flower classification with R, Python, or Julia"},{"location":"examples/examples/#sentiment-analysis-with-neon","text":"This example implements the machine learning template pipeline discussed in this blog post . It trains and utilizes a neural network (implemented in Python using Nervana Neon) to infer the sentiment of movie reviews based on data from IMDB. Neon - Sentiment Analysis","title":"Sentiment analysis with Neon"},{"location":"examples/examples/#pix2pix-with-tensorflow","text":"If you haven't seen pix2pix, check out this great demo . In this example, we implement the training and image translation of the pix2pix model in Pachyderm, so you can generate cat images from edge drawings, day time photos from night time photos, etc. TensorFlow - pix2pix","title":"pix2pix with TensorFlow"},{"location":"examples/examples/#recurrent-neural-network-with-tensorflow","text":"Based on this Tensorflow example , this pipeline generates a new Game of Thrones script using a model trained on existing Game of Thrones scripts. Tensorflow - Recurrent Neural Network","title":"Recurrent Neural Network with Tensorflow"},{"location":"examples/examples/#distributed-hyperparameter-tuning","text":"This example demonstrates how you can evaluate a model or function in a distributed manner on multiple sets of parameters. In this particular case, we will evaluate many machine learning models, each configured uses different sets of parameters (aka hyperparameters), and we will output only the best performing model or models. Hyperparameter Tuning","title":"Distributed Hyperparameter Tuning"},{"location":"examples/examples/#spark-example","text":"This example demonstrates integration of Spark with Pachyderm by launching a Spark job on an existing cluster from within a Pachyderm Job. The job uses configuration info that is versioned within Pachyderm, and stores it's reduced result back into a Pachyderm output repo, maintaining full provenance and version history within Pachyderm, while taking advantage of Spark for computation. Spark Example","title":"Spark Example"},{"location":"getting_started/beginner_tutorial/","text":"Beginner Tutorial \u00b6 Welcome to the beginner tutorial for Pachyderm! If you have already installed Pachyderm, this tutorial should take about 15. This tutorial introduces basic Pachyderm concepts. Image processing with OpenCV \u00b6 This tutorial walks you through the deployment of a Pachyderm pipeline that performs edge detection on a few images. Thanks to Pachyderm's built-in processing primitives, we can keep our code simple but still run the pipeline in a distributed, streaming fashion. Moreover, as new data is added, the pipeline automatically processes it and outputs the results. If you hit any errors not covered in this guide, get help in our public community Slack , submit an issue on GitHub , or email us at support@pachyderm.io . We are more than happy to help! Prerequisites \u00b6 This guide assumes that you already have Pachyderm running locally. If you haven't done so already, install Pachyderm on your local machine as described in Local Installation . Create a Repo \u00b6 A repo is the highest level data primitive in Pachyderm. Like many things in Pachyderm, it shares its name with a primitive in Git and is designed to behave analogously. Generally, repos should be dedicated to a single source of data such as log messages from a particular service, a users table, or training data for an ML model. Repos are easy to create and do not take much space when empty so do not worry about making tons of them. For this demo, we create a repo called images to hold the data we want to process: $ pachctl create repo images $ pachctl list repo NAME CREATED SIZE ( MASTER ) images 7 seconds ago 0B This output shows that the repo has been successfully created. Because we have not added anything to it yet, the size of the repository HEAD commit on the master branch is 0B. Adding Data to Pachyderm \u00b6 Now that we have created a repo it is time to add some data. In Pachyderm, you write data to an explicit commit . Commits are immutable snapshots of your data which give Pachyderm its version control properties. You can add, remove, or update files in a given commit. Let's start by just adding a file, in this case an image, to a new commit. We have provided some sample images for you that we host on Imgur. Use the pachctl put file command along with the -f flag. The -f flag can take either a local file, a URL, or a object storage bucket which it scrapes automatically. In this case, we simply pass the URL. Unlike Git, commits in Pachyderm must be explicitly started and finished as they can contain huge amounts of data and we do not want that much dirty data hanging around in an unpersisted state. pachctl put file automatically starts and finishes a commit for you so you can add files more easily. If you want to add many files over a period of time, you can do pachctl start commit and pachctl finish commit yourself. We also specify the repo name \"images\" , the branch name \"master\" , and the file name: \"liberty.png\" . Here is an example atomic commit of the file liberty.png to the images repo master branch: $ pachctl put file images@master:liberty.png -f http://imgur.com/46Q8nDz.png We can check to make sure the data we just added is in Pachyderm. Use the pachctl list repo command to check that data has been added: $ pachctl list repo NAME CREATED SIZE ( MASTER ) images About a minute ago 57 .27KiB View the commit that was just created: $ pachctl list commit images REPO COMMIT PARENT STARTED DURATION SIZE images d89758a7496a4c56920b0eaa7d7d3255 <none> 29 seconds ago Less than a second 57 .27KiB View the file in that commit: $ pachctl list file images@master COMMIT NAME TYPE COMMITTED SIZE d89758a7496a4c56920b0eaa7d7d3255 /liberty.png file About a minute ago 57 .27KiB Also, you can view the file you have just added to Pachyderm. Because this is an image, you cannot just print it out in the terminal, but the following commands will let you view it easily: If you are on macOS, run: $ pachctl get file images@master:liberty.png | open -f -a /Applications/Preview.app If you on Linux, run: $ pachctl get file images@master:liberty.png | display Create a Pipeline \u00b6 Now that you have some data in your repo, it is time to do something with it. Pipelines are the core processing primitive in Pachyderm and you can define them with a JSON encoding. For this example, we have already created the pipeline for you and you can find the code on GitHub . When you want to create your own pipelines later, you can refer to the full Pipeline Specification to use more advanced options. Options include building your own code into a container instead of the pre-built Docker image that we are using in this tutorial. For now, we are going to create a single pipeline that takes in images and does some simple edge detection. Below is the pipeline spec and python code that we are using. Let's walk through the details. # edges.json { \"pipeline\" : { \"name\" : \"edges\" } , \"transform\" : { \"cmd\" : [ \"python3\" , \"/edges.py\" ] , \"image\" : \"pachyderm/opencv\" } , \"input\" : { \"pfs\" : { \"repo\" : \"images\" , \"glob\" : \"/*\" } } } Our pipeline spec contains a few simple sections. First, it is the pipeline name , edges. Then we have the transform which specifies the docker image we want to use, pachyderm/opencv (defaults to DockerHub as the registry), and the entry point edges.py . Lastly, we specify the input. Here we only have one PFS input, our images repo with a particular glob pattern. The glob pattern defines how the input data can be broken up if we want to distribute our computation. /* means that each file can be processed individually, which makes sense for images. Glob patterns are one of the most powerful features in Pachyderm. The following text is the Python code that we run in this pipeline: # edges.py import cv2 import numpy as np from matplotlib import pyplot as plt import os # make_edges reads an image from /pfs/images and outputs the result of running # edge detection on that image to /pfs/out. Note that /pfs/images and # /pfs/out are special directories that Pachyderm injects into the container. def make_edges ( image ): img = cv2 . imread ( image ) tail = os . path . split ( image )[ 1 ] edges = cv2 . Canny ( img , 100 , 200 ) plt . imsave ( os . path . join ( \"/pfs/out\" , os . path . splitext ( tail )[ 0 ] + '.png' ), edges , cmap = 'gray' ) # walk /pfs/images and call make_edges on every file found for dirpath , dirs , files in os . walk ( \"/pfs/images\" ): for file in files : make_edges ( os . path . join ( dirpath , file )) The code simply walks over all the images in /pfs/images , performs edge detection, and writes the result to /pfs/out . /pfs/images and /pfs/out are special local directories that Pachyderm creates within the container automatically. All the input data for a pipeline is stored in /pfs/<input_repo_name> and your code should always write out to /pfs/out . Pachyderm automatically gathers everything you write to /pfs/out and version it as this pipeline output. Now, let's create the pipeline in Pachyderm: $ pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/master/examples/opencv/edges.json What Happens When You Create a Pipeline \u00b6 Creating a pipeline tells Pachyderm to run your code on the data in your input repo (the HEAD commit) as well as all future commits that occur after the pipeline is created. Our repo already had a commit, so Pachyderm automatically launched a job to process that data. The first time Pachyderm runs a pipeline job, it needs to download the Docker image (specified in the pipeline spec) from the specified Docker registry (DockerHub in this case). This first run this might take a minute or so because of the image download, depending on your Internet connection. Subsequent runs will be much faster. You can view the job with: $ pachctl list job ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 0f6a53829eeb4ca193bb7944fe693700 edges 16 seconds ago Less than a second 0 1 + 0 / 1 57 .27KiB 22 .22KiB success Yay! Our pipeline succeeded! Pachyderm creates a corresponding output repo for every pipeline. This output repo will have the same name as the pipeline, and all the results of that pipeline will be versioned in this output repo. In our example, the edges pipeline created a repo called edges to store the results. $ pachctl list repo NAME CREATED SIZE ( MASTER ) edges 2 minutes ago 22 .22KiB images 5 minutes ago 57 .27KiB Reading the Output \u00b6 We can view the output data from the edges repo in the same fashion that we viewed the input data. # on macOS $ pachctl get file edges@master:liberty.png | open -f -a /Applications/Preview.app # on Linux $ pachctl get file edges@master:liberty.png | display The output should look similar to: Processing More Data \u00b6 Pipelines will also automatically process the data from new commits as they are created. Think of pipelines as being subscribed to any new commits on their input repo(s). Also similar to Git, commits have a parental structure that tracks which files have changed. In this case we are going to be adding more images. Let's create two new commits in a parental structure. To do this we will simply do two more put file commands and by specifying master as the branch, it automatically parents our commits onto each other. Branch names are just references to a particular HEAD commit. $ pachctl put file images@master:AT-AT.png -f http://imgur.com/8MN9Kg0.png $ pachctl put file images@master:kitten.png -f http://imgur.com/g2QnNqa.png Adding a new commit of data will automatically trigger the pipeline to run on the new data we've added. We'll see corresponding jobs get started and commits to the output \"edges\" repo. Let's also view our new outputs. # view the jobs that were kicked off $ pachctl list job ID STARTED DURATION RESTART PROGRESS DL UL STATE 81ae47a802f14038b95f8f248cddbed2 7 seconds ago Less than a second 0 1 + 2 / 3 102 .4KiB 74 .21KiB success ce448c12d0dd4410b3a5ae0c0f07e1f9 16 seconds ago Less than a second 0 1 + 1 / 2 78 .7KiB 37 .15KiB success 490a28be32de491e942372018cd42460 9 minutes ago 35 seconds 0 1 + 0 / 1 57 .27KiB 22 .22KiB success # View the output data # on macOS $ pachctl get file edges@master:AT-AT.png | open -f -a /Applications/Preview.app $ pachctl get file edges@master:kitten.png | open -f -a /Applications/Preview.app # on Linux $ pachctl get file edges@master:AT-AT.png | display $ pachctl get file edges@master:kitten.png | display Adding Another Pipeline \u00b6 We have successfully deployed and used a single stage Pachyderm pipeline. Now, let's add a processing stage to illustrate a multi-stage Pachyderm pipeline. Specifically, let's add a montage pipeline that take our original and edge detected images and arranges them into a single montage of images: Below is the pipeline spec for this new pipeline: # montage.json { \"pipeline\" : { \"name\" : \"montage\" } , \"input\" : { \"cross\" : [ { \"pfs\" : { \"glob\" : \"/\" , \"repo\" : \"images\" } } , { \"pfs\" : { \"glob\" : \"/\" , \"repo\" : \"edges\" } } ] } , \"transform\" : { \"cmd\" : [ \"sh\" ] , \"image\" : \"v4tech/imagemagick\" , \"stdin\" : [ \"montage -shadow -background SkyBlue -geometry 300x300+2+2 $( find /pfs -type f | sort ) /pfs/out/montage.png\" ] } } This montage pipeline spec is similar to our edges pipeline except for three differences: (1) we are using a different Docker image that has imagemagick installed, (2) we are executing a sh command with stdin instead of a python script, and (3) we have multiple input data repositories. In the montage pipeline we are combining our multiple input data repositories using a cross pattern. This cross pattern creates a single pairing of our input images with our edge detected images. There are several interesting ways to combine data in Pachyderm, which are discussed here and here . We create the montage pipeline as before, with pachctl : $ pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/master/examples/opencv/montage.json Pipeline creating triggers a job that generates a montage for all the current HEAD commits of the input repos: $ pachctl list job ID STARTED DURATION RESTART PROGRESS DL UL STATE 92cecc40c3144fd5b4e07603bb24b104 45 seconds ago 6 seconds 0 1 + 0 / 1 371 .9KiB 1 .284MiB success 81ae47a802f14038b95f8f248cddbed2 2 minutes ago Less than a second 0 1 + 2 / 3 102 .4KiB 74 .21KiB success ce448c12d0dd4410b3a5ae0c0f07e1f9 2 minutes ago Less than a second 0 1 + 1 / 2 78 .7KiB 37 .15KiB success 490a28be32de491e942372018cd42460 11 minutes ago 35 seconds 0 1 + 0 / 1 57 .27KiB 22 .22KiB success And you can view the generated montage image via: # on macOS $ pachctl get file montage@master:montage.png | open -f -a /Applications/Preview.app # on Linux $ pachctl get file montage@master:montage.png | display Exploring your DAG in the Pachyderm dashboard \u00b6 When you deployed Pachyderm locally, the Pachyderm Enterprise dashboard was also deployed by default. This dashboard will let you interactively explore your pipeline, visualize the structure of the pipeline, explore your data, debug jobs, etc. To access the dashboard visit localhost:30080 in an Internet browser (e.g., Google Chrome). You should see something similar to this: Enter your email address if you would like to obtain a free trial token for the dashboard. Upon entering this trial token, you will be able to see your pipeline structure and interactively explore the various pieces of your pipeline as pictured below: Next Steps \u00b6 Pachyderm is now running locally with data and a pipeline! To play with Pachyderm locally, you can use what you've learned to build on or change this pipeline. You can also dig in and learn more details about: Deploying Pachyderm to the cloud or on prem Load Your Data into Pachyderm Individual Developer Workflow We'd love to help and see what you come up with, so submit any issues/questions you come across on GitHub , Slack , or email at support@pachyderm.io if you want to show off anything nifty you've created!","title":"Beginner Tutorial"},{"location":"getting_started/beginner_tutorial/#beginner-tutorial","text":"Welcome to the beginner tutorial for Pachyderm! If you have already installed Pachyderm, this tutorial should take about 15. This tutorial introduces basic Pachyderm concepts.","title":"Beginner Tutorial"},{"location":"getting_started/beginner_tutorial/#image-processing-with-opencv","text":"This tutorial walks you through the deployment of a Pachyderm pipeline that performs edge detection on a few images. Thanks to Pachyderm's built-in processing primitives, we can keep our code simple but still run the pipeline in a distributed, streaming fashion. Moreover, as new data is added, the pipeline automatically processes it and outputs the results. If you hit any errors not covered in this guide, get help in our public community Slack , submit an issue on GitHub , or email us at support@pachyderm.io . We are more than happy to help!","title":"Image processing with OpenCV"},{"location":"getting_started/beginner_tutorial/#prerequisites","text":"This guide assumes that you already have Pachyderm running locally. If you haven't done so already, install Pachyderm on your local machine as described in Local Installation .","title":"Prerequisites"},{"location":"getting_started/beginner_tutorial/#create-a-repo","text":"A repo is the highest level data primitive in Pachyderm. Like many things in Pachyderm, it shares its name with a primitive in Git and is designed to behave analogously. Generally, repos should be dedicated to a single source of data such as log messages from a particular service, a users table, or training data for an ML model. Repos are easy to create and do not take much space when empty so do not worry about making tons of them. For this demo, we create a repo called images to hold the data we want to process: $ pachctl create repo images $ pachctl list repo NAME CREATED SIZE ( MASTER ) images 7 seconds ago 0B This output shows that the repo has been successfully created. Because we have not added anything to it yet, the size of the repository HEAD commit on the master branch is 0B.","title":"Create a Repo"},{"location":"getting_started/beginner_tutorial/#adding-data-to-pachyderm","text":"Now that we have created a repo it is time to add some data. In Pachyderm, you write data to an explicit commit . Commits are immutable snapshots of your data which give Pachyderm its version control properties. You can add, remove, or update files in a given commit. Let's start by just adding a file, in this case an image, to a new commit. We have provided some sample images for you that we host on Imgur. Use the pachctl put file command along with the -f flag. The -f flag can take either a local file, a URL, or a object storage bucket which it scrapes automatically. In this case, we simply pass the URL. Unlike Git, commits in Pachyderm must be explicitly started and finished as they can contain huge amounts of data and we do not want that much dirty data hanging around in an unpersisted state. pachctl put file automatically starts and finishes a commit for you so you can add files more easily. If you want to add many files over a period of time, you can do pachctl start commit and pachctl finish commit yourself. We also specify the repo name \"images\" , the branch name \"master\" , and the file name: \"liberty.png\" . Here is an example atomic commit of the file liberty.png to the images repo master branch: $ pachctl put file images@master:liberty.png -f http://imgur.com/46Q8nDz.png We can check to make sure the data we just added is in Pachyderm. Use the pachctl list repo command to check that data has been added: $ pachctl list repo NAME CREATED SIZE ( MASTER ) images About a minute ago 57 .27KiB View the commit that was just created: $ pachctl list commit images REPO COMMIT PARENT STARTED DURATION SIZE images d89758a7496a4c56920b0eaa7d7d3255 <none> 29 seconds ago Less than a second 57 .27KiB View the file in that commit: $ pachctl list file images@master COMMIT NAME TYPE COMMITTED SIZE d89758a7496a4c56920b0eaa7d7d3255 /liberty.png file About a minute ago 57 .27KiB Also, you can view the file you have just added to Pachyderm. Because this is an image, you cannot just print it out in the terminal, but the following commands will let you view it easily: If you are on macOS, run: $ pachctl get file images@master:liberty.png | open -f -a /Applications/Preview.app If you on Linux, run: $ pachctl get file images@master:liberty.png | display","title":"Adding Data to Pachyderm"},{"location":"getting_started/beginner_tutorial/#create-a-pipeline","text":"Now that you have some data in your repo, it is time to do something with it. Pipelines are the core processing primitive in Pachyderm and you can define them with a JSON encoding. For this example, we have already created the pipeline for you and you can find the code on GitHub . When you want to create your own pipelines later, you can refer to the full Pipeline Specification to use more advanced options. Options include building your own code into a container instead of the pre-built Docker image that we are using in this tutorial. For now, we are going to create a single pipeline that takes in images and does some simple edge detection. Below is the pipeline spec and python code that we are using. Let's walk through the details. # edges.json { \"pipeline\" : { \"name\" : \"edges\" } , \"transform\" : { \"cmd\" : [ \"python3\" , \"/edges.py\" ] , \"image\" : \"pachyderm/opencv\" } , \"input\" : { \"pfs\" : { \"repo\" : \"images\" , \"glob\" : \"/*\" } } } Our pipeline spec contains a few simple sections. First, it is the pipeline name , edges. Then we have the transform which specifies the docker image we want to use, pachyderm/opencv (defaults to DockerHub as the registry), and the entry point edges.py . Lastly, we specify the input. Here we only have one PFS input, our images repo with a particular glob pattern. The glob pattern defines how the input data can be broken up if we want to distribute our computation. /* means that each file can be processed individually, which makes sense for images. Glob patterns are one of the most powerful features in Pachyderm. The following text is the Python code that we run in this pipeline: # edges.py import cv2 import numpy as np from matplotlib import pyplot as plt import os # make_edges reads an image from /pfs/images and outputs the result of running # edge detection on that image to /pfs/out. Note that /pfs/images and # /pfs/out are special directories that Pachyderm injects into the container. def make_edges ( image ): img = cv2 . imread ( image ) tail = os . path . split ( image )[ 1 ] edges = cv2 . Canny ( img , 100 , 200 ) plt . imsave ( os . path . join ( \"/pfs/out\" , os . path . splitext ( tail )[ 0 ] + '.png' ), edges , cmap = 'gray' ) # walk /pfs/images and call make_edges on every file found for dirpath , dirs , files in os . walk ( \"/pfs/images\" ): for file in files : make_edges ( os . path . join ( dirpath , file )) The code simply walks over all the images in /pfs/images , performs edge detection, and writes the result to /pfs/out . /pfs/images and /pfs/out are special local directories that Pachyderm creates within the container automatically. All the input data for a pipeline is stored in /pfs/<input_repo_name> and your code should always write out to /pfs/out . Pachyderm automatically gathers everything you write to /pfs/out and version it as this pipeline output. Now, let's create the pipeline in Pachyderm: $ pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/master/examples/opencv/edges.json","title":"Create a Pipeline"},{"location":"getting_started/beginner_tutorial/#what-happens-when-you-create-a-pipeline","text":"Creating a pipeline tells Pachyderm to run your code on the data in your input repo (the HEAD commit) as well as all future commits that occur after the pipeline is created. Our repo already had a commit, so Pachyderm automatically launched a job to process that data. The first time Pachyderm runs a pipeline job, it needs to download the Docker image (specified in the pipeline spec) from the specified Docker registry (DockerHub in this case). This first run this might take a minute or so because of the image download, depending on your Internet connection. Subsequent runs will be much faster. You can view the job with: $ pachctl list job ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 0f6a53829eeb4ca193bb7944fe693700 edges 16 seconds ago Less than a second 0 1 + 0 / 1 57 .27KiB 22 .22KiB success Yay! Our pipeline succeeded! Pachyderm creates a corresponding output repo for every pipeline. This output repo will have the same name as the pipeline, and all the results of that pipeline will be versioned in this output repo. In our example, the edges pipeline created a repo called edges to store the results. $ pachctl list repo NAME CREATED SIZE ( MASTER ) edges 2 minutes ago 22 .22KiB images 5 minutes ago 57 .27KiB","title":"What Happens When You Create a Pipeline"},{"location":"getting_started/beginner_tutorial/#reading-the-output","text":"We can view the output data from the edges repo in the same fashion that we viewed the input data. # on macOS $ pachctl get file edges@master:liberty.png | open -f -a /Applications/Preview.app # on Linux $ pachctl get file edges@master:liberty.png | display The output should look similar to:","title":"Reading the Output"},{"location":"getting_started/beginner_tutorial/#processing-more-data","text":"Pipelines will also automatically process the data from new commits as they are created. Think of pipelines as being subscribed to any new commits on their input repo(s). Also similar to Git, commits have a parental structure that tracks which files have changed. In this case we are going to be adding more images. Let's create two new commits in a parental structure. To do this we will simply do two more put file commands and by specifying master as the branch, it automatically parents our commits onto each other. Branch names are just references to a particular HEAD commit. $ pachctl put file images@master:AT-AT.png -f http://imgur.com/8MN9Kg0.png $ pachctl put file images@master:kitten.png -f http://imgur.com/g2QnNqa.png Adding a new commit of data will automatically trigger the pipeline to run on the new data we've added. We'll see corresponding jobs get started and commits to the output \"edges\" repo. Let's also view our new outputs. # view the jobs that were kicked off $ pachctl list job ID STARTED DURATION RESTART PROGRESS DL UL STATE 81ae47a802f14038b95f8f248cddbed2 7 seconds ago Less than a second 0 1 + 2 / 3 102 .4KiB 74 .21KiB success ce448c12d0dd4410b3a5ae0c0f07e1f9 16 seconds ago Less than a second 0 1 + 1 / 2 78 .7KiB 37 .15KiB success 490a28be32de491e942372018cd42460 9 minutes ago 35 seconds 0 1 + 0 / 1 57 .27KiB 22 .22KiB success # View the output data # on macOS $ pachctl get file edges@master:AT-AT.png | open -f -a /Applications/Preview.app $ pachctl get file edges@master:kitten.png | open -f -a /Applications/Preview.app # on Linux $ pachctl get file edges@master:AT-AT.png | display $ pachctl get file edges@master:kitten.png | display","title":"Processing More Data"},{"location":"getting_started/beginner_tutorial/#adding-another-pipeline","text":"We have successfully deployed and used a single stage Pachyderm pipeline. Now, let's add a processing stage to illustrate a multi-stage Pachyderm pipeline. Specifically, let's add a montage pipeline that take our original and edge detected images and arranges them into a single montage of images: Below is the pipeline spec for this new pipeline: # montage.json { \"pipeline\" : { \"name\" : \"montage\" } , \"input\" : { \"cross\" : [ { \"pfs\" : { \"glob\" : \"/\" , \"repo\" : \"images\" } } , { \"pfs\" : { \"glob\" : \"/\" , \"repo\" : \"edges\" } } ] } , \"transform\" : { \"cmd\" : [ \"sh\" ] , \"image\" : \"v4tech/imagemagick\" , \"stdin\" : [ \"montage -shadow -background SkyBlue -geometry 300x300+2+2 $( find /pfs -type f | sort ) /pfs/out/montage.png\" ] } } This montage pipeline spec is similar to our edges pipeline except for three differences: (1) we are using a different Docker image that has imagemagick installed, (2) we are executing a sh command with stdin instead of a python script, and (3) we have multiple input data repositories. In the montage pipeline we are combining our multiple input data repositories using a cross pattern. This cross pattern creates a single pairing of our input images with our edge detected images. There are several interesting ways to combine data in Pachyderm, which are discussed here and here . We create the montage pipeline as before, with pachctl : $ pachctl create pipeline -f https://raw.githubusercontent.com/pachyderm/pachyderm/master/examples/opencv/montage.json Pipeline creating triggers a job that generates a montage for all the current HEAD commits of the input repos: $ pachctl list job ID STARTED DURATION RESTART PROGRESS DL UL STATE 92cecc40c3144fd5b4e07603bb24b104 45 seconds ago 6 seconds 0 1 + 0 / 1 371 .9KiB 1 .284MiB success 81ae47a802f14038b95f8f248cddbed2 2 minutes ago Less than a second 0 1 + 2 / 3 102 .4KiB 74 .21KiB success ce448c12d0dd4410b3a5ae0c0f07e1f9 2 minutes ago Less than a second 0 1 + 1 / 2 78 .7KiB 37 .15KiB success 490a28be32de491e942372018cd42460 11 minutes ago 35 seconds 0 1 + 0 / 1 57 .27KiB 22 .22KiB success And you can view the generated montage image via: # on macOS $ pachctl get file montage@master:montage.png | open -f -a /Applications/Preview.app # on Linux $ pachctl get file montage@master:montage.png | display","title":"Adding Another Pipeline"},{"location":"getting_started/beginner_tutorial/#exploring-your-dag-in-the-pachyderm-dashboard","text":"When you deployed Pachyderm locally, the Pachyderm Enterprise dashboard was also deployed by default. This dashboard will let you interactively explore your pipeline, visualize the structure of the pipeline, explore your data, debug jobs, etc. To access the dashboard visit localhost:30080 in an Internet browser (e.g., Google Chrome). You should see something similar to this: Enter your email address if you would like to obtain a free trial token for the dashboard. Upon entering this trial token, you will be able to see your pipeline structure and interactively explore the various pieces of your pipeline as pictured below:","title":"Exploring your DAG in the Pachyderm dashboard"},{"location":"getting_started/beginner_tutorial/#next-steps","text":"Pachyderm is now running locally with data and a pipeline! To play with Pachyderm locally, you can use what you've learned to build on or change this pipeline. You can also dig in and learn more details about: Deploying Pachyderm to the cloud or on prem Load Your Data into Pachyderm Individual Developer Workflow We'd love to help and see what you come up with, so submit any issues/questions you come across on GitHub , Slack , or email at support@pachyderm.io if you want to show off anything nifty you've created!","title":"Next Steps"},{"location":"getting_started/local_installation/","text":"Local Installation \u00b6 This guide walks you through the steps to install Pachyderm on macOS\u00ae, Linux\u00ae, or Windows\u00ae. Local installation helps you to learn some of the Pachyderm basics and is not designed to be a production environment. Prerequisites \u00b6 Before you can deploy Pachyderm, make sure you have the following programs installed on your computer: Minikube Oracle\u00ae VirtualBox\u2122 or Docker Desktop (v18.06+) Pachyderm Command Line Interface If you install Pachyderm on Windows 10 or later, you must have the following components installed in addition to the ones listed above: Windows Subsystem for Linux (WSL) Note For a Windows installation, use Minikube. Using Minikube \u00b6 On your local machine, you can run Pachyderm in a minikube virtual machine. Minikube is a tool that creates a single-node Kubernetes cluster. This limited installation is sufficient to try basic Pachyderm functionality and complete the Beginner Tutorial. To configure Minikube, follow these steps: Install minikube and VirtualBox in your operating system as described in the Kubernetes documentation . Install kubectl . Start minikube : minikube start Note Any time you want to stop and restart Pachyderm, run minikube delete and minikube start . Minikube is not meant to be a production environment and does not handle being restarted well without a full wipe. Docker Desktop \u00b6 If you are using Minikube, skip this section and proceed to Install pachctl You can use Docker Desktop instead of Minikube on macOS or Linux by following these steps: In the Docker Desktop settings, verify that Kubernetes is enabled: From the command prompt, confirm that Kubernetes is running: $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 56d To reset your Kubernetes cluster that runs on Docker Desktop, click the Reset button in the Preferences sub-menu. Install pachctl \u00b6 pachctl is a command-line utility that you can use to interact with a Pachyderm cluster. To deploy Pachyderm locally, you need to have pachctl installed on your machine by following these steps: Run the corresponding steps for your operating system: For macOS, run: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 For a Debian-based Linux 64-bit or Windows 10 or later running on WSL: $ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_amd64.deb && sudo dpkg -i /tmp/pachctl.deb For all other Linux flavors: $ curl -o /tmp/pachctl.tar.gz -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_linux_amd64.tar.gz && tar -xvf /tmp/pachctl.tar.gz -C /tmp && sudo cp /tmp/pachctl_1.9.5_linux_amd64/pachctl /usr/local/bin Verify that installation was successful by running pachctl version --client-only : $ pachctl version --client-only COMPONENT VERSION pachctl 1 .9.5 If you run pachctl version without --client-only , the command times out. This is expected behavior because pachd is not yet running. Deploy Pachyderm \u00b6 After you configure all the Prerequisites , deploy Pachyderm by following these steps: For macOS or Linux, run: $ pachctl deploy local This command generates a Pachyderm manifest and deploys Pachyderm on Kubernetes. For Windows: Start WSL. In WSL, run: $ pachctl deploy local --dry-run > pachyderm.json Copy the pachyderm.json file into your Pachyderm directory. From the same directory, run: kubectl create -f . \\p achyderm.json Because Pachyderm needs to pull the Pachyderm Docker image from DockerHub, it might take a few minutes for the Pachyderm pods status to change to Running . Check the status of the Pachyderm pods by periodically running kubectl get pods . When Pachyderm is ready for use, all Pachyderm pods must be in the Running status. $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-vb972 2 /2 Running 0 6m etcd-7dbb489f44-9v5jj 1 /1 Running 0 6m pachd-6c878bbc4c-f2h2c 1 /1 Running 0 6m If you see a few restarts on the pachd nodes, that means that Kubernetes tried to bring up those pods before etcd was ready. Therefore, Kubernetes restarted those pods. You can safely ignore that message. Run pachctl version to verify that pachd has been deployed. $ pachctl version COMPONENT VERSION pachctl 1 .9.5 pachd 1 .9.5 Open a new terminal window. Use port forwarding to access the Pachyderm dashboard. $ pachctl port-forward This command runs continuosly and does not exit unless you interrupt it. Alternatively, you can set up Pachyderm to directly connect to the Minikube instance: Get your Minikube IP address: $ minikube ip Configure Pachyderm to connect directly to the Minikube instance: bash $ pachctl config update context `pachctl config get active-context` --pachd-address=`minikube ip`:30650 Next Steps \u00b6 After you install and configure Pachyderm, continue exploring Pachyderm: Complete the Beginner Tutorial to learn the basics of Pachyderm, such as adding data and building analysis pipelines. Explore the Pachyderm Dashboard. By default, Pachyderm deploys the Pachyderm Enterprise dashboard. You can use a FREE trial token to experiment with the dashboard. Point your browser to port 30080 on your minikube IP. Alternatively, if you cannot connect directly, enable port forwarding by running pachctl port-forward , and then point your browser to localhost:30080 . See also: General Troubleshooting","title":"Local Installation"},{"location":"getting_started/local_installation/#local-installation","text":"This guide walks you through the steps to install Pachyderm on macOS\u00ae, Linux\u00ae, or Windows\u00ae. Local installation helps you to learn some of the Pachyderm basics and is not designed to be a production environment.","title":"Local Installation"},{"location":"getting_started/local_installation/#prerequisites","text":"Before you can deploy Pachyderm, make sure you have the following programs installed on your computer: Minikube Oracle\u00ae VirtualBox\u2122 or Docker Desktop (v18.06+) Pachyderm Command Line Interface If you install Pachyderm on Windows 10 or later, you must have the following components installed in addition to the ones listed above: Windows Subsystem for Linux (WSL) Note For a Windows installation, use Minikube.","title":"Prerequisites"},{"location":"getting_started/local_installation/#using-minikube","text":"On your local machine, you can run Pachyderm in a minikube virtual machine. Minikube is a tool that creates a single-node Kubernetes cluster. This limited installation is sufficient to try basic Pachyderm functionality and complete the Beginner Tutorial. To configure Minikube, follow these steps: Install minikube and VirtualBox in your operating system as described in the Kubernetes documentation . Install kubectl . Start minikube : minikube start Note Any time you want to stop and restart Pachyderm, run minikube delete and minikube start . Minikube is not meant to be a production environment and does not handle being restarted well without a full wipe.","title":"Using Minikube"},{"location":"getting_started/local_installation/#docker-desktop","text":"If you are using Minikube, skip this section and proceed to Install pachctl You can use Docker Desktop instead of Minikube on macOS or Linux by following these steps: In the Docker Desktop settings, verify that Kubernetes is enabled: From the command prompt, confirm that Kubernetes is running: $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 56d To reset your Kubernetes cluster that runs on Docker Desktop, click the Reset button in the Preferences sub-menu.","title":"Docker Desktop"},{"location":"getting_started/local_installation/#install-pachctl","text":"pachctl is a command-line utility that you can use to interact with a Pachyderm cluster. To deploy Pachyderm locally, you need to have pachctl installed on your machine by following these steps: Run the corresponding steps for your operating system: For macOS, run: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 For a Debian-based Linux 64-bit or Windows 10 or later running on WSL: $ curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_amd64.deb && sudo dpkg -i /tmp/pachctl.deb For all other Linux flavors: $ curl -o /tmp/pachctl.tar.gz -L https://github.com/pachyderm/pachyderm/releases/download/v1.9.5/pachctl_1.9.5_linux_amd64.tar.gz && tar -xvf /tmp/pachctl.tar.gz -C /tmp && sudo cp /tmp/pachctl_1.9.5_linux_amd64/pachctl /usr/local/bin Verify that installation was successful by running pachctl version --client-only : $ pachctl version --client-only COMPONENT VERSION pachctl 1 .9.5 If you run pachctl version without --client-only , the command times out. This is expected behavior because pachd is not yet running.","title":"Install pachctl"},{"location":"getting_started/local_installation/#deploy-pachyderm","text":"After you configure all the Prerequisites , deploy Pachyderm by following these steps: For macOS or Linux, run: $ pachctl deploy local This command generates a Pachyderm manifest and deploys Pachyderm on Kubernetes. For Windows: Start WSL. In WSL, run: $ pachctl deploy local --dry-run > pachyderm.json Copy the pachyderm.json file into your Pachyderm directory. From the same directory, run: kubectl create -f . \\p achyderm.json Because Pachyderm needs to pull the Pachyderm Docker image from DockerHub, it might take a few minutes for the Pachyderm pods status to change to Running . Check the status of the Pachyderm pods by periodically running kubectl get pods . When Pachyderm is ready for use, all Pachyderm pods must be in the Running status. $ kubectl get pods NAME READY STATUS RESTARTS AGE dash-6c9dc97d9c-vb972 2 /2 Running 0 6m etcd-7dbb489f44-9v5jj 1 /1 Running 0 6m pachd-6c878bbc4c-f2h2c 1 /1 Running 0 6m If you see a few restarts on the pachd nodes, that means that Kubernetes tried to bring up those pods before etcd was ready. Therefore, Kubernetes restarted those pods. You can safely ignore that message. Run pachctl version to verify that pachd has been deployed. $ pachctl version COMPONENT VERSION pachctl 1 .9.5 pachd 1 .9.5 Open a new terminal window. Use port forwarding to access the Pachyderm dashboard. $ pachctl port-forward This command runs continuosly and does not exit unless you interrupt it. Alternatively, you can set up Pachyderm to directly connect to the Minikube instance: Get your Minikube IP address: $ minikube ip Configure Pachyderm to connect directly to the Minikube instance: bash $ pachctl config update context `pachctl config get active-context` --pachd-address=`minikube ip`:30650","title":"Deploy Pachyderm"},{"location":"getting_started/local_installation/#next-steps","text":"After you install and configure Pachyderm, continue exploring Pachyderm: Complete the Beginner Tutorial to learn the basics of Pachyderm, such as adding data and building analysis pipelines. Explore the Pachyderm Dashboard. By default, Pachyderm deploys the Pachyderm Enterprise dashboard. You can use a FREE trial token to experiment with the dashboard. Point your browser to port 30080 on your minikube IP. Alternatively, if you cannot connect directly, enable port forwarding by running pachctl port-forward , and then point your browser to localhost:30080 . See also: General Troubleshooting","title":"Next Steps"},{"location":"how-tos/combining/","text":"Combine, Merge, and Join Data \u00b6 Info Before you read this section, make sure that you understand the concepts described in Distributed Processing . In some of your projects, you might need to match datums from multiple data repositories to process, join, or aggregate data. For example, you might need to process together multiple records that correspond to a certain user, experiment, or device. In these cases, you can create to pipelines that perform the following steps: More specifically, you need to create the following pipelines: Create a pipeline that groups all of the records for a specific key and index. Create another pipeline that takes that grouped output and performs the merging, joining, or other processing for the group. You can use these two data-combining pipelines for merging or grouped processing of data from various experiments, devices, and so on. You can also apply the same pattern to perform distributed joins of tabular data or data from database tables. For example, you can join user email records together with user IP records on the key and index of a user ID. You can parallelize each of the stages across workers to scale with the size of your data and the number of data sources that you want to merge. Tip If your data is not split into separate files for each record, you can split it automatically as described in Splitting Data for Distributed Processing . Group Matching Records \u00b6 The first pipeline that you create groups the records that need to be processed together. In this example, you have two repositories A and B with JSON records. These repositories might correspond to two experiments, two geographic regions, two different devices that generate data, or other. The following diagram displays the first pipeline: The repository A has the following structure: $ pachctl list file A@master NAME TYPE SIZE 1 .json file 39 B 2 .json file 39 B 3 .json file 39 B The repository B has the following structure: $ pachctl list file B@master NAME TYPE SIZE 1 .json file 39 B 2 .json file 39 B 3 .json file 39 B If you want to process A/1.json with B/1.json to merge their contents or otherwise process them together, you need to group each set of JSON records into respective datums that the pipelines that you create in Process grouped records can process together. The grouping pipeline takes a union of A and B as inputs, each with glob pattern /* . While the pipeline processes a JSON file, the data is copied to a folder in the output that corresponds to the key and index for that record. In this example, it is just the number in the file name. Pachyderm also renames the files to unique names that correspond to the source: /1 A.json B.json /2 A.json B.json /3 A.json B.json When you group your data, set the following parameters in the pipeline specification: In the pfs section, set \"empty_files\": true to avoid unnecessary downloads of data. Use symlinks to avoid unnecessary uploads of data and unnecessary data duplication. Process Grouped Records \u00b6 After you group the records together by using the grouping pipeline, you can use a merging pipeline on the group repository with a glob pattern of /* . By using the glob pattern of /* the pipeline can process each grouping of records in parallel. THe following diagram displays the second pipeline: The second pipeline performs merging, aggregation, or other processing on the respective grouping of records. It can also output each respective result to the root of the output directory: $ pachctl list file merge@master NAME TYPE SIZE result_1.json file 39 B result_2.json file 39 B result_3.json file 39 B","title":"Combine, Merge, and Join Data"},{"location":"how-tos/combining/#combine-merge-and-join-data","text":"Info Before you read this section, make sure that you understand the concepts described in Distributed Processing . In some of your projects, you might need to match datums from multiple data repositories to process, join, or aggregate data. For example, you might need to process together multiple records that correspond to a certain user, experiment, or device. In these cases, you can create to pipelines that perform the following steps: More specifically, you need to create the following pipelines: Create a pipeline that groups all of the records for a specific key and index. Create another pipeline that takes that grouped output and performs the merging, joining, or other processing for the group. You can use these two data-combining pipelines for merging or grouped processing of data from various experiments, devices, and so on. You can also apply the same pattern to perform distributed joins of tabular data or data from database tables. For example, you can join user email records together with user IP records on the key and index of a user ID. You can parallelize each of the stages across workers to scale with the size of your data and the number of data sources that you want to merge. Tip If your data is not split into separate files for each record, you can split it automatically as described in Splitting Data for Distributed Processing .","title":"Combine, Merge, and Join Data"},{"location":"how-tos/combining/#group-matching-records","text":"The first pipeline that you create groups the records that need to be processed together. In this example, you have two repositories A and B with JSON records. These repositories might correspond to two experiments, two geographic regions, two different devices that generate data, or other. The following diagram displays the first pipeline: The repository A has the following structure: $ pachctl list file A@master NAME TYPE SIZE 1 .json file 39 B 2 .json file 39 B 3 .json file 39 B The repository B has the following structure: $ pachctl list file B@master NAME TYPE SIZE 1 .json file 39 B 2 .json file 39 B 3 .json file 39 B If you want to process A/1.json with B/1.json to merge their contents or otherwise process them together, you need to group each set of JSON records into respective datums that the pipelines that you create in Process grouped records can process together. The grouping pipeline takes a union of A and B as inputs, each with glob pattern /* . While the pipeline processes a JSON file, the data is copied to a folder in the output that corresponds to the key and index for that record. In this example, it is just the number in the file name. Pachyderm also renames the files to unique names that correspond to the source: /1 A.json B.json /2 A.json B.json /3 A.json B.json When you group your data, set the following parameters in the pipeline specification: In the pfs section, set \"empty_files\": true to avoid unnecessary downloads of data. Use symlinks to avoid unnecessary uploads of data and unnecessary data duplication.","title":"Group Matching Records"},{"location":"how-tos/combining/#process-grouped-records","text":"After you group the records together by using the grouping pipeline, you can use a merging pipeline on the group repository with a glob pattern of /* . By using the glob pattern of /* the pipeline can process each grouping of records in parallel. THe following diagram displays the second pipeline: The second pipeline performs merging, aggregation, or other processing on the respective grouping of records. It can also output each respective result to the root of the output directory: $ pachctl list file merge@master NAME TYPE SIZE result_1.json file 39 B result_2.json file 39 B result_3.json file 39 B","title":"Process Grouped Records"},{"location":"how-tos/create-ml-workflow/","text":"Create a Machine Learning Workflow \u00b6 Because Pachyderm is a language and framework agnostic and platform, and because it easily distributes analysis over large data sets, data scientists can use any tooling for creating machine learning workflows. Even if that tooling is not familiar to the rest of an engineering organization, data scientists can autonomously develop and deploy scalable solutions by using containers. Moreover, Pachyderm\u2019s pipeline logic paired with data versioning make any results reproducible for debugging purposes or during the development of improvements to a model. For maximum leverage of Pachyderm's built functionality, Pachyderm recommends that you combine model training processes, persisted models, and model utilization processes, such as making inferences or generating results, into a single Pachyderm pipeline Directed Acyclic Graph (DAG). Such a pipeline enables you to achieve the following goals: Keep a rigorous historical record of which models were used on what data to produce which results. Automatically update online ML models when training data or parameterization changes. Easily revert to other versions of an ML model when a new model does not produce an expected result or when bad data is introduced into a training data set. The following diagram demonstrates an ML pipeline: You can update the training dataset at any time to automatically train a new persisted model. Also, you can use any language or framework, including Apache Spark\u2122, Tensorflow\u2122, scikit-learn\u2122, or other, and output any format of persisted model, such as pickle, XML, POJO, or other. Regardless of the framework, Pachyderm versions the model so that you can track the data that was used to train each model. Pachyderm processes new data coming into the input repository with the updated model. Also, you can recompute old predictions with the updated model, or test new models on previously input and versioned data. This feature enables you to avoid manual updates to historical results or swapping ML models in production. For examples of ML workflows in Pachyderm see Machine Learning Examples .","title":"Create a Machine Learning Workflow"},{"location":"how-tos/create-ml-workflow/#create-a-machine-learning-workflow","text":"Because Pachyderm is a language and framework agnostic and platform, and because it easily distributes analysis over large data sets, data scientists can use any tooling for creating machine learning workflows. Even if that tooling is not familiar to the rest of an engineering organization, data scientists can autonomously develop and deploy scalable solutions by using containers. Moreover, Pachyderm\u2019s pipeline logic paired with data versioning make any results reproducible for debugging purposes or during the development of improvements to a model. For maximum leverage of Pachyderm's built functionality, Pachyderm recommends that you combine model training processes, persisted models, and model utilization processes, such as making inferences or generating results, into a single Pachyderm pipeline Directed Acyclic Graph (DAG). Such a pipeline enables you to achieve the following goals: Keep a rigorous historical record of which models were used on what data to produce which results. Automatically update online ML models when training data or parameterization changes. Easily revert to other versions of an ML model when a new model does not produce an expected result or when bad data is introduced into a training data set. The following diagram demonstrates an ML pipeline: You can update the training dataset at any time to automatically train a new persisted model. Also, you can use any language or framework, including Apache Spark\u2122, Tensorflow\u2122, scikit-learn\u2122, or other, and output any format of persisted model, such as pickle, XML, POJO, or other. Regardless of the framework, Pachyderm versions the model so that you can track the data that was used to train each model. Pachyderm processes new data coming into the input repository with the updated model. Also, you can recompute old predictions with the updated model, or test new models on previously input and versioned data. This feature enables you to avoid manual updates to historical results or swapping ML models in production. For examples of ML workflows in Pachyderm see Machine Learning Examples .","title":"Create a Machine Learning Workflow"},{"location":"how-tos/deferred_processing/","text":"Deferred Processing of Data \u00b6 While a Pachyderm pipeline is running, it processes any new data that you commit to its input branch. However, in some cases, you want to commit data more frequently than you want to process it. Because Pachyderm pipelines do not reprocess the data that has already been processed, in most cases, this is not an issue. But, some pipelines might need to process everything from scratch. For example, you might want to commit data every hour, but only want to retrain a machine learning model on that data daily because it needs to train on all the data from scratch. In these cases, you can leverage a massive performance benefit from deferred processing. This section covers how to achieve that and control what gets processed. Pachyderm controls what is being processed by using the filesystem , rather than at the pipeline level. Although pipelines are inflexible, they are simple and always try to process the data at the heads of their input branches. In contrast, the filesystem is very flexible and gives you the ability to commit data in different places and then efficiently move and rename the data so that it gets processed when you want. Configure a Staging Branch in an Input repository \u00b6 When you want to load data into Pachyderm without triggering a pipeline, you can upload it to a staging branch and then submit accumulated changes in one batch by re-pointing the HEAD of your master branch to a commit in the staging branch. Although, in this section, the branch in which you consolidate changes is called staging , you can name it as you like. Also, you can have multiple staging branches. For example, dev1 , dev2 , and so on. In the example below, the repository that is created called data . To configure a staging branch, complete the following steps: Create a repository. For example, data . $ pachctl create repo data Create a master branch. $ pachctl create branch data@master View the created branch: $ pachctl list branch data BRANCH HEAD master - No HEAD means that nothing has yet been committed into this branch. When you commit data to the master branch, the pipeline immediately starts a job to process it. However, if you want to commit something without immediately processing it, you need to commit it to a different branch. Commit a file to the staging branch: $ pachctl put file data@staging -f <file> Pachyderm automatically creates the staging branch. Your repo now has 2 branches, staging and master . In this example, the staging name is used, but you can name the branch as you want. Verify that the branches were created: $ pachctl list branch data BRANCH HEAD staging f3506f0fab6e483e8338754081109e69 master - The master branch still does not have a HEAD commit, but the new branch, staging , does. There still have been no jobs, because there are no pipelines that take staging as inputs. You can continue to commit to staging to add new data to the branch, and the pipeline will not process anything. When you are ready to process the data, update the master branch to point it to the head of the staging branch: $ pachctl create branch data@master --head staging List your branches to verify that the master branch has a HEAD commit: $ pachctl list branch staging f3506f0fab6e483e8338754081109e69 master f3506f0fab6e483e8338754081109e69 The master and staging branches now have the same HEAD commit. This means that your pipeline has data to process. Verify that the pipeline has new jobs: $ pachctl list job ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 061b0ef8f44f41bab5247420b4e62ca2 test 32 seconds ago Less than a second 0 6 + 0 / 6 108B 24B success You should see one job that Pachyderm created for all the changes you have submitted to the staging branch. While the commits to the staging branch are ancestors of the current HEAD in master , they were never the actual HEAD of master themselves, so they do not get processed. This behavior works for most of the use cases because commits in Pachyderm are generally additive, so processing the HEAD commit also processes data from previous commits. Process Specific Commits \u00b6 Sometimes you want to process specific intermediary commits that are not in the HEAD of the branch. To do this, you need to set master to have these commits as HEAD . For example, if you submitted ten commits in the staging branch and you want to process the seventh, third, and most recent commits, you need to run the following commands respectively: $ pachctl create branch data@master --head staging^7 $ pachctl create branch data@master --head staging^3 $ pachctl create branch data@master --head staging When you run the commands above, Pachyderm creates a job for each of the commands one after another. Therefore, when one job is completed, Pachyderm starts the next one. To verify that Pachyderm created jobs for these commands, run pachctl list job . Change the HEAD of your Branch \u00b6 You can move backward to previous commits as easily as advancing to the latest commits. For example, if you want to change the final output to be the result of processing staging^1 , you can roll back your HEAD commit by running the following command: $ pachctl create branch data@master --head staging^1 This command starts a new job to process staging^1 . The HEAD commit on your output repo will be the result of processing staging^1 instead of staging . Copy Files from One Branch to Another \u00b6 Using a staging branch allows you to defer processing. To use this functionality you need to know your input commits in advance. However, sometimes you want to be able to commit data in an ad-hoc, disorganized manner and then organize it later. Instead of pointing your master branch to a commit in a staging branch, you can copy individual files from staging to master . When you run copy file , Pachyderm only copies references to the files and does not move the actual data for the files around. To copy files from one branch to another, complete the following steps: Start a commit: $ pachctl start commit data@master Copy files: $ pachctl copy file data@staging:file1 data@master:file1 $ pachctl copy file data@staging:file2 data@master:file2 ... Close the commit: $ pachctl finish commit data@master Also, you can run pachctl delete file and pachctl put file while the commit is open if you want to remove something from the parent commit or add something that is not stored anywhere else. Deferred Processing in Output Repositories \u00b6 You can perform same deferred processing opertions with data in output repositories. To do so, rather than committing to a staging branch, configure the output_branch field in your pipeline specification. To configure deffered processing in an output repository, complete the following steps: In the pipeline specification, add the output_branch field with the name of the branch in which you want to accumulate your data before processing: \"output_branch\" : \"staging\" When you want to process data, run: $ pachctl create-branch pipeline master --head staging Automate Branch Switching \u00b6 Typically, repointing from one branch to another happens when a certain condition is met. For example, you might want to repoint your branch when you have a specific number of commits, or when the amount of unprocessed data reaches a certain size, or at a specific time interval, such as daily, or other. To configure this functionality, you need to create a Kubernetes application that uses Pachyderm APIs and watches the repositories for the specified condition. When the condition is met, the application switches the Pachyderm branch from staging to master .","title":"Deferred Processing of Data"},{"location":"how-tos/deferred_processing/#deferred-processing-of-data","text":"While a Pachyderm pipeline is running, it processes any new data that you commit to its input branch. However, in some cases, you want to commit data more frequently than you want to process it. Because Pachyderm pipelines do not reprocess the data that has already been processed, in most cases, this is not an issue. But, some pipelines might need to process everything from scratch. For example, you might want to commit data every hour, but only want to retrain a machine learning model on that data daily because it needs to train on all the data from scratch. In these cases, you can leverage a massive performance benefit from deferred processing. This section covers how to achieve that and control what gets processed. Pachyderm controls what is being processed by using the filesystem , rather than at the pipeline level. Although pipelines are inflexible, they are simple and always try to process the data at the heads of their input branches. In contrast, the filesystem is very flexible and gives you the ability to commit data in different places and then efficiently move and rename the data so that it gets processed when you want.","title":"Deferred Processing of Data"},{"location":"how-tos/deferred_processing/#configure-a-staging-branch-in-an-input-repository","text":"When you want to load data into Pachyderm without triggering a pipeline, you can upload it to a staging branch and then submit accumulated changes in one batch by re-pointing the HEAD of your master branch to a commit in the staging branch. Although, in this section, the branch in which you consolidate changes is called staging , you can name it as you like. Also, you can have multiple staging branches. For example, dev1 , dev2 , and so on. In the example below, the repository that is created called data . To configure a staging branch, complete the following steps: Create a repository. For example, data . $ pachctl create repo data Create a master branch. $ pachctl create branch data@master View the created branch: $ pachctl list branch data BRANCH HEAD master - No HEAD means that nothing has yet been committed into this branch. When you commit data to the master branch, the pipeline immediately starts a job to process it. However, if you want to commit something without immediately processing it, you need to commit it to a different branch. Commit a file to the staging branch: $ pachctl put file data@staging -f <file> Pachyderm automatically creates the staging branch. Your repo now has 2 branches, staging and master . In this example, the staging name is used, but you can name the branch as you want. Verify that the branches were created: $ pachctl list branch data BRANCH HEAD staging f3506f0fab6e483e8338754081109e69 master - The master branch still does not have a HEAD commit, but the new branch, staging , does. There still have been no jobs, because there are no pipelines that take staging as inputs. You can continue to commit to staging to add new data to the branch, and the pipeline will not process anything. When you are ready to process the data, update the master branch to point it to the head of the staging branch: $ pachctl create branch data@master --head staging List your branches to verify that the master branch has a HEAD commit: $ pachctl list branch staging f3506f0fab6e483e8338754081109e69 master f3506f0fab6e483e8338754081109e69 The master and staging branches now have the same HEAD commit. This means that your pipeline has data to process. Verify that the pipeline has new jobs: $ pachctl list job ID PIPELINE STARTED DURATION RESTART PROGRESS DL UL STATE 061b0ef8f44f41bab5247420b4e62ca2 test 32 seconds ago Less than a second 0 6 + 0 / 6 108B 24B success You should see one job that Pachyderm created for all the changes you have submitted to the staging branch. While the commits to the staging branch are ancestors of the current HEAD in master , they were never the actual HEAD of master themselves, so they do not get processed. This behavior works for most of the use cases because commits in Pachyderm are generally additive, so processing the HEAD commit also processes data from previous commits.","title":"Configure a Staging Branch in an Input repository"},{"location":"how-tos/deferred_processing/#process-specific-commits","text":"Sometimes you want to process specific intermediary commits that are not in the HEAD of the branch. To do this, you need to set master to have these commits as HEAD . For example, if you submitted ten commits in the staging branch and you want to process the seventh, third, and most recent commits, you need to run the following commands respectively: $ pachctl create branch data@master --head staging^7 $ pachctl create branch data@master --head staging^3 $ pachctl create branch data@master --head staging When you run the commands above, Pachyderm creates a job for each of the commands one after another. Therefore, when one job is completed, Pachyderm starts the next one. To verify that Pachyderm created jobs for these commands, run pachctl list job .","title":"Process Specific Commits"},{"location":"how-tos/deferred_processing/#change-the-head-of-your-branch","text":"You can move backward to previous commits as easily as advancing to the latest commits. For example, if you want to change the final output to be the result of processing staging^1 , you can roll back your HEAD commit by running the following command: $ pachctl create branch data@master --head staging^1 This command starts a new job to process staging^1 . The HEAD commit on your output repo will be the result of processing staging^1 instead of staging .","title":"Change the HEAD of your Branch"},{"location":"how-tos/deferred_processing/#copy-files-from-one-branch-to-another","text":"Using a staging branch allows you to defer processing. To use this functionality you need to know your input commits in advance. However, sometimes you want to be able to commit data in an ad-hoc, disorganized manner and then organize it later. Instead of pointing your master branch to a commit in a staging branch, you can copy individual files from staging to master . When you run copy file , Pachyderm only copies references to the files and does not move the actual data for the files around. To copy files from one branch to another, complete the following steps: Start a commit: $ pachctl start commit data@master Copy files: $ pachctl copy file data@staging:file1 data@master:file1 $ pachctl copy file data@staging:file2 data@master:file2 ... Close the commit: $ pachctl finish commit data@master Also, you can run pachctl delete file and pachctl put file while the commit is open if you want to remove something from the parent commit or add something that is not stored anywhere else.","title":"Copy Files from One Branch to Another"},{"location":"how-tos/deferred_processing/#deferred-processing-in-output-repositories","text":"You can perform same deferred processing opertions with data in output repositories. To do so, rather than committing to a staging branch, configure the output_branch field in your pipeline specification. To configure deffered processing in an output repository, complete the following steps: In the pipeline specification, add the output_branch field with the name of the branch in which you want to accumulate your data before processing: \"output_branch\" : \"staging\" When you want to process data, run: $ pachctl create-branch pipeline master --head staging","title":"Deferred Processing in Output Repositories"},{"location":"how-tos/deferred_processing/#automate-branch-switching","text":"Typically, repointing from one branch to another happens when a certain condition is met. For example, you might want to repoint your branch when you have a specific number of commits, or when the amount of unprocessed data reaches a certain size, or at a specific time interval, such as daily, or other. To configure this functionality, you need to create a Kubernetes application that uses Pachyderm APIs and watches the repositories for the specified condition. When the condition is met, the application switches the Pachyderm branch from staging to master .","title":"Automate Branch Switching"},{"location":"how-tos/distributed_computing/","text":"Configure Distributed Computing \u00b6 Distributing your computations across multiple workers is a fundamental part of any big data processing. When you build production-scale pipelines, you need to adjust the number of workers and resources that are allocated to each job to optimize throughput. A Pachyderm worker is an identical Kubernetes pod that runs the Docker image that you specified in the pipeline spec . Your analysis code does not affect how Pachyderm distributes the workload among workers. Instead, Pachyderm spreads out the data that needs to be processed across the various workers and makes that data available for your code. When you create a pipeline, Pachyderm spins up worker pods that continuously run in the cluster waiting for new data to be available for processing. You can change this behavior by setting \"standby\" :true . Therefore, you do not need to recreate and schedule workers for every new job. For each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all the datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance. The following animation shows how distributed computing works: In the diagram above, you have three Pachyderm worker pods that process your data. When a pod finishes processing a datum, it automatically takes another datum from the queue to process it. Datums might be different in size and, therefore, some of them might be processed faster than others. Each datum goes through the following processing phases inside a Pachyderm worker pod: Phase Description Downloading The Pachyderm worker pod downloads the datum contents into Pachyderm. Processing The Pachyderm worker pod runs the contents of the datum against your code. Uploading The Pachyderm worker pod uploads the results of processing into an output repository. When a datum completes a phase, the Pachyderm worker moves it to the next one while another datum from the queue takes its place in the processing sequence. The following animation displays what happens inside a pod during the datum processing: You can control the number of worker pods that Pachyderm runs in a pipeline by defining the parallelism parameter in the pipeline specification . Example \"parallelism_spec\" : { // Exactly one of these two fields should be set \"constant\" : int \"coefficient\" : double Pachyderm has the following parallelism strategies that you can set in the pipeline spec: Strategy Description constant Pachyderm starts the specified number of workers. For example, if you set \"constant\":10 , Pachyderm spreads the computation workload among ten workers. coefficient Pachyderm starts a number of workers that is a multiple of your Kubernetes cluster size. For example, if your Kubernetes cluster has ten nodes, and you set \"coefficient\": 0.5 , Pachyderm starts five workers. If you set parallelism to \"coefficient\": 2.0 , Pachyderm starts twenty workers. By default, Pachyderm sets parallelism to \u201cconstant\": 1 , which means that it spawns one worker per Kubernetes node for this pipeline. See also: Glob Pattern Pipeline Specification","title":"Configure Distributed Computing"},{"location":"how-tos/distributed_computing/#configure-distributed-computing","text":"Distributing your computations across multiple workers is a fundamental part of any big data processing. When you build production-scale pipelines, you need to adjust the number of workers and resources that are allocated to each job to optimize throughput. A Pachyderm worker is an identical Kubernetes pod that runs the Docker image that you specified in the pipeline spec . Your analysis code does not affect how Pachyderm distributes the workload among workers. Instead, Pachyderm spreads out the data that needs to be processed across the various workers and makes that data available for your code. When you create a pipeline, Pachyderm spins up worker pods that continuously run in the cluster waiting for new data to be available for processing. You can change this behavior by setting \"standby\" :true . Therefore, you do not need to recreate and schedule workers for every new job. For each job, all the datums are queued up and then distributed across the available workers. When a worker finishes processing its datum, it grabs a new datum from the queue until all the datums complete processing. If a worker pod crashes, its datums are redistributed to other workers for maximum fault tolerance. The following animation shows how distributed computing works: In the diagram above, you have three Pachyderm worker pods that process your data. When a pod finishes processing a datum, it automatically takes another datum from the queue to process it. Datums might be different in size and, therefore, some of them might be processed faster than others. Each datum goes through the following processing phases inside a Pachyderm worker pod: Phase Description Downloading The Pachyderm worker pod downloads the datum contents into Pachyderm. Processing The Pachyderm worker pod runs the contents of the datum against your code. Uploading The Pachyderm worker pod uploads the results of processing into an output repository. When a datum completes a phase, the Pachyderm worker moves it to the next one while another datum from the queue takes its place in the processing sequence. The following animation displays what happens inside a pod during the datum processing: You can control the number of worker pods that Pachyderm runs in a pipeline by defining the parallelism parameter in the pipeline specification . Example \"parallelism_spec\" : { // Exactly one of these two fields should be set \"constant\" : int \"coefficient\" : double Pachyderm has the following parallelism strategies that you can set in the pipeline spec: Strategy Description constant Pachyderm starts the specified number of workers. For example, if you set \"constant\":10 , Pachyderm spreads the computation workload among ten workers. coefficient Pachyderm starts a number of workers that is a multiple of your Kubernetes cluster size. For example, if your Kubernetes cluster has ten nodes, and you set \"coefficient\": 0.5 , Pachyderm starts five workers. If you set parallelism to \"coefficient\": 2.0 , Pachyderm starts twenty workers. By default, Pachyderm sets parallelism to \u201cconstant\": 1 , which means that it spawns one worker per Kubernetes node for this pipeline. See also: Glob Pattern Pipeline Specification","title":"Configure Distributed Computing"},{"location":"how-tos/export-data-out-pachyderm/","text":"Export Your Data From Pachyderm \u00b6 After you build a pipeline, you probably want to see the results that the pipeline has produced. Every commit into an input repository results in a corresponding commit into an output repository. To access the results of a pipeline, you can use one of the following methods: By running the pachctl get file command. This command returns the contents of the specified file. To get the list of files in a repo, you should first run the pachctl list file command. See Export Your Data with pachctl . By configuring the pipeline. A pipeline can push or expose output data to external sources. You can configure the following data exporting methods in a Pachyderm pipeline: An egress property enables you to export your data to an external datastore, such as Amazon S3, Google Cloud Storage, and others. See Export data by using egress . A service. A Pachyderm service exposes the results of the pipeline processing on a specific port in the form of a dashboard or similar endpoint. See Service . Configure your code to connect to an external data source. Because a pipeline is a Docker container that runs your code, you can egress your data to any data source, even to those that the egress field does not support, by connecting to that source from within your code. By using the S3 gateway. Pachyderm Enterprise users can reuse their existing tools and libraries that work with object store to export their data with the S3 gateway. See Using the S3 Gateway . Export Your Data with pachctl \u00b6 The pachctl get file command enables you to get the contents of a file in a Pachyderm repository. You need to know the file path to specify it in the command. To export your data with pachctl: Get the list of files in the repository: $ pachctl list file <repo>@<branch> Example: $ pachctl list commit data@master REPO BRANCH COMMIT PARENT STARTED DURATION SIZE data master 230103d3c6bd45b483ab6d0b7ae858d5 f82b76f463ca4799817717a49ab74fac 2 seconds ago Less than a second 750B data master f82b76f463ca4799817717a49ab74fac <none> 40 seconds ago Less than a second 375B Get the contents of a specific file: pachctl get file <repo>@<branch>:<path/to/file> Example: $ pachctl get file data@master:user_data.csv 1 ,cyukhtin0@stumbleupon.com,144.155.176.12 2 ,csisneros1@over-blog.com,26.119.26.5 3 ,jeye2@instagram.com,13.165.230.106 4 ,rnollet3@hexun.com,58.52.147.83 5 ,bposkitt4@irs.gov,51.247.120.167 6 ,vvenmore5@hubpages.com,161.189.245.212 7 ,lcoyte6@ask.com,56.13.147.134 8 ,atuke7@psu.edu,78.178.247.163 9 ,nmorrell8@howstuffworks.com,28.172.10.170 10 ,afynn9@google.com.au,166.14.112.65 Also, you can view the parent, grandparent, and any previous revision by using the caret ( ^ ) symbol with a number that corresponds to an ancestor in sequence: To view a parent of a commit: List files in the parent commit: $ pachctl list commit <repo>@<branch-or-commit>^:<path/to/file> Get the contents of a file: $ pachctl get file <repo>@<branch-or-commit>^:<path/to/file> To view an <n> parent of a commit: List files in the parent commit: $ pachctl list commit <repo>@<branch-or-commit>^<n>:<path/to/file> Example: NAME TYPE SIZE /user_data.csv file 375B Get the contents of a file: $ pachctl get file <repo>@<branch-or-commit>^<n>:<path/to/file> Example: $ pachctl get file datas@master^4:user_data.csv You can specify any number in the ^<n> notation. If the file exists in that commit, Pachyderm returns it. If the file does not exist in that revision, Pachyderm displays the following message: $ pachctl get file <repo>@<branch-or-commit>^<n>:<path/to/file> file \"<path/to/file>\" not found Export Your Data with egress \u00b6 The egress field in the Pachyderm pipeline specification enables you to push the results of a pipeline to an external datastore such as Amazon S3, Google Cloud Storage, or Azure Blob Storage. After the user code has finished running, but before the job is marked as successful, Pachyderm pushes the data to the specified destination. You can specify the following egress protocols for the corresponding storage: Cloud Platform Protocol Description Google Cloud Storage gs:// GCP uses the utility called gsutil to access GCP storage resources from a CLI. This utility uses the gs:// prefix to access these resources. Example: gs://gs-bucket/gs-dir Amazon S3 s3:// The Amazon S3 storage protocol requires you to specify an s3:// prefix before the address of an Amazon resource. A valid address must include an endpoint and a bucket, and, optionally, a directory in your Amazon storage. Example: s3://s3-endpoint/s3-bucket/s3-dir Azure Blob Storage wasb:// Microsoft Windows Azure Storage Blob (WASB) is the default Azure filesystem that outputs your data through HDInsight . To output your data to Azure Blob Storage, use the wasb:// prefix, the container name, and your storage account in the path to your directory. Example: wasb://default-container@storage-account/az-dir Example \"egress\" : { \"URL\" : \"s3://bucket/dir\" } ,","title":"Export Your Data From Pachyderm"},{"location":"how-tos/export-data-out-pachyderm/#export-your-data-from-pachyderm","text":"After you build a pipeline, you probably want to see the results that the pipeline has produced. Every commit into an input repository results in a corresponding commit into an output repository. To access the results of a pipeline, you can use one of the following methods: By running the pachctl get file command. This command returns the contents of the specified file. To get the list of files in a repo, you should first run the pachctl list file command. See Export Your Data with pachctl . By configuring the pipeline. A pipeline can push or expose output data to external sources. You can configure the following data exporting methods in a Pachyderm pipeline: An egress property enables you to export your data to an external datastore, such as Amazon S3, Google Cloud Storage, and others. See Export data by using egress . A service. A Pachyderm service exposes the results of the pipeline processing on a specific port in the form of a dashboard or similar endpoint. See Service . Configure your code to connect to an external data source. Because a pipeline is a Docker container that runs your code, you can egress your data to any data source, even to those that the egress field does not support, by connecting to that source from within your code. By using the S3 gateway. Pachyderm Enterprise users can reuse their existing tools and libraries that work with object store to export their data with the S3 gateway. See Using the S3 Gateway .","title":"Export Your Data From Pachyderm"},{"location":"how-tos/export-data-out-pachyderm/#export-your-data-with-pachctl","text":"The pachctl get file command enables you to get the contents of a file in a Pachyderm repository. You need to know the file path to specify it in the command. To export your data with pachctl: Get the list of files in the repository: $ pachctl list file <repo>@<branch> Example: $ pachctl list commit data@master REPO BRANCH COMMIT PARENT STARTED DURATION SIZE data master 230103d3c6bd45b483ab6d0b7ae858d5 f82b76f463ca4799817717a49ab74fac 2 seconds ago Less than a second 750B data master f82b76f463ca4799817717a49ab74fac <none> 40 seconds ago Less than a second 375B Get the contents of a specific file: pachctl get file <repo>@<branch>:<path/to/file> Example: $ pachctl get file data@master:user_data.csv 1 ,cyukhtin0@stumbleupon.com,144.155.176.12 2 ,csisneros1@over-blog.com,26.119.26.5 3 ,jeye2@instagram.com,13.165.230.106 4 ,rnollet3@hexun.com,58.52.147.83 5 ,bposkitt4@irs.gov,51.247.120.167 6 ,vvenmore5@hubpages.com,161.189.245.212 7 ,lcoyte6@ask.com,56.13.147.134 8 ,atuke7@psu.edu,78.178.247.163 9 ,nmorrell8@howstuffworks.com,28.172.10.170 10 ,afynn9@google.com.au,166.14.112.65 Also, you can view the parent, grandparent, and any previous revision by using the caret ( ^ ) symbol with a number that corresponds to an ancestor in sequence: To view a parent of a commit: List files in the parent commit: $ pachctl list commit <repo>@<branch-or-commit>^:<path/to/file> Get the contents of a file: $ pachctl get file <repo>@<branch-or-commit>^:<path/to/file> To view an <n> parent of a commit: List files in the parent commit: $ pachctl list commit <repo>@<branch-or-commit>^<n>:<path/to/file> Example: NAME TYPE SIZE /user_data.csv file 375B Get the contents of a file: $ pachctl get file <repo>@<branch-or-commit>^<n>:<path/to/file> Example: $ pachctl get file datas@master^4:user_data.csv You can specify any number in the ^<n> notation. If the file exists in that commit, Pachyderm returns it. If the file does not exist in that revision, Pachyderm displays the following message: $ pachctl get file <repo>@<branch-or-commit>^<n>:<path/to/file> file \"<path/to/file>\" not found","title":"Export Your Data with pachctl"},{"location":"how-tos/export-data-out-pachyderm/#export-your-data-with-egress","text":"The egress field in the Pachyderm pipeline specification enables you to push the results of a pipeline to an external datastore such as Amazon S3, Google Cloud Storage, or Azure Blob Storage. After the user code has finished running, but before the job is marked as successful, Pachyderm pushes the data to the specified destination. You can specify the following egress protocols for the corresponding storage: Cloud Platform Protocol Description Google Cloud Storage gs:// GCP uses the utility called gsutil to access GCP storage resources from a CLI. This utility uses the gs:// prefix to access these resources. Example: gs://gs-bucket/gs-dir Amazon S3 s3:// The Amazon S3 storage protocol requires you to specify an s3:// prefix before the address of an Amazon resource. A valid address must include an endpoint and a bucket, and, optionally, a directory in your Amazon storage. Example: s3://s3-endpoint/s3-bucket/s3-dir Azure Blob Storage wasb:// Microsoft Windows Azure Storage Blob (WASB) is the default Azure filesystem that outputs your data through HDInsight . To output your data to Azure Blob Storage, use the wasb:// prefix, the container name, and your storage account in the path to your directory. Example: wasb://default-container@storage-account/az-dir Example \"egress\" : { \"URL\" : \"s3://bucket/dir\" } ,","title":"Export Your Data with egress"},{"location":"how-tos/individual-developer-workflow/","text":"Individual Developer Workflow \u00b6 A typical Pachyderm workflow involves multiple iterations of experimenting with your code and pipeline specs. Info Before you read this section, make sure that you understand basic Pachyderm pipeline concepts described in Concepts . How it works \u00b6 Working with Pachyderm includes multiple iterations of the following steps: Step 1: Write Your Analysis Code \u00b6 Because Pachyderm is completely language-agnostic, the code that is used to process data in Pachyderm can be written in any language and can use any libraries of choice. Whether your code is as simple as a bash command or as complicated as a TensorFlow neural network, it needs to be built with all the required dependencies into a container that can run anywhere, including inside of Pachyderm. See Examples . Your code does not have to import any special Pachyderm functionality or libraries. However, it must meet the following requirements: Read files from a local file system . Pachyderm automatically mounts each input data repository as /pfs/<repo_name> in the running containers of your Docker image. Therefore, the code that you write needs to read input data from this directory, similar to any other file system. Because Pachyderm automatically spreads data across parallel containers, your analysis code does not have to deal with data sharding or parallelization. For example, if you have four containers that run your Python code, Pachyderm automatically supplies \u00bc of the input data to /pfs/<repo_name> in each running container. These workload balancing settings can be adjusted as needed through Pachyderm tunable parameters in the pipeline specification. Write files into a local file system , such as saving results. Your code must write to the /pfs/out directory that Pachyderm mounts in all of your running containers. Similar to reading data, your code does not have to manage parallelization or sharding. Step 2: Build Your Docker Image \u00b6 When you create a Pachyderm pipeline, you need to specify a Docker image that includes the code or binary that you want to run. Therefore, every time you modify your code, you need to build a new Docker image, push it to your image registry, and update the image tag in the pipeline spec. This section describes one way of building Docker images, but if you have your own routine, feel free to apply it. To build an image, you need to create a Dockerfile . However, do not use the CMD field in your Dockerfile to specify the commands that you want to run. Instead, you add them in the cmd field in your pipeline specification. Pachyderm runs these commands inside the container during the job execution rather than relying on Docker to run them. The reason is that Pachyderm cannot execute your code immediately when your container starts, so it runs a shim process in your container instead, and then, it calls your pipeline specification's cmd from there. After building your image, you need to upload the image into a public or private image registry, such as DockerHub or other. Alternatively, you can use the Pachyderm's built-in functionality to tag, build, and push images by running the pachctl update pipeline command with the --build and --push-images flags. For more information, see Update a pipelines . Note The Dockerfile example below is provided for your reference only. Your Dockerfile might look completely different. To build a Docker image, complete the following steps: If you do not have a registry, create one with a preferred provider. If you decide to use DockerHub, follow the Docker Hub Quickstart to create a repository for your project. Create a Dockerfile for your project. See the OpenCV example . Log in to an image registry. If you use DockerHub, run: docker login --username = <dockerhub-username> --password = <dockerhub-password> <dockerhub-fqdn> Build a new image from the Dockerfile by specifying a tag: docker build -t <IMAGE>:<TAG> . Push your image to your image registry. If you use DockerHub, run: docker push <image>:tag For more information about building Docker images, see Docker documentation . Step 3: Create a Pipeline \u00b6 Pachyderm's pipeline specifications store the configuration information about the Docker image and code that Pachyderm should run. Pipeline specifications are stored in JSON format. As soon as you create a pipeline, Pachyderm immediately spins a pod or pods on a Kubernetes worker node in which pipeline code runs. By default, after the pipeline finishes running, the pods continue to run while waiting for the new data to be committed into the Pachyderm input repository. You can configure this parameter, as well as many others, in the pipeline specification. A minimum pipeline specification must include the following parameters: name transform parallelism input You can store your pipeline locally or in a remote location, such as a GitHub repository. To create a Pipeline, complete the following steps: Create a pipeline specification. Here is an example of a pipeline spec: # my-pipeline.json { \"pipeline\" : { \"name\" : \"my-pipeline\" } , \"transform\" : { \"image\" : \"my-pipeline-image\" , \"cmd\" : [ \"/binary\" , \"/pfs/data\" , \"/pfs/out\" ] } , \"input\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } } } Create a Pachyderm pipeline from the spec: $ pachctl create pipeline -f my-pipeline.json You can specify a local file or a file stored in a remote location, such as a GitHub repository. For example, https://raw.githubusercontent.com/pachyderm/pachyderm/master/examples/opencv/edges.json . See also: Pipeline Specification","title":"Individual Developer Workflow"},{"location":"how-tos/individual-developer-workflow/#individual-developer-workflow","text":"A typical Pachyderm workflow involves multiple iterations of experimenting with your code and pipeline specs. Info Before you read this section, make sure that you understand basic Pachyderm pipeline concepts described in Concepts .","title":"Individual Developer Workflow"},{"location":"how-tos/individual-developer-workflow/#how-it-works","text":"Working with Pachyderm includes multiple iterations of the following steps:","title":"How it works"},{"location":"how-tos/individual-developer-workflow/#step-1-write-your-analysis-code","text":"Because Pachyderm is completely language-agnostic, the code that is used to process data in Pachyderm can be written in any language and can use any libraries of choice. Whether your code is as simple as a bash command or as complicated as a TensorFlow neural network, it needs to be built with all the required dependencies into a container that can run anywhere, including inside of Pachyderm. See Examples . Your code does not have to import any special Pachyderm functionality or libraries. However, it must meet the following requirements: Read files from a local file system . Pachyderm automatically mounts each input data repository as /pfs/<repo_name> in the running containers of your Docker image. Therefore, the code that you write needs to read input data from this directory, similar to any other file system. Because Pachyderm automatically spreads data across parallel containers, your analysis code does not have to deal with data sharding or parallelization. For example, if you have four containers that run your Python code, Pachyderm automatically supplies \u00bc of the input data to /pfs/<repo_name> in each running container. These workload balancing settings can be adjusted as needed through Pachyderm tunable parameters in the pipeline specification. Write files into a local file system , such as saving results. Your code must write to the /pfs/out directory that Pachyderm mounts in all of your running containers. Similar to reading data, your code does not have to manage parallelization or sharding.","title":"Step 1: Write Your Analysis Code"},{"location":"how-tos/individual-developer-workflow/#step-2-build-your-docker-image","text":"When you create a Pachyderm pipeline, you need to specify a Docker image that includes the code or binary that you want to run. Therefore, every time you modify your code, you need to build a new Docker image, push it to your image registry, and update the image tag in the pipeline spec. This section describes one way of building Docker images, but if you have your own routine, feel free to apply it. To build an image, you need to create a Dockerfile . However, do not use the CMD field in your Dockerfile to specify the commands that you want to run. Instead, you add them in the cmd field in your pipeline specification. Pachyderm runs these commands inside the container during the job execution rather than relying on Docker to run them. The reason is that Pachyderm cannot execute your code immediately when your container starts, so it runs a shim process in your container instead, and then, it calls your pipeline specification's cmd from there. After building your image, you need to upload the image into a public or private image registry, such as DockerHub or other. Alternatively, you can use the Pachyderm's built-in functionality to tag, build, and push images by running the pachctl update pipeline command with the --build and --push-images flags. For more information, see Update a pipelines . Note The Dockerfile example below is provided for your reference only. Your Dockerfile might look completely different. To build a Docker image, complete the following steps: If you do not have a registry, create one with a preferred provider. If you decide to use DockerHub, follow the Docker Hub Quickstart to create a repository for your project. Create a Dockerfile for your project. See the OpenCV example . Log in to an image registry. If you use DockerHub, run: docker login --username = <dockerhub-username> --password = <dockerhub-password> <dockerhub-fqdn> Build a new image from the Dockerfile by specifying a tag: docker build -t <IMAGE>:<TAG> . Push your image to your image registry. If you use DockerHub, run: docker push <image>:tag For more information about building Docker images, see Docker documentation .","title":"Step 2: Build Your Docker Image"},{"location":"how-tos/individual-developer-workflow/#step-3-create-a-pipeline","text":"Pachyderm's pipeline specifications store the configuration information about the Docker image and code that Pachyderm should run. Pipeline specifications are stored in JSON format. As soon as you create a pipeline, Pachyderm immediately spins a pod or pods on a Kubernetes worker node in which pipeline code runs. By default, after the pipeline finishes running, the pods continue to run while waiting for the new data to be committed into the Pachyderm input repository. You can configure this parameter, as well as many others, in the pipeline specification. A minimum pipeline specification must include the following parameters: name transform parallelism input You can store your pipeline locally or in a remote location, such as a GitHub repository. To create a Pipeline, complete the following steps: Create a pipeline specification. Here is an example of a pipeline spec: # my-pipeline.json { \"pipeline\" : { \"name\" : \"my-pipeline\" } , \"transform\" : { \"image\" : \"my-pipeline-image\" , \"cmd\" : [ \"/binary\" , \"/pfs/data\" , \"/pfs/out\" ] } , \"input\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } } } Create a Pachyderm pipeline from the spec: $ pachctl create pipeline -f my-pipeline.json You can specify a local file or a file stored in a remote location, such as a GitHub repository. For example, https://raw.githubusercontent.com/pachyderm/pachyderm/master/examples/opencv/edges.json . See also: Pipeline Specification","title":"Step 3: Create a Pipeline"},{"location":"how-tos/ingressing_from_diff_cloud/","text":"Ingress and Egress Data from an External Object Store \u00b6 Occasionally, you might need to download data from or upload data to an object store that runs in a different cloud platform. For example, you might be running a Pachyderm cluster in Microsoft Azure, but you need to ingress files from an S3 bucket that resides on Amazon AWS. You can configure Pachyderm to work with an external object store by using the following methods: Ingress data from an external object store by using the pachtl put file with a URL to the S3 bucket. Example: $ pachctl put file repo@branch -f <s3://my_bucket/file> Egress data to an external object store by configuring the egress files in the pipeline specification. Example: # pipeline.json \"egress\" : { \"URL\" : \"s3://bucket/dir\" Configure Credentials \u00b6 You can configure Pachyderm to ingress and egress from and to any number of supported cloud object stores, including Amazon S3, Microsoft Azure Blob storage, and Google Cloud Storage. You need to provide Pachyderm with the credentials to communicate with the selected cloud provider. The credentials are stored in a Kubernetes secret and share the same security properties. Note For each cloud provider, parameters and configuration steps might vary. To provide Pachyderm with the object store credentials, complete the following steps: Deploy object storage: $ pachctl deploy storage <storage-provider> ... In the command above, specify aws , google , or azure as a storage provider. Depending on the storage provider, configure the required parameters. Run pachctl deploy storage <backend> --help for more information. For example, if you select aws , you need to specify the following parameters: $ pachctl deploy storage aws <region> <bucket-name> <access key id> <secret access key> See also: Custom Object Store Create a Custom Pachyderm Deployment Pipeline Specification","title":"Ingress and Egress Data from an External Object Store"},{"location":"how-tos/ingressing_from_diff_cloud/#ingress-and-egress-data-from-an-external-object-store","text":"Occasionally, you might need to download data from or upload data to an object store that runs in a different cloud platform. For example, you might be running a Pachyderm cluster in Microsoft Azure, but you need to ingress files from an S3 bucket that resides on Amazon AWS. You can configure Pachyderm to work with an external object store by using the following methods: Ingress data from an external object store by using the pachtl put file with a URL to the S3 bucket. Example: $ pachctl put file repo@branch -f <s3://my_bucket/file> Egress data to an external object store by configuring the egress files in the pipeline specification. Example: # pipeline.json \"egress\" : { \"URL\" : \"s3://bucket/dir\"","title":"Ingress and Egress Data from an External Object Store"},{"location":"how-tos/ingressing_from_diff_cloud/#configure-credentials","text":"You can configure Pachyderm to ingress and egress from and to any number of supported cloud object stores, including Amazon S3, Microsoft Azure Blob storage, and Google Cloud Storage. You need to provide Pachyderm with the credentials to communicate with the selected cloud provider. The credentials are stored in a Kubernetes secret and share the same security properties. Note For each cloud provider, parameters and configuration steps might vary. To provide Pachyderm with the object store credentials, complete the following steps: Deploy object storage: $ pachctl deploy storage <storage-provider> ... In the command above, specify aws , google , or azure as a storage provider. Depending on the storage provider, configure the required parameters. Run pachctl deploy storage <backend> --help for more information. For example, if you select aws , you need to specify the following parameters: $ pachctl deploy storage aws <region> <bucket-name> <access key id> <secret access key> See also: Custom Object Store Create a Custom Pachyderm Deployment Pipeline Specification","title":"Configure Credentials"},{"location":"how-tos/load-data-into-pachyderm/","text":"Load Your Data Into Pachyderm \u00b6 Info Before you read this section, make sure that you are familiar with the Data Concepts and Pipeline Concepts . The data that you commit to Pachyderm is stored in an object store of your choice, such as Amazon S3, MinIO, Google Cloud Storage, or other. Pachyderm records the cryptographic hash ( SHA ) of each portion of your data and stores it as a commit with a unique identifier (ID). Although the data is stored as an unstructured blob, Pachyderm enables you to interact with versioned data as you typically do in a standard file system. Pachyderm stores versioned data in repositories which can contain one or multiple files, as well as files arranged in directories. Regardless of the repository structure, Pachyderm versions the state of each data repository as the data changes over time. To put data into Pachyderm, a commit must be started , or opened . Data can then be put into Pachyderm as part of that open commit and is available once the commit is finished or closed . Pachyderm provides the following options to load data: By using the pachctl put file command. This option is great for testing, development, integration with CI/CD, and for users who prefer scripting. See Load Your Data by Using pachctl . By creating a pipeline to pull data from an outside source. Because Pachyderm pipelines can be any arbitrary code that runs in a Docker container, you can call out to external APIs or data sources and pull in data from there. Your pipeline code can be triggered on-demand or continuously with the following special types of pipelines: Spout: A spout enables you to continuously load streaming data from a streaming data source, such as a messaging system or message queue into Pachyderm. See Spout . Cron: A cron triggers your pipeline periodically based on the interval that you configure in your pipeline spec. See Cron . Note: Pipelines enable you to do much more than just ingressing data into Pachyderm. Pipelines can run all kinds of data transformations on your input data sources, such as a Pachyderm repository, and be configured to run your code automatically as new data is committed. For more information, see Pipeline . By using a Pachyderm language client. This option is ideal for Go or Python users who want to push data into Pachyderm from services or applications written in those languages. If you did not find your favorite language in the list of supported language clients, Pachyderm uses a protobuf API which supports many other languages. See Pachyderm Language Clients . If you are using the Pachyderm Enterprise version, you can use these additional options: By using the S3 gateway. This option is great to use with the existing tools and libraries that interact with S3-compatible object stores. See Using the S3 Gateway . By using the Pachyderm dashboard. The Pachyderm Enterprise dashboard provides a convenient way to upload data right from the UI. Note In the Pachyderm UI, you can only specify an S3 data source. Uploading data from your local device is not supported. Load Your Data by Using pachctl \u00b6 The pachctl put file command enables you to do everything from loading local files into Pachyderm to pulling data from an existing object store bucket and extracting data from a website. With pachctl put file , you can append new data to the existing data or overwrite the existing data. All these options can be configured by using the flags available with this command. Run pachctl put file --help to view the complete list of flags that you can specify. To load your data into Pachyderm by using pachctl , you first need to create one or more data repositories. Then, you can use the pachctl put file command to put your data into the created repository. In Pachyderm, you can start and finish commits. If you just run pachctl put file and no open commit exists, Pachyderm starts a new commit, adds the data to which you specified the path in your command, and finishes the commit. This is called an atomic commit. Alternatively, you can run pachctl start commit to start a new commit. Then, add your data in multiple put file calls, and finally, when ready, close the commit by running pachctl finish commit . To load your data into a repository, complete the following steps: Create a Pachyderm repository: $ pachctl create repo <repo name> Select from the following options: To start and finish an atomic commit, run: $ pachctl put file <repo>@<branch>:</path/to/file1> -f <file1> To start a commit and add data in iterations: Start a commit: $ pachctl start commit <repo>@<branch> 1. Put your data: $ pachctl put file <repo>@<branch>:</path/to/file1> -f <file1> Work on your changes, and when ready, put more data: $ pachctl put file <repo>@<branch>:</path/to/file2> -f <file2> Close the commit: $ pachctl finish commit <repo>@<branch> Filepath Format \u00b6 In Pachyderm, you specify the path to file by using the -f option. A path to file can be a local path or a URL to an external resource. You can add multiple files or directories by using the -i option. To add contents of a directory, use the -r flag. The following table provides examples of pachctl put file commands with various filepaths and data sources: Put data from a URL: $ pachctl put file <repo>@<branch>:</path/to/file> -f http://url_path Put data from an object store. You can use s3:// , gcs:// , or as:// in your filepath: $ pachctl put file <repo>@<branch>:</path/to/file> -f s3://object_store_url Note If you are configuring a local cluster to access an S3 bucket, you need to first deploy a Kubernetes Secret for the selected object store. Add multiple files at once by using the -i option or multiple -f flags. In the case of -i , the target file must be a list of files, paths, or URLs that you want to input all at once: $ pachctl put file <repo>@<branch> -i <file containing list of files, paths, or URLs> Input data from stdin into a data repository by using a pipe: $ echo \"data\" | pachctl put file <repo>@<branch> -f </path/to/file> Add an entire directory or all of the contents at a particular URL, either HTTP(S) or object store URL, s3:// , gcs:// , and as:// , by using the recursive flag, -r : $ pachctl put file <repo>@<branch> -r -f <dir> Loading Your Data Partially \u00b6 Depending on your use case, you might decide not to import all of your data into Pachyderm but only store and apply version control to some of it. For example, if you have a 10 PB dataset, loading the whole dataset into Pachyderm is a costly operation that takes a lot of time and resources. To optimize performance and the use of resources, you might decide to load some of this data into Pachyderm, leaving the rest of it in its original source. One possible way of doing this is by adding a metadata file with a URL to the specific file or directory in your dataset to a Pachyderm repository and refer to that file in your pipeline. Your pipeline code would read the URL or path in the external data source and retrieve that data as needed for processing instead of needing to preload it all into a Pachyderm repo. This method works particularly well for mostly immutable data because in this case, Pachyderm will not keep versions of the source file, but it will keep track and provenance of the resulting output commits in its version-control system.","title":"Load Your Data Into Pachyderm"},{"location":"how-tos/load-data-into-pachyderm/#load-your-data-into-pachyderm","text":"Info Before you read this section, make sure that you are familiar with the Data Concepts and Pipeline Concepts . The data that you commit to Pachyderm is stored in an object store of your choice, such as Amazon S3, MinIO, Google Cloud Storage, or other. Pachyderm records the cryptographic hash ( SHA ) of each portion of your data and stores it as a commit with a unique identifier (ID). Although the data is stored as an unstructured blob, Pachyderm enables you to interact with versioned data as you typically do in a standard file system. Pachyderm stores versioned data in repositories which can contain one or multiple files, as well as files arranged in directories. Regardless of the repository structure, Pachyderm versions the state of each data repository as the data changes over time. To put data into Pachyderm, a commit must be started , or opened . Data can then be put into Pachyderm as part of that open commit and is available once the commit is finished or closed . Pachyderm provides the following options to load data: By using the pachctl put file command. This option is great for testing, development, integration with CI/CD, and for users who prefer scripting. See Load Your Data by Using pachctl . By creating a pipeline to pull data from an outside source. Because Pachyderm pipelines can be any arbitrary code that runs in a Docker container, you can call out to external APIs or data sources and pull in data from there. Your pipeline code can be triggered on-demand or continuously with the following special types of pipelines: Spout: A spout enables you to continuously load streaming data from a streaming data source, such as a messaging system or message queue into Pachyderm. See Spout . Cron: A cron triggers your pipeline periodically based on the interval that you configure in your pipeline spec. See Cron . Note: Pipelines enable you to do much more than just ingressing data into Pachyderm. Pipelines can run all kinds of data transformations on your input data sources, such as a Pachyderm repository, and be configured to run your code automatically as new data is committed. For more information, see Pipeline . By using a Pachyderm language client. This option is ideal for Go or Python users who want to push data into Pachyderm from services or applications written in those languages. If you did not find your favorite language in the list of supported language clients, Pachyderm uses a protobuf API which supports many other languages. See Pachyderm Language Clients . If you are using the Pachyderm Enterprise version, you can use these additional options: By using the S3 gateway. This option is great to use with the existing tools and libraries that interact with S3-compatible object stores. See Using the S3 Gateway . By using the Pachyderm dashboard. The Pachyderm Enterprise dashboard provides a convenient way to upload data right from the UI. Note In the Pachyderm UI, you can only specify an S3 data source. Uploading data from your local device is not supported.","title":"Load Your Data Into Pachyderm"},{"location":"how-tos/load-data-into-pachyderm/#load-your-data-by-using-pachctl","text":"The pachctl put file command enables you to do everything from loading local files into Pachyderm to pulling data from an existing object store bucket and extracting data from a website. With pachctl put file , you can append new data to the existing data or overwrite the existing data. All these options can be configured by using the flags available with this command. Run pachctl put file --help to view the complete list of flags that you can specify. To load your data into Pachyderm by using pachctl , you first need to create one or more data repositories. Then, you can use the pachctl put file command to put your data into the created repository. In Pachyderm, you can start and finish commits. If you just run pachctl put file and no open commit exists, Pachyderm starts a new commit, adds the data to which you specified the path in your command, and finishes the commit. This is called an atomic commit. Alternatively, you can run pachctl start commit to start a new commit. Then, add your data in multiple put file calls, and finally, when ready, close the commit by running pachctl finish commit . To load your data into a repository, complete the following steps: Create a Pachyderm repository: $ pachctl create repo <repo name> Select from the following options: To start and finish an atomic commit, run: $ pachctl put file <repo>@<branch>:</path/to/file1> -f <file1> To start a commit and add data in iterations: Start a commit: $ pachctl start commit <repo>@<branch> 1. Put your data: $ pachctl put file <repo>@<branch>:</path/to/file1> -f <file1> Work on your changes, and when ready, put more data: $ pachctl put file <repo>@<branch>:</path/to/file2> -f <file2> Close the commit: $ pachctl finish commit <repo>@<branch>","title":"Load Your Data by Using pachctl"},{"location":"how-tos/load-data-into-pachyderm/#filepath-format","text":"In Pachyderm, you specify the path to file by using the -f option. A path to file can be a local path or a URL to an external resource. You can add multiple files or directories by using the -i option. To add contents of a directory, use the -r flag. The following table provides examples of pachctl put file commands with various filepaths and data sources: Put data from a URL: $ pachctl put file <repo>@<branch>:</path/to/file> -f http://url_path Put data from an object store. You can use s3:// , gcs:// , or as:// in your filepath: $ pachctl put file <repo>@<branch>:</path/to/file> -f s3://object_store_url Note If you are configuring a local cluster to access an S3 bucket, you need to first deploy a Kubernetes Secret for the selected object store. Add multiple files at once by using the -i option or multiple -f flags. In the case of -i , the target file must be a list of files, paths, or URLs that you want to input all at once: $ pachctl put file <repo>@<branch> -i <file containing list of files, paths, or URLs> Input data from stdin into a data repository by using a pipe: $ echo \"data\" | pachctl put file <repo>@<branch> -f </path/to/file> Add an entire directory or all of the contents at a particular URL, either HTTP(S) or object store URL, s3:// , gcs:// , and as:// , by using the recursive flag, -r : $ pachctl put file <repo>@<branch> -r -f <dir>","title":"Filepath Format"},{"location":"how-tos/load-data-into-pachyderm/#loading-your-data-partially","text":"Depending on your use case, you might decide not to import all of your data into Pachyderm but only store and apply version control to some of it. For example, if you have a 10 PB dataset, loading the whole dataset into Pachyderm is a costly operation that takes a lot of time and resources. To optimize performance and the use of resources, you might decide to load some of this data into Pachyderm, leaving the rest of it in its original source. One possible way of doing this is by adding a metadata file with a URL to the specific file or directory in your dataset to a Pachyderm repository and refer to that file in your pipeline. Your pipeline code would read the URL or path in the external data source and retrieve that data as needed for processing instead of needing to preload it all into a Pachyderm repo. This method works particularly well for mostly immutable data because in this case, Pachyderm will not keep versions of the source file, but it will keep track and provenance of the resulting output commits in its version-control system.","title":"Loading Your Data Partially"},{"location":"how-tos/removing_data_from_pachyderm/","text":"Delete Data \u00b6 If bad data was committed into a Pachyderm input repository, your pipeline might result in an error. In this case, you might need to delete this data to resolve the issue. Depending on the nature of the bad data and whether or not the bad data is in the HEAD of the branch, you can perform one of the following actions: Delete the HEAD of a Branch . If the incorrect data was added in the latest commit and no additional data was committed since then, follow the steps in this section to fix the HEAD of the corrupted branch. Delete Old Commits . If after committing the incorrect data, you have added more data to the same branch, follow the steps in this section to delete corrupted files. Delete sensitive data . If the bad commit included sensitive data that you need immediately and completely erase from Pachyderm, follow the steps in this section to purge data. Delete the HEAD of a Branch \u00b6 If you have just committed incorrect, corrupt, or otherwise bad data to a branch in a Pachyderm repository, the HEAD of your branch, or the latest commit is bad. Users who read from that commit might be misled, and pipelines subscribed to it might fail or produce bad downstream output. You can solve this issue by running the pachctl delete commit command. To fix a broken HEAD, run the following command: $ pachctl delete commit <repo>@<branch-or-commit-id> When you delete a bad commit, Pachyderm performs the following actions: Deletes the commit metadata. Changes HEADs of all the branches that had the bad commit as their HEAD to the bad commit's parent. If the bad commit does not have a parent, Pachyderm sets the branch's HEAD to nil . If the bad commit has children, sets their parents to the deleted commit parent. If the deleted commit does not have a parent, then the children commit parents are set to nil . Deletes all the jobs that were triggered by the bad commit. Also, Pachyderm interrupts all running jobs, including not only the jobs that use the bad commit as a direct input but also the ones farther downstream in your DAG. Deletes the output commits from the deleted jobs. All the actions listed above are applied to those commits as well. Delete Old Commits \u00b6 If you have committed more data to the branch after the bad data was added, you can try to delete the commit as described in Deleting the HEAD of a Branch . However, unless the subsequent commits overwrote or deleted the bad data, the bad data might still be present in the children commits. Deleting a commit does not modify its children. In Git terms, pachctl delete commit is equivalent to squashing a commit out of existence, such as with the git reset --hard command. The delete commit command is not equivalent to reverting a commit in Git. The reason for this behavior is that the semantics of revert can get ambiguous when the files that are being reverted have been otherwise modified. Because Pachyderm is a centralized system and the volume of data that you typically store in Pachyderm is large, merge conflicts can quickly become untenable. Therefore, Pachyderm prevents merge conflicts entirely. To resolve issues with the commits that are not at the tip of the branch, you can try to delete the children commits. However, those commits might also have the data that you might want to keep. To delete a file in an older commit, complete the following steps: Start a new commit: $ pachctl start commit <repo>@<branch> Delete all corrupted files from the newly opened commit: $ pachctl delete file <repo>@<branch or commitID>:/path/to/files Finish the commit: $ pachctl finish commit <repo>@<branch> Delete the initial bad commit and all its children up to the newly finished commit. Depending on how you use Pachyderm, the final step might be optional. After you finish the commit, the HEADs of all your branches converge to correct results as downstream jobs finish. However, deleting those commits cleans up your commit history and ensures that the errant data is not available when non-HEAD versions of the data is read. Delete Sensitive Data \u00b6 When you delete data as described in Delete Old Commits , Pachyderm does not immediately delete it from the physical disk. Instead, Pachyderm deletes references to the underlying data and later performs garbage collection. That is when the data is truly erased from the disk. If you have accidentally committed sensitive data and you need to ensure that it is immediately erased and inaccessible, complete the following steps: Delete all the references to data as described in Delete Old Commits . Run garbage-collect : $ pachctl garbage-collect To make garbage collection more comprehensive, increase the amount of memory that is used during the garbage collection operation by specifying the --memory flag. The default value is 10 MB.","title":"Delete Data"},{"location":"how-tos/removing_data_from_pachyderm/#delete-data","text":"If bad data was committed into a Pachyderm input repository, your pipeline might result in an error. In this case, you might need to delete this data to resolve the issue. Depending on the nature of the bad data and whether or not the bad data is in the HEAD of the branch, you can perform one of the following actions: Delete the HEAD of a Branch . If the incorrect data was added in the latest commit and no additional data was committed since then, follow the steps in this section to fix the HEAD of the corrupted branch. Delete Old Commits . If after committing the incorrect data, you have added more data to the same branch, follow the steps in this section to delete corrupted files. Delete sensitive data . If the bad commit included sensitive data that you need immediately and completely erase from Pachyderm, follow the steps in this section to purge data.","title":"Delete Data"},{"location":"how-tos/removing_data_from_pachyderm/#delete-the-head-of-a-branch","text":"If you have just committed incorrect, corrupt, or otherwise bad data to a branch in a Pachyderm repository, the HEAD of your branch, or the latest commit is bad. Users who read from that commit might be misled, and pipelines subscribed to it might fail or produce bad downstream output. You can solve this issue by running the pachctl delete commit command. To fix a broken HEAD, run the following command: $ pachctl delete commit <repo>@<branch-or-commit-id> When you delete a bad commit, Pachyderm performs the following actions: Deletes the commit metadata. Changes HEADs of all the branches that had the bad commit as their HEAD to the bad commit's parent. If the bad commit does not have a parent, Pachyderm sets the branch's HEAD to nil . If the bad commit has children, sets their parents to the deleted commit parent. If the deleted commit does not have a parent, then the children commit parents are set to nil . Deletes all the jobs that were triggered by the bad commit. Also, Pachyderm interrupts all running jobs, including not only the jobs that use the bad commit as a direct input but also the ones farther downstream in your DAG. Deletes the output commits from the deleted jobs. All the actions listed above are applied to those commits as well.","title":"Delete the HEAD of a Branch"},{"location":"how-tos/removing_data_from_pachyderm/#delete-old-commits","text":"If you have committed more data to the branch after the bad data was added, you can try to delete the commit as described in Deleting the HEAD of a Branch . However, unless the subsequent commits overwrote or deleted the bad data, the bad data might still be present in the children commits. Deleting a commit does not modify its children. In Git terms, pachctl delete commit is equivalent to squashing a commit out of existence, such as with the git reset --hard command. The delete commit command is not equivalent to reverting a commit in Git. The reason for this behavior is that the semantics of revert can get ambiguous when the files that are being reverted have been otherwise modified. Because Pachyderm is a centralized system and the volume of data that you typically store in Pachyderm is large, merge conflicts can quickly become untenable. Therefore, Pachyderm prevents merge conflicts entirely. To resolve issues with the commits that are not at the tip of the branch, you can try to delete the children commits. However, those commits might also have the data that you might want to keep. To delete a file in an older commit, complete the following steps: Start a new commit: $ pachctl start commit <repo>@<branch> Delete all corrupted files from the newly opened commit: $ pachctl delete file <repo>@<branch or commitID>:/path/to/files Finish the commit: $ pachctl finish commit <repo>@<branch> Delete the initial bad commit and all its children up to the newly finished commit. Depending on how you use Pachyderm, the final step might be optional. After you finish the commit, the HEADs of all your branches converge to correct results as downstream jobs finish. However, deleting those commits cleans up your commit history and ensures that the errant data is not available when non-HEAD versions of the data is read.","title":"Delete Old Commits"},{"location":"how-tos/removing_data_from_pachyderm/#delete-sensitive-data","text":"When you delete data as described in Delete Old Commits , Pachyderm does not immediately delete it from the physical disk. Instead, Pachyderm deletes references to the underlying data and later performs garbage collection. That is when the data is truly erased from the disk. If you have accidentally committed sensitive data and you need to ensure that it is immediately erased and inaccessible, complete the following steps: Delete all the references to data as described in Delete Old Commits . Run garbage-collect : $ pachctl garbage-collect To make garbage collection more comprehensive, increase the amount of memory that is used during the garbage collection operation by specifying the --memory flag. The default value is 10 MB.","title":"Delete Sensitive Data"},{"location":"how-tos/run_pipeline/","text":"Run a Pipeline on a Specific Commit \u00b6 Sometimes you need to see the result of merging different commits to analyze and identify correct combinations and potential flows in data collection and processing. Or, you just want to rerun a failed job manually. Pachyderm enables you to run your pipelines with old commits or in different handpicked combinations of old commits that are stored in separate repositories or branches. This functionality is particularly useful for cross-pipelines, but you can also use it with other types of pipelines. For example, you have branches A , B , and C . Each of these branches has had commits on it at various times, and each new commit triggered a new job from the pipeline. Your branches have the following commit history: Branch A has commits A1 , A2 , A3 , and A4 . Branch B has commits B1 and B2 . Branch C has commits C1 , C2 , and C3 . For example, you need to see the result of the pipeline with the combination of data after A4 , B1 , and C2 were committed. But none of the output commits were triggered on this particular combination. To get the result of this combination, you can run the pachctl run pipeline cross-pipe command. Example $ pachctl run pipeline cross-pipe A4 B1 C2 This command triggers a new job that creates a commit on the pipeline's output branch with the result of that combination of data. Because A4 is the head of branch A , you can also omit the A4 commit in the command and specify only the C2 and B1 commits: Example $ pachctl run pipeline cross-pipe C2 B1 Pachyderm automatically uses the head for any branch that did not have a commit specified. The order in which you specify the commits does not matter. Pachyderm knows how to match them to the right place. Also, you can reference the head commit of a branch by using the branch name. Example $ pachctl run pipeline cross-pipe A B1 C2 This behavior implies that if you want to re-run the pipeline on the most recent commits, you can just run pachctl run pipeline cross-pipe . If you try to use a commit from a branch that is not an input branch, pachctl returns an error. Similarly, specifying multiple commits from the same branch results in error. You do not need to run the pachctl run pipeline cross-pipe command to initiate a newly created pipeline. Pachyderm runs the new pipelines automatically as you add new commits to the corresponding input branches.","title":"Run a Pipeline on a Specific Commit"},{"location":"how-tos/run_pipeline/#run-a-pipeline-on-a-specific-commit","text":"Sometimes you need to see the result of merging different commits to analyze and identify correct combinations and potential flows in data collection and processing. Or, you just want to rerun a failed job manually. Pachyderm enables you to run your pipelines with old commits or in different handpicked combinations of old commits that are stored in separate repositories or branches. This functionality is particularly useful for cross-pipelines, but you can also use it with other types of pipelines. For example, you have branches A , B , and C . Each of these branches has had commits on it at various times, and each new commit triggered a new job from the pipeline. Your branches have the following commit history: Branch A has commits A1 , A2 , A3 , and A4 . Branch B has commits B1 and B2 . Branch C has commits C1 , C2 , and C3 . For example, you need to see the result of the pipeline with the combination of data after A4 , B1 , and C2 were committed. But none of the output commits were triggered on this particular combination. To get the result of this combination, you can run the pachctl run pipeline cross-pipe command. Example $ pachctl run pipeline cross-pipe A4 B1 C2 This command triggers a new job that creates a commit on the pipeline's output branch with the result of that combination of data. Because A4 is the head of branch A , you can also omit the A4 commit in the command and specify only the C2 and B1 commits: Example $ pachctl run pipeline cross-pipe C2 B1 Pachyderm automatically uses the head for any branch that did not have a commit specified. The order in which you specify the commits does not matter. Pachyderm knows how to match them to the right place. Also, you can reference the head commit of a branch by using the branch name. Example $ pachctl run pipeline cross-pipe A B1 C2 This behavior implies that if you want to re-run the pipeline on the most recent commits, you can just run pachctl run pipeline cross-pipe . If you try to use a commit from a branch that is not an input branch, pachctl returns an error. Similarly, specifying multiple commits from the same branch results in error. You do not need to run the pachctl run pipeline cross-pipe command to initiate a newly created pipeline. Pachyderm runs the new pipelines automatically as you add new commits to the corresponding input branches.","title":"Run a Pipeline on a Specific Commit"},{"location":"how-tos/team-developer-workflow/","text":"Team Developer Workflow \u00b6 This section describes an example of how you can incorporate Pachyderm into your existing enterprise infrastructure. If you are just starting to use Pachyderm and not setting up automation for your Pachyderm build processes, see Individual Developer Workflow . Pachyderm is a powerful system for providing data provenance and scalable processing to data scientists and engineers. You can make it even more powerful by integrating it with your existing continuous integration and continuous deployment (CI/CD) workflows and systems. This section walks you through the CI/CD processes that you can create to enable Pachyderm collaboration within your data science and engineering groups. As you write code, you test it in containers and notebooks against sample data that you store in Pachyderm repos or mount it locally. Your code runs in development pipelines in Pachyderm. Pachyderm provides capabilities that assist with day-to-day development practices, including the --build and --push-images flags to the pachctl update pipeline command that enable you to build and push images to a Docker registry. Although initial CI setup might require extra effort on your side, in the long run, it brings significant benefits to your team, including the following: Simplified workflow for data scientists. Data scientists do not need to be aware of the complexity of the underlying containerized infrastructure. They can follow an established Git process, and the CI platform takes care of the Docker build and push process behind the scenes. Your CI platform can run additional unit tests against the submitted code before creating the build. Flexibility in tagging Docker images, such as specifying a custom name and tag or using the commit SHA for tagging. The following diagram demonstrates automated Pachyderm development workflow: The automated developer workflow includes the following steps: A new commit triggers a Git hook. Typically, Pachyderm users store the following artifacts in a Git repository: A Dockerfile that you use to build local images. A pipeline.json specification file that you can use in a Makefile to create local builds, as well as in the CI/CD workflows. The code that performs data transformations. A commit hook in Git for your repository triggers the CI/CD process. It uses the information in your pipeline specification for subsequent steps. Build an image. Your CI process automatically starts the build of a Docker container image based on your code and the Dockerfile. Push the image tagged with commit ID to an image registry. Your CI process pushes a Docker image created in Step 2 to your preferred image registry. When a data scientist submits their code to Git, a CI process uses the Dockerfile in the repository to build, tag with a Git commit SHA, and push the container to your image registry. Update the pipeline spec with the tagged image. In this step, your CI/CD infrastructure uses your updated pipeline.json specification and fills in the Git commit SHA for the version of the image that must be used in this pipeline. Then, it runs the pachctl update pipeline command to push the updated pipeline specification to Pachyderm. After that, Pachyderm pulls a new image from the registry automatically. When the production pipeline is updated with the pipeline.json file that has the correct image tag in it, Pachyderm restarts all pods for this pipeline with the new image automatically.","title":"Team Developer Workflow"},{"location":"how-tos/team-developer-workflow/#team-developer-workflow","text":"This section describes an example of how you can incorporate Pachyderm into your existing enterprise infrastructure. If you are just starting to use Pachyderm and not setting up automation for your Pachyderm build processes, see Individual Developer Workflow . Pachyderm is a powerful system for providing data provenance and scalable processing to data scientists and engineers. You can make it even more powerful by integrating it with your existing continuous integration and continuous deployment (CI/CD) workflows and systems. This section walks you through the CI/CD processes that you can create to enable Pachyderm collaboration within your data science and engineering groups. As you write code, you test it in containers and notebooks against sample data that you store in Pachyderm repos or mount it locally. Your code runs in development pipelines in Pachyderm. Pachyderm provides capabilities that assist with day-to-day development practices, including the --build and --push-images flags to the pachctl update pipeline command that enable you to build and push images to a Docker registry. Although initial CI setup might require extra effort on your side, in the long run, it brings significant benefits to your team, including the following: Simplified workflow for data scientists. Data scientists do not need to be aware of the complexity of the underlying containerized infrastructure. They can follow an established Git process, and the CI platform takes care of the Docker build and push process behind the scenes. Your CI platform can run additional unit tests against the submitted code before creating the build. Flexibility in tagging Docker images, such as specifying a custom name and tag or using the commit SHA for tagging. The following diagram demonstrates automated Pachyderm development workflow: The automated developer workflow includes the following steps: A new commit triggers a Git hook. Typically, Pachyderm users store the following artifacts in a Git repository: A Dockerfile that you use to build local images. A pipeline.json specification file that you can use in a Makefile to create local builds, as well as in the CI/CD workflows. The code that performs data transformations. A commit hook in Git for your repository triggers the CI/CD process. It uses the information in your pipeline specification for subsequent steps. Build an image. Your CI process automatically starts the build of a Docker container image based on your code and the Dockerfile. Push the image tagged with commit ID to an image registry. Your CI process pushes a Docker image created in Step 2 to your preferred image registry. When a data scientist submits their code to Git, a CI process uses the Dockerfile in the repository to build, tag with a Git commit SHA, and push the container to your image registry. Update the pipeline spec with the tagged image. In this step, your CI/CD infrastructure uses your updated pipeline.json specification and fills in the Git commit SHA for the version of the image that must be used in this pipeline. Then, it runs the pachctl update pipeline command to push the updated pipeline specification to Pachyderm. After that, Pachyderm pulls a new image from the registry automatically. When the production pipeline is updated with the pipeline.json file that has the correct image tag in it, Pachyderm restarts all pods for this pipeline with the new image automatically.","title":"Team Developer Workflow"},{"location":"how-tos/time_windows/","text":"Processing Time-Windowed Data \u00b6 Info Before you read this section, make sure that you understand the concepts described in the following sections: Datum Distributed Computing Individual Developer Worflow If you are analyzing data that is changing over time, you might need to analyze historical data. For example, you might need to examine the last two weeks of data , January's data , or some other moving or static time window of data. Pachyderm provides the following approaches to this task: Fixed time windows - for rigid, fixed time windows, such as months (Jan, Feb, and so on) or days\u201401-01-17, 01-02-17, and so on). Moving time windows - for rolling time windows of data, such as three-day windows or two-week windows. Fixed Time Windows \u00b6 Datum is the basic unit of data partitioning in Pachyderm. The glob pattern property in the pipeline specification defines a datum. When you analyze data within fixed time windows, such as the data that corresponds to fixed calendar dates, Pachyderm recommends that you organize your data repositories so that each of the time windows that you plan to analyze corresponds to a separate file or directory in your repository, and therefore, Pachyderm processes it as a separate datum. Organizing your repository as described above, enables you to do the following: Analyze each time window in parallel. Only re-process data within a time window when that data, or a corresponding data pipeline, changes. For example, if you have monthly time windows of sales data stored in JSON format that needs to be analyzed, you can create a sales data repository with the following data: sales \u251c\u2500\u2500 January | \u251c\u2500\u2500 01-01-17.json | \u251c\u2500\u2500 01-02-17.json | \u2514\u2500\u2500 ... \u251c\u2500\u2500 February | \u251c\u2500\u2500 01-01-17.json | \u251c\u2500\u2500 01-02-17.json | \u2514\u2500\u2500 ... \u2514\u2500\u2500 March \u251c\u2500\u2500 01-01-17.json \u251c\u2500\u2500 01-02-17.json \u2514\u2500\u2500 ... When you run a pipeline with sales as an input repository and a glob pattern of /* , Pachyderm processes each month's worth of sales data in parallel if workers are available. When you add new data into a subset of the months or add data into a new month, for example, May, Pachyderm processes only these updated datums. More generally, this structure enables you to create the following types of pipelines: Pipelines that aggregate or otherwise process daily data on a monthly basis by using the /* glob pattern. Pipelines that only analyze a particular month's data by using a /subdir/* or /subdir/ glob pattern. For example, /January/* or /January/ . Pipelines that process data on daily by using the /*/* glob pattern. Any combination of the above. Moving Time Windows \u00b6 In some cases, you need to run analyses for moving or rolling time windows that do not correspond to certain calendar months or days. For example, you might need to analyze the last three days of data, the three days of data before that, or similar. In other words, you need to run an analysis for every rolling length of time. For rolling or moving time windows, there are a couple of recommended patterns: Bin your data in repository folders for each of the moving time windows. Maintain a time-windowed set of data that corresponds to the latest of the moving time windows. Bin Data into Moving Time Windows \u00b6 In this method of processing rolling time windows, you create the following two-pipeline DAGs to analyze time windows efficiently: Pipeline Description Pipeline 1 Reads in data, determines to which bins the data corresponds, and writes the data into those bins. Pipeline 2 Read in and analyze the binned data. By splitting this analysis into two pipelines, you can benefit from using parallelism at the file level. In other words, Pipeline 1 can be easily parallelized for each file, and Pipeline 2 can be parallelized per bin. This structure enables easy pipeline scaling as the number of files increases. For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. In the first repo, called sales , you commit data for the first day of sales: sales \u2514\u2500\u2500 01 -01-17.json In the first pipeline, you specify to bin this data into a directory that corresponds to the first rolling time window from 01-01-17 to 01-03-17: binned_sales \u2514\u2500\u2500 01 -01-17_to_01-03-17 \u2514\u2500\u2500 01 -01-17.json When the next day's worth of sales is committed, that data lands in the sales repository: sales \u251c\u2500\u2500 01 -01-17.json \u2514\u2500\u2500 01 -02-17.json Then, the first pipeline executes again to bin the 01-02-17 data into relevant bins. In this case, the data is placed in the previously created bin named 01-01-17 to 01-03-17 . However, the data also goes to the bin that stores the data that is received starting on 01-02-17 : binned_sales \u251c\u2500\u2500 01 -01-17_to_01-03-17 | \u251c\u2500\u2500 01 -01-17.json | \u2514\u2500\u2500 01 -02-17.json \u2514\u2500\u2500 01 -02-17_to_01-04-17 \u2514\u2500\u2500 01 -02-17.json As more and more daily data is added, your repository structure starting to looks as follows: binned_sales \u251c\u2500\u2500 01 -01-17_to_01-03-17 | \u251c\u2500\u2500 01 -01-17.json | \u251c\u2500\u2500 01 -02-17.json | \u2514\u2500\u2500 01 -03-17.json \u251c\u2500\u2500 01 -02-17_to_01-04-17 | \u251c\u2500\u2500 01 -02-17.json | \u251c\u2500\u2500 01 -03-17.json | \u2514\u2500\u2500 01 -04-17.json \u251c\u2500\u2500 01 -03-17_to_01-05-17 | \u251c\u2500\u2500 01 -03-17.json | \u251c\u2500\u2500 01 -04-17.json | \u2514\u2500\u2500 01 -05-17.json \u2514\u2500\u2500 ... The following diagram describes how data accumulates in the repository over time: Your second pipeline can then process these bins in parallel according to the glob pattern of /* or as described further. Both pipelines can be easily parallelized. In the above directory structure, it might seem that data is duplicated. However, under the hood, Pachyderm deduplicates all of these files and maintains a space-efficient representation of your data. The binning of the data is merely a structural re-arrangement to enable you process these types of moving time windows. It might also seem as if Pachyderm performs unnecessary data transfers over the network to bin files. However, Pachyderm ensures that these data operations do not require transferring data over the network. Maintaining a Single Time-Windowed Data Set \u00b6 The advantage of the binning pattern above is that any of the moving time windows are available for processing. They can be compared, aggregated, and combined in any way, and any results or aggregations are kept in sync with updates to the bins. However, you do need to create a process to maintain the binning directory structure. There is another pattern for moving time windows that avoids the binning of the above approach and maintains an up-to-date version of a moving time-windowed data set. This approach involves the creation of the following pipelines: Pipeline Description Pipeline 1 Reads in data, determines which files belong in your moving time window, and writes the relevant files into an updated version of the moving time-windowed data set. Pipeline 2 Reads in and analyzes the moving time-windowed data set. For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. The input data is stored in the sales repository: sales \u251c\u2500\u2500 01 -01-17.json \u251c\u2500\u2500 01 -02-17.json \u251c\u2500\u2500 01 -03-17.json \u2514\u2500\u2500 01 -04-17.json When the January 4 th file, 01-04-17.json , is committed, the first pipeline pulls out the last three days of data and arranges it in the following order: last_three_days \u251c\u2500\u2500 01 -02-17.json \u251c\u2500\u2500 01 -03-17.json \u2514\u2500\u2500 01 -04-17.json When the January 5 th file, 01-05-17.json , is committed into the sales repository: sales \u251c\u2500\u2500 01 -01-17.json \u251c\u2500\u2500 01 -02-17.json \u251c\u2500\u2500 01 -03-17.json \u251c\u2500\u2500 01 -04-17.json \u2514\u2500\u2500 01 -05-17.json the first pipeline updates the moving window: last_three_days \u251c\u2500\u2500 01 -03-17.json \u251c\u2500\u2500 01 -04-17.json \u2514\u2500\u2500 01 -05-17.json The analysis that you need to run on the moving windowed dataset in moving_sales_window can use the / or /* glob pattern, depending on whether you need to process all of the time-windowed files together or if they can be processed in parallel. Warning When you create this type of moving time-windowed data set, the concept of now or today is relative. You must define the time based on your use case. For example, by configuring to use UTC . Do not use functions such as time.now() to determine the current time. The actual time when this pipeline runs might vary.","title":"Processing Time-Windowed Data"},{"location":"how-tos/time_windows/#processing-time-windowed-data","text":"Info Before you read this section, make sure that you understand the concepts described in the following sections: Datum Distributed Computing Individual Developer Worflow If you are analyzing data that is changing over time, you might need to analyze historical data. For example, you might need to examine the last two weeks of data , January's data , or some other moving or static time window of data. Pachyderm provides the following approaches to this task: Fixed time windows - for rigid, fixed time windows, such as months (Jan, Feb, and so on) or days\u201401-01-17, 01-02-17, and so on). Moving time windows - for rolling time windows of data, such as three-day windows or two-week windows.","title":"Processing Time-Windowed Data"},{"location":"how-tos/time_windows/#fixed-time-windows","text":"Datum is the basic unit of data partitioning in Pachyderm. The glob pattern property in the pipeline specification defines a datum. When you analyze data within fixed time windows, such as the data that corresponds to fixed calendar dates, Pachyderm recommends that you organize your data repositories so that each of the time windows that you plan to analyze corresponds to a separate file or directory in your repository, and therefore, Pachyderm processes it as a separate datum. Organizing your repository as described above, enables you to do the following: Analyze each time window in parallel. Only re-process data within a time window when that data, or a corresponding data pipeline, changes. For example, if you have monthly time windows of sales data stored in JSON format that needs to be analyzed, you can create a sales data repository with the following data: sales \u251c\u2500\u2500 January | \u251c\u2500\u2500 01-01-17.json | \u251c\u2500\u2500 01-02-17.json | \u2514\u2500\u2500 ... \u251c\u2500\u2500 February | \u251c\u2500\u2500 01-01-17.json | \u251c\u2500\u2500 01-02-17.json | \u2514\u2500\u2500 ... \u2514\u2500\u2500 March \u251c\u2500\u2500 01-01-17.json \u251c\u2500\u2500 01-02-17.json \u2514\u2500\u2500 ... When you run a pipeline with sales as an input repository and a glob pattern of /* , Pachyderm processes each month's worth of sales data in parallel if workers are available. When you add new data into a subset of the months or add data into a new month, for example, May, Pachyderm processes only these updated datums. More generally, this structure enables you to create the following types of pipelines: Pipelines that aggregate or otherwise process daily data on a monthly basis by using the /* glob pattern. Pipelines that only analyze a particular month's data by using a /subdir/* or /subdir/ glob pattern. For example, /January/* or /January/ . Pipelines that process data on daily by using the /*/* glob pattern. Any combination of the above.","title":"Fixed Time Windows"},{"location":"how-tos/time_windows/#moving-time-windows","text":"In some cases, you need to run analyses for moving or rolling time windows that do not correspond to certain calendar months or days. For example, you might need to analyze the last three days of data, the three days of data before that, or similar. In other words, you need to run an analysis for every rolling length of time. For rolling or moving time windows, there are a couple of recommended patterns: Bin your data in repository folders for each of the moving time windows. Maintain a time-windowed set of data that corresponds to the latest of the moving time windows.","title":"Moving Time Windows"},{"location":"how-tos/time_windows/#bin-data-into-moving-time-windows","text":"In this method of processing rolling time windows, you create the following two-pipeline DAGs to analyze time windows efficiently: Pipeline Description Pipeline 1 Reads in data, determines to which bins the data corresponds, and writes the data into those bins. Pipeline 2 Read in and analyze the binned data. By splitting this analysis into two pipelines, you can benefit from using parallelism at the file level. In other words, Pipeline 1 can be easily parallelized for each file, and Pipeline 2 can be parallelized per bin. This structure enables easy pipeline scaling as the number of files increases. For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. In the first repo, called sales , you commit data for the first day of sales: sales \u2514\u2500\u2500 01 -01-17.json In the first pipeline, you specify to bin this data into a directory that corresponds to the first rolling time window from 01-01-17 to 01-03-17: binned_sales \u2514\u2500\u2500 01 -01-17_to_01-03-17 \u2514\u2500\u2500 01 -01-17.json When the next day's worth of sales is committed, that data lands in the sales repository: sales \u251c\u2500\u2500 01 -01-17.json \u2514\u2500\u2500 01 -02-17.json Then, the first pipeline executes again to bin the 01-02-17 data into relevant bins. In this case, the data is placed in the previously created bin named 01-01-17 to 01-03-17 . However, the data also goes to the bin that stores the data that is received starting on 01-02-17 : binned_sales \u251c\u2500\u2500 01 -01-17_to_01-03-17 | \u251c\u2500\u2500 01 -01-17.json | \u2514\u2500\u2500 01 -02-17.json \u2514\u2500\u2500 01 -02-17_to_01-04-17 \u2514\u2500\u2500 01 -02-17.json As more and more daily data is added, your repository structure starting to looks as follows: binned_sales \u251c\u2500\u2500 01 -01-17_to_01-03-17 | \u251c\u2500\u2500 01 -01-17.json | \u251c\u2500\u2500 01 -02-17.json | \u2514\u2500\u2500 01 -03-17.json \u251c\u2500\u2500 01 -02-17_to_01-04-17 | \u251c\u2500\u2500 01 -02-17.json | \u251c\u2500\u2500 01 -03-17.json | \u2514\u2500\u2500 01 -04-17.json \u251c\u2500\u2500 01 -03-17_to_01-05-17 | \u251c\u2500\u2500 01 -03-17.json | \u251c\u2500\u2500 01 -04-17.json | \u2514\u2500\u2500 01 -05-17.json \u2514\u2500\u2500 ... The following diagram describes how data accumulates in the repository over time: Your second pipeline can then process these bins in parallel according to the glob pattern of /* or as described further. Both pipelines can be easily parallelized. In the above directory structure, it might seem that data is duplicated. However, under the hood, Pachyderm deduplicates all of these files and maintains a space-efficient representation of your data. The binning of the data is merely a structural re-arrangement to enable you process these types of moving time windows. It might also seem as if Pachyderm performs unnecessary data transfers over the network to bin files. However, Pachyderm ensures that these data operations do not require transferring data over the network.","title":"Bin Data into Moving Time Windows"},{"location":"how-tos/time_windows/#maintaining-a-single-time-windowed-data-set","text":"The advantage of the binning pattern above is that any of the moving time windows are available for processing. They can be compared, aggregated, and combined in any way, and any results or aggregations are kept in sync with updates to the bins. However, you do need to create a process to maintain the binning directory structure. There is another pattern for moving time windows that avoids the binning of the above approach and maintains an up-to-date version of a moving time-windowed data set. This approach involves the creation of the following pipelines: Pipeline Description Pipeline 1 Reads in data, determines which files belong in your moving time window, and writes the relevant files into an updated version of the moving time-windowed data set. Pipeline 2 Reads in and analyzes the moving time-windowed data set. For example, you have three-day moving time windows, and you want to analyze three-day moving windows of sales data. The input data is stored in the sales repository: sales \u251c\u2500\u2500 01 -01-17.json \u251c\u2500\u2500 01 -02-17.json \u251c\u2500\u2500 01 -03-17.json \u2514\u2500\u2500 01 -04-17.json When the January 4 th file, 01-04-17.json , is committed, the first pipeline pulls out the last three days of data and arranges it in the following order: last_three_days \u251c\u2500\u2500 01 -02-17.json \u251c\u2500\u2500 01 -03-17.json \u2514\u2500\u2500 01 -04-17.json When the January 5 th file, 01-05-17.json , is committed into the sales repository: sales \u251c\u2500\u2500 01 -01-17.json \u251c\u2500\u2500 01 -02-17.json \u251c\u2500\u2500 01 -03-17.json \u251c\u2500\u2500 01 -04-17.json \u2514\u2500\u2500 01 -05-17.json the first pipeline updates the moving window: last_three_days \u251c\u2500\u2500 01 -03-17.json \u251c\u2500\u2500 01 -04-17.json \u2514\u2500\u2500 01 -05-17.json The analysis that you need to run on the moving windowed dataset in moving_sales_window can use the / or /* glob pattern, depending on whether you need to process all of the time-windowed files together or if they can be processed in parallel. Warning When you create this type of moving time-windowed data set, the concept of now or today is relative. You must define the time based on your use case. For example, by configuring to use UTC . Do not use functions such as time.now() to determine the current time. The actual time when this pipeline runs might vary.","title":"Maintaining a Single Time-Windowed Data Set"},{"location":"how-tos/updating_pipelines/","text":"Update a Pipeline \u00b6 While working with your data, you often need to modify an existing pipeline with new transformation code or pipeline parameters. For example, when you develop a machine learning model, you might need to try different versions of your model while your training data stays relatively constant. To make changes to a pipeline, you need to use the pachctl update pipeline command. Update Your Pipeline Specification \u00b6 If you need to update your pipeline specification , such as change the parallelism settings, add an input repository, or other, you need to update your JSON file and then run the pachctl update pipeline command. By default, when you update your code, the new pipeline specification does not reprocess the data that has already been processed. Instead, it processes only the new data that you submit to the input repo. If you want to run the changes in your pipeline against the data in the HEAD commit of your input repo, use the --reprocess flag. After that, the updated pipeline continues to process new input data. Previous results remain accessible through the corresponding commit IDs. To update a pipeline specification, complete the following steps: Make the changes in your pipeline specification JSON file. Update the pipeline with the new configuration: $ pachctl update pipeline -f pipeline.json Similar to create pipeline , update pipeline with the -f flag can also take a URL if your JSON manifest is hosted on GitHub or other remote location. Update the Code in a Pipeline \u00b6 The pachctl update pipeline updates the code that you use in one or more of your pipelines. To apply your code changes, you need to build a new Docker image and push it to your Docker image registry. You can either use your registry instructions to build and push your new image or push the new image by using the flags built into the pachctl update pipeline command. Both procedures achieve the same goal, and it is entirely a matter of a personal preference which one of them to follow. If you do not have a build-push process that you already follow, you might prefer to use Pachyderm's built-in functionality. To create a new image by using the Pachyderm commands, you need to use the --build flag with the pachctl update pipeline command. By default, if you do not specify a registry with the --registry flag, Pachyderm uses DockerHub . When you build your image with Pachyderm, it assigns a random tag to your new image. If you use a private registry or any other registry that is different from the default value, use the --registry flag to specify it. Make sure that you specify the private registry in the pipeline specification . For example, if you want to push a pachyderm/opencv image to a registry located at localhost:5000 , you need to add this in your pipeline spec: \"image\" : \"localhost:5000/pachyderm/opencv\" Also, to be able to build and push images, you need to make sure that the Docker daemon is running. Depending on your operating system and the Docker distribution that you use, steps for enabling it might vary. To update the code in your pipeline, complete the following steps: Make the code changes. Verify that the Docker daemon is running: $ docker ps If you get an error message similar to the following: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? enable the Docker daemon. To enable the Docker daemon, see the Docker documentation for your operating system and platform. For example, if you use minikube on macOS, run the following command: $ eval $( minikube docker-env ) Build, tag, and push the new image to your image registry: If you prefer to use Pachyderm commands: Run the following command: $ pachctl update pipeline -f <pipeline name> --build --registry <registry> --username <registry user> If you use DockerHub, omit the --registry flag. Example: $ pachctl update pipeline -f edges.json --build --username testuser When prompted, type your image registry password: Example: Password for docker.io/testuser: Building pachyderm/opencv:f1e0239fce5441c483b09de425f06b40, this may take a while. If you prefer to use instructions for your image registry: Build, tag, and push a new image as described in the image registry documentation. For example, if you use DockerHub, see Docker Documentation . Update the pipeline: $ pachctl update pipeline -f <pipeline.json>","title":"Update a Pipeline"},{"location":"how-tos/updating_pipelines/#update-a-pipeline","text":"While working with your data, you often need to modify an existing pipeline with new transformation code or pipeline parameters. For example, when you develop a machine learning model, you might need to try different versions of your model while your training data stays relatively constant. To make changes to a pipeline, you need to use the pachctl update pipeline command.","title":"Update a Pipeline"},{"location":"how-tos/updating_pipelines/#update-your-pipeline-specification","text":"If you need to update your pipeline specification , such as change the parallelism settings, add an input repository, or other, you need to update your JSON file and then run the pachctl update pipeline command. By default, when you update your code, the new pipeline specification does not reprocess the data that has already been processed. Instead, it processes only the new data that you submit to the input repo. If you want to run the changes in your pipeline against the data in the HEAD commit of your input repo, use the --reprocess flag. After that, the updated pipeline continues to process new input data. Previous results remain accessible through the corresponding commit IDs. To update a pipeline specification, complete the following steps: Make the changes in your pipeline specification JSON file. Update the pipeline with the new configuration: $ pachctl update pipeline -f pipeline.json Similar to create pipeline , update pipeline with the -f flag can also take a URL if your JSON manifest is hosted on GitHub or other remote location.","title":"Update Your Pipeline Specification"},{"location":"how-tos/updating_pipelines/#update-the-code-in-a-pipeline","text":"The pachctl update pipeline updates the code that you use in one or more of your pipelines. To apply your code changes, you need to build a new Docker image and push it to your Docker image registry. You can either use your registry instructions to build and push your new image or push the new image by using the flags built into the pachctl update pipeline command. Both procedures achieve the same goal, and it is entirely a matter of a personal preference which one of them to follow. If you do not have a build-push process that you already follow, you might prefer to use Pachyderm's built-in functionality. To create a new image by using the Pachyderm commands, you need to use the --build flag with the pachctl update pipeline command. By default, if you do not specify a registry with the --registry flag, Pachyderm uses DockerHub . When you build your image with Pachyderm, it assigns a random tag to your new image. If you use a private registry or any other registry that is different from the default value, use the --registry flag to specify it. Make sure that you specify the private registry in the pipeline specification . For example, if you want to push a pachyderm/opencv image to a registry located at localhost:5000 , you need to add this in your pipeline spec: \"image\" : \"localhost:5000/pachyderm/opencv\" Also, to be able to build and push images, you need to make sure that the Docker daemon is running. Depending on your operating system and the Docker distribution that you use, steps for enabling it might vary. To update the code in your pipeline, complete the following steps: Make the code changes. Verify that the Docker daemon is running: $ docker ps If you get an error message similar to the following: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? enable the Docker daemon. To enable the Docker daemon, see the Docker documentation for your operating system and platform. For example, if you use minikube on macOS, run the following command: $ eval $( minikube docker-env ) Build, tag, and push the new image to your image registry: If you prefer to use Pachyderm commands: Run the following command: $ pachctl update pipeline -f <pipeline name> --build --registry <registry> --username <registry user> If you use DockerHub, omit the --registry flag. Example: $ pachctl update pipeline -f edges.json --build --username testuser When prompted, type your image registry password: Example: Password for docker.io/testuser: Building pachyderm/opencv:f1e0239fce5441c483b09de425f06b40, this may take a while. If you prefer to use instructions for your image registry: Build, tag, and push a new image as described in the image registry documentation. For example, if you use DockerHub, see Docker Documentation . Update the pipeline: $ pachctl update pipeline -f <pipeline.json>","title":"Update the Code in a Pipeline"},{"location":"how-tos/splitting-data/","text":".. _splitting-data: Split Data \u00b6 Pachyderm provides functionality that enables you to split your data while it is being loaded into a Pachyderm input repository which helps to optimize pipeline processing time and resource utilization. Splitting data helps you to address the following use cases: Optimize data processing . If you have large size files, it might take Pachyderm a significant amount of time to process them. In addition, Pachyderm considers such a file as a single datum, and every time you apply even a minor change to that datum, Pachyderm processes the whole file from scratch. Increase diff granularity . Pachyderm does not create per-line diffs that display line-by-line changes. Instead, Pachyderm provides per-file diffing. Therefore, if all of your data is in one huge file, you might not be able to see what has changed in that file. Breaking up the file to smaller chunks addresses this issue. This section provides examples of how you can use the --split command that breaks up your data into smaller chunks, called split-files and what happens when you update your data. This section includes the following topics: .. toctree:: :maxdepth: 2 splitting.md adjusting_data_processing_w_split.md","title":"Overview"},{"location":"how-tos/splitting-data/#split-data","text":"Pachyderm provides functionality that enables you to split your data while it is being loaded into a Pachyderm input repository which helps to optimize pipeline processing time and resource utilization. Splitting data helps you to address the following use cases: Optimize data processing . If you have large size files, it might take Pachyderm a significant amount of time to process them. In addition, Pachyderm considers such a file as a single datum, and every time you apply even a minor change to that datum, Pachyderm processes the whole file from scratch. Increase diff granularity . Pachyderm does not create per-line diffs that display line-by-line changes. Instead, Pachyderm provides per-file diffing. Therefore, if all of your data is in one huge file, you might not be able to see what has changed in that file. Breaking up the file to smaller chunks addresses this issue. This section provides examples of how you can use the --split command that breaks up your data into smaller chunks, called split-files and what happens when you update your data. This section includes the following topics: .. toctree:: :maxdepth: 2 splitting.md adjusting_data_processing_w_split.md","title":"Split Data"},{"location":"how-tos/splitting-data/adjusting_data_processing_w_split/","text":"Adjusting Data Processing by Splitting Data \u00b6 Info Before you read this section, make sure that you understand the concepts described in File , Glob Pattern , Pipeline Specification , and Individual Developer Workflow . Unlike source code version-control systems, such as Git, that mostly store and version text files, Pachyderm does not perform intra-file diffing. This means that if you have a 10,000 lines CSV file and change a single line in that file, a pipeline that is subscribed to the repository where that file is located processes the whole file. You can adjust this behavior by splitting your file upon loading into chunks. Pachyderm applies diffing at per file level. Therefore, if one bit of a file changes, Pachyderm identifies that change as a new file. Similarly, Pachyderm can only distribute computation at the level of a single file. If your data is stored in one large file, it can only be processed by a single worker, which might affect performance. To optimize performance and processing time, you might want to break up large files into smaller chunks while Pachyderm uploads data to an input repository. For simple data types, you can run the pachctl put file command with the --split flag. For more complex splitting pattern, such as when you work with .avro or other binary formats, you need to manually split your data either at ingest or by configuring splitting in a Pachyderm pipeline. Using Split and Target-File Flags \u00b6 For common file types that are often used in data science, such as CSV, line-delimited text files, JavaScript Object Notation (JSON) files, Pachyderm includes the --split , --target-file-bytes , and --target-file-datums flags. Note In this section, a chunk of data is called a split-file . Flag Description --split Divides a file into chunks based on a record , such as newlines in a line-delimited files or by JSON object for JSON files. The --split flag takes one of the following arguments\u2014 line , json , or sql . For example, --split line ensures that Pachyderm only breaks up a file on a newline boundaries and not in the middle of a line. --target-file-bytes This flag must be used with the --split flag. The --target-file-bytes flag fills each of the split-files with data up to the specified number of bytes, splitting on the nearest record boundary. For example, you have a line-delimited file of 50 lines, with each line having about 20 bytes. If you run the --split lines --target-file-bytes 100 command, you see the input file split into about 10 files and each file has about 5 lines. Each split-file\u2019s size hovers above the target value of 100 bytes, not going below 100 bytes until the last split-file, which might be less than 100 bytes. --target-file-datums This flag must be used with the --split flag. The --target-file-datums attempts to fill each split-file with the specified number of datums. If you run --split lines --target-file-datums 2 on the line-delimited 100-line file mentioned above, you see the file split into 50 split-files and each file has 2 lines. If you specify both --target-file-datums and --target-file-bytes flags, Pachyderm creates split-files until it hits one of the constraints. See also: Splitting Data for Distributed Processing Example: Splitting a File \u00b6 In this example, you have a 50-line file called my-data.txt . You create a repository named line-data and load my-data.txt into that repository. Then, you can analyze how the data is split in the repository. To complete this example, perform the following steps: Create a file with fifty lines named my-data.txt . You can add random lines, such as numbers from one to fifty, or US states, or anything else. Examples: Zero One Two Three ... Fifty Create a repository called line-data : $ pachctl create repo line-data $ pachctl put file line-data@master -f my-data.txt --split line List the filesystem objects in the repository: $ pachctl list file line-data@master NAME TYPE SIZE /my-data.txt dir 1 .071KiB The pachctl list file command shows that the line-oriented file my-data.txt that was uploaded has been transformed into a directory that includes the chunks of the original my-data.txt file. Each chunk is put into a split-file and given a 16-character filename, left-padded with 0. Pachyderm numbers each filename sequentially in hexadecimal. We modify the command to list the contents of \u201cmy-data.txt\u201d, and the output reveals the naming structure. List the files in the my-data.txt directory: $ pachctl list file line-data@master my-data.txt NAME TYPE SIZE /my-data.txt/0000000000000000 file 21B /my-data.txt/0000000000000001 file 22B /my-data.txt/0000000000000002 file 24B /my-data.txt/0000000000000003 file 21B ... NAME TYPE SIZE /my-data.txt/0000000000000031 file 22B Example: Appending to files with -\u2013split \u00b6 Combining --split with the default append behavior of pachctl put file enables flexible and scalable processing of record-oriented file data from external, legacy systems. Pachyderm ensures that only the newly added data gets processed when you append to an existing file by using --split . Pachyderm optimizes storage utilization by automatically deduplicating each split-file. If you split a large file with many duplicate lines or objects with identical hashes might use less space in PFS than it does as a single file outside of PFS. To complete this example, follow these steps: Create a file count.txt with the following lines: Zero One Two Three Four Put the count.txt file into a Pachyderm repository called raw_data : $ pachctl put file -f count.txt raw_data@master --split line This command splits the count.txt file by line and creates a separate file with one line in each file. Also, this operation creates five datums that are processed by the pipelines that use this repository as input. View the repository contents: $ pachctl list file raw_data@master NAME TYPE SIZE /count.txt dir 24B Pachyderm created a directory called count.txt . View the contents of the count.txt directory: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 4B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 6B /count.txt/0000000000000003 file 5B /count.txt/0000000000000004 file 5B In the output above, you can see that Pachyderm created five split-files from the original count.txt file. Each file has one line from the original count.txt . You can check the contents of each file by running the pachctl get file command. For example, to get the contents of count.txt/0000000000000000 , run the following command: $ pachctl get file raw_data@master:count.txt/0000000000000000 Zero This operation creates five datums that are processed by the pipelines that use this repository as input. Create a one-line file called more-count.txt with the following content: Five Load this file into Pachyderm by appending it to the count.txt file: $ pachctl put file raw_data@master:count.txt -f more-count.txt --split line If you do not specify --split flag while appending to a file that was previously split, Pachyderm displays the following error message: could not put file at \"/count.txt\" ; a file of type directory is already there Verify that another file was added: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 4B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 6B /count.txt/0000000000000003 file 5B /count.txt/0000000000000004 file 5B /count.txt/0000000000000005 file 4B The /count.txt/0000000000000005 file was added to the input repository. Pachyderm considers this new file as a separate datum. Therefore, pipelines process only that datum instead of all the chunks of count.txt . Get the contents of the /count.txt/0000000000000005 file: $ pachctl get file raw_data@master:count.txt/0000000000000005 Five Example: Overwriting Files with \u2013-split \u00b6 The behavior of Pachyderm when a file loaded with --split is overwritten is simple to explain but subtle in its implications. Most importantly, it can have major implications when new rows are inserted within the file as opposed to just being appended to the end. The loaded file is split into those sequentially-named files, as shown above. If any of those resulting split-files hashes differently than the one it is replacing, it causes the Pachyderm Pipeline System to process that data. This can have significant consequences for downstream processing. To complete this example, follow these steps: Create a file count.txt with the following lines: One Two Three Four Five Put the file into a Pachyderm repository called raw_data : $ pachctl put file -f count.txt raw_data@master --split line This command splits the count.txt file by line and creates a separate file with one line in each file. Also, this operation creates five datums that are processed by the pipelines that use this repository as input. View the repository contents: $ pachctl list file raw_data@master NAME TYPE SIZE /count.txt dir 24B Pachyderm created a directory called count.txt . View the contents of the count.txt directory: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 4B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 6B /count.txt/0000000000000003 file 5B /count.txt/0000000000000004 file 5B In the output above, you can see that Pachyderm created five split-files from the original count.txt file. Each file has one line from the original count.txt file. You can check the contents of each file by running the pachctl get file command. For example, to get the contents of count.txt/0000000000000000 , run the following command: $ pachctl get file raw_data@master:count.txt/0000000000000000 One In your local directory, modify the original count.txt file by inserting the word Zero on the first line: Zero One Two Three Four Five Upload the updated count.txt file into the raw_data repository by using the --split and --overwrite flags: $ pachctl put file -f count.txt raw_data@master:count.txt --split line --overwrite Because Pachyderm takes the file name into account when hashing data for a pipeline, it considers every single split-file as new, and the pipelines that use this repository as input process all six datums. List the files in the directory: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 5B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 4B /count.txt/0000000000000003 file 6B /count.txt/0000000000000004 file 5B /count.txt/0000000000000005 file 5B The /count.txt/0000000000000000 file now has the newly added Zero line. To verify the contents of the file, run: $ pachctl get file raw_data@master:count.txt/0000000000000000 Zero See also: Splitting Data","title":"Adjusting Data Processing by Splitting Data"},{"location":"how-tos/splitting-data/adjusting_data_processing_w_split/#adjusting-data-processing-by-splitting-data","text":"Info Before you read this section, make sure that you understand the concepts described in File , Glob Pattern , Pipeline Specification , and Individual Developer Workflow . Unlike source code version-control systems, such as Git, that mostly store and version text files, Pachyderm does not perform intra-file diffing. This means that if you have a 10,000 lines CSV file and change a single line in that file, a pipeline that is subscribed to the repository where that file is located processes the whole file. You can adjust this behavior by splitting your file upon loading into chunks. Pachyderm applies diffing at per file level. Therefore, if one bit of a file changes, Pachyderm identifies that change as a new file. Similarly, Pachyderm can only distribute computation at the level of a single file. If your data is stored in one large file, it can only be processed by a single worker, which might affect performance. To optimize performance and processing time, you might want to break up large files into smaller chunks while Pachyderm uploads data to an input repository. For simple data types, you can run the pachctl put file command with the --split flag. For more complex splitting pattern, such as when you work with .avro or other binary formats, you need to manually split your data either at ingest or by configuring splitting in a Pachyderm pipeline.","title":"Adjusting Data Processing by Splitting Data"},{"location":"how-tos/splitting-data/adjusting_data_processing_w_split/#using-split-and-target-file-flags","text":"For common file types that are often used in data science, such as CSV, line-delimited text files, JavaScript Object Notation (JSON) files, Pachyderm includes the --split , --target-file-bytes , and --target-file-datums flags. Note In this section, a chunk of data is called a split-file . Flag Description --split Divides a file into chunks based on a record , such as newlines in a line-delimited files or by JSON object for JSON files. The --split flag takes one of the following arguments\u2014 line , json , or sql . For example, --split line ensures that Pachyderm only breaks up a file on a newline boundaries and not in the middle of a line. --target-file-bytes This flag must be used with the --split flag. The --target-file-bytes flag fills each of the split-files with data up to the specified number of bytes, splitting on the nearest record boundary. For example, you have a line-delimited file of 50 lines, with each line having about 20 bytes. If you run the --split lines --target-file-bytes 100 command, you see the input file split into about 10 files and each file has about 5 lines. Each split-file\u2019s size hovers above the target value of 100 bytes, not going below 100 bytes until the last split-file, which might be less than 100 bytes. --target-file-datums This flag must be used with the --split flag. The --target-file-datums attempts to fill each split-file with the specified number of datums. If you run --split lines --target-file-datums 2 on the line-delimited 100-line file mentioned above, you see the file split into 50 split-files and each file has 2 lines. If you specify both --target-file-datums and --target-file-bytes flags, Pachyderm creates split-files until it hits one of the constraints. See also: Splitting Data for Distributed Processing","title":"Using Split and Target-File Flags"},{"location":"how-tos/splitting-data/adjusting_data_processing_w_split/#example-splitting-a-file","text":"In this example, you have a 50-line file called my-data.txt . You create a repository named line-data and load my-data.txt into that repository. Then, you can analyze how the data is split in the repository. To complete this example, perform the following steps: Create a file with fifty lines named my-data.txt . You can add random lines, such as numbers from one to fifty, or US states, or anything else. Examples: Zero One Two Three ... Fifty Create a repository called line-data : $ pachctl create repo line-data $ pachctl put file line-data@master -f my-data.txt --split line List the filesystem objects in the repository: $ pachctl list file line-data@master NAME TYPE SIZE /my-data.txt dir 1 .071KiB The pachctl list file command shows that the line-oriented file my-data.txt that was uploaded has been transformed into a directory that includes the chunks of the original my-data.txt file. Each chunk is put into a split-file and given a 16-character filename, left-padded with 0. Pachyderm numbers each filename sequentially in hexadecimal. We modify the command to list the contents of \u201cmy-data.txt\u201d, and the output reveals the naming structure. List the files in the my-data.txt directory: $ pachctl list file line-data@master my-data.txt NAME TYPE SIZE /my-data.txt/0000000000000000 file 21B /my-data.txt/0000000000000001 file 22B /my-data.txt/0000000000000002 file 24B /my-data.txt/0000000000000003 file 21B ... NAME TYPE SIZE /my-data.txt/0000000000000031 file 22B","title":"Example: Splitting a File"},{"location":"how-tos/splitting-data/adjusting_data_processing_w_split/#example-appending-to-files-with-split","text":"Combining --split with the default append behavior of pachctl put file enables flexible and scalable processing of record-oriented file data from external, legacy systems. Pachyderm ensures that only the newly added data gets processed when you append to an existing file by using --split . Pachyderm optimizes storage utilization by automatically deduplicating each split-file. If you split a large file with many duplicate lines or objects with identical hashes might use less space in PFS than it does as a single file outside of PFS. To complete this example, follow these steps: Create a file count.txt with the following lines: Zero One Two Three Four Put the count.txt file into a Pachyderm repository called raw_data : $ pachctl put file -f count.txt raw_data@master --split line This command splits the count.txt file by line and creates a separate file with one line in each file. Also, this operation creates five datums that are processed by the pipelines that use this repository as input. View the repository contents: $ pachctl list file raw_data@master NAME TYPE SIZE /count.txt dir 24B Pachyderm created a directory called count.txt . View the contents of the count.txt directory: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 4B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 6B /count.txt/0000000000000003 file 5B /count.txt/0000000000000004 file 5B In the output above, you can see that Pachyderm created five split-files from the original count.txt file. Each file has one line from the original count.txt . You can check the contents of each file by running the pachctl get file command. For example, to get the contents of count.txt/0000000000000000 , run the following command: $ pachctl get file raw_data@master:count.txt/0000000000000000 Zero This operation creates five datums that are processed by the pipelines that use this repository as input. Create a one-line file called more-count.txt with the following content: Five Load this file into Pachyderm by appending it to the count.txt file: $ pachctl put file raw_data@master:count.txt -f more-count.txt --split line If you do not specify --split flag while appending to a file that was previously split, Pachyderm displays the following error message: could not put file at \"/count.txt\" ; a file of type directory is already there Verify that another file was added: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 4B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 6B /count.txt/0000000000000003 file 5B /count.txt/0000000000000004 file 5B /count.txt/0000000000000005 file 4B The /count.txt/0000000000000005 file was added to the input repository. Pachyderm considers this new file as a separate datum. Therefore, pipelines process only that datum instead of all the chunks of count.txt . Get the contents of the /count.txt/0000000000000005 file: $ pachctl get file raw_data@master:count.txt/0000000000000005 Five","title":"Example: Appending to files with -\u2013split"},{"location":"how-tos/splitting-data/adjusting_data_processing_w_split/#example-overwriting-files-with-split","text":"The behavior of Pachyderm when a file loaded with --split is overwritten is simple to explain but subtle in its implications. Most importantly, it can have major implications when new rows are inserted within the file as opposed to just being appended to the end. The loaded file is split into those sequentially-named files, as shown above. If any of those resulting split-files hashes differently than the one it is replacing, it causes the Pachyderm Pipeline System to process that data. This can have significant consequences for downstream processing. To complete this example, follow these steps: Create a file count.txt with the following lines: One Two Three Four Five Put the file into a Pachyderm repository called raw_data : $ pachctl put file -f count.txt raw_data@master --split line This command splits the count.txt file by line and creates a separate file with one line in each file. Also, this operation creates five datums that are processed by the pipelines that use this repository as input. View the repository contents: $ pachctl list file raw_data@master NAME TYPE SIZE /count.txt dir 24B Pachyderm created a directory called count.txt . View the contents of the count.txt directory: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 4B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 6B /count.txt/0000000000000003 file 5B /count.txt/0000000000000004 file 5B In the output above, you can see that Pachyderm created five split-files from the original count.txt file. Each file has one line from the original count.txt file. You can check the contents of each file by running the pachctl get file command. For example, to get the contents of count.txt/0000000000000000 , run the following command: $ pachctl get file raw_data@master:count.txt/0000000000000000 One In your local directory, modify the original count.txt file by inserting the word Zero on the first line: Zero One Two Three Four Five Upload the updated count.txt file into the raw_data repository by using the --split and --overwrite flags: $ pachctl put file -f count.txt raw_data@master:count.txt --split line --overwrite Because Pachyderm takes the file name into account when hashing data for a pipeline, it considers every single split-file as new, and the pipelines that use this repository as input process all six datums. List the files in the directory: $ pachctl list file raw_data@master:count.txt NAME TYPE SIZE /count.txt/0000000000000000 file 5B /count.txt/0000000000000001 file 4B /count.txt/0000000000000002 file 4B /count.txt/0000000000000003 file 6B /count.txt/0000000000000004 file 5B /count.txt/0000000000000005 file 5B The /count.txt/0000000000000000 file now has the newly added Zero line. To verify the contents of the file, run: $ pachctl get file raw_data@master:count.txt/0000000000000000 Zero See also: Splitting Data","title":"Example: Overwriting Files with \u2013-split"},{"location":"how-tos/splitting-data/splitting/","text":"Splitting Data for Distributed Processing \u00b6 Before you read this section, make sure that you understand the concepts described in Distributed Computing . Pachyderm enables you to parallelize computations over data as long as that data can be split up into multiple datums . However, in many cases, you might have a dataset that you want or need to commit into Pachyderm as a single file rather than a bunch of smaller files that are easily mapped to datums, such as one file per record. For such cases, Pachyderm provides an easy way to prepare your dataset for subsequent distributed computing by splitting it upon uploading to a Pachyderm repository. In this example, you have a dataset that consists of information about your users and a repository called user . This data is in CSV format in a single file called user_data.csv with one record per line: $ head user_data.csv 1,cyukhtin0@stumbleupon.com,144.155.176.12 2,csisneros1@over-blog.com,26.119.26.5 3,jeye2@instagram.com,13.165.230.106 4,rnollet3@hexun.com,58.52.147.83 5,bposkitt4@irs.gov,51.247.120.167 6,vvenmore5@hubpages.com,161.189.245.212 7,lcoyte6@ask.com,56.13.147.134 8,atuke7@psu.edu,78.178.247.163 9,nmorrell8@howstuffworks.com,28.172.10.170 10,afynn9@google.com.au,166.14.112.65 If you put this data into Pachyderm as a single file, Pachyderm processes them a single datum. It cannot process each of these user records in parallel as separate datums . Potentially, you can manually separate these user records into standalone files before you commit them into the users repository or through a pipeline stage dedicated to this splitting task. However, Pachyderm provides an optimized way of completing this task. The put file API includes an option for splitting the file into separate datums automatically. You can use the --split flag with the put file command. To complete this example, follow the steps below: Create a users repository by running: $ pachctl create repo users Create a file called user_data.csv with the contents listed above. Put your user_data.csv file into Pachyderm and automatically split it into separate datums for each line: $ pachctl put file users@master -f user_data.csv --split line --target-file-datums 1 The --split line argument specifies that Pachyderm splits this file into lines, and the --target-file-datums 1 argument specifies that each resulting file must include at most one datum or one line. View the list of files in the master branch of the users repository: $ pachctl list file users@master NAME TYPE SIZE user_data.csv dir 5 .346 KiB If you run pachctl list file command for the master branch in the users repository, Pachyderm still shows the user_data.csv entity to you as one entity in the repo However, this entity is now a directory that contains all of the split records. To view the detailed information about the user_data.csv file, run the command with the file name specified after a colon: $ pachctl list file users@master:user_data.csv NAME TYPE SIZE user_data.csv/0000000000000000 file 43 B user_data.csv/0000000000000001 file 39 B user_data.csv/0000000000000002 file 37 B user_data.csv/0000000000000003 file 34 B user_data.csv/0000000000000004 file 35 B user_data.csv/0000000000000005 file 41 B user_data.csv/0000000000000006 file 32 B etc... Then, a pipeline that takes the repo users as input with a glob pattern of /user_data.csv/* processes each user record, such as each line in the CSV file in parallel. JSON and Text File Splitting Examples \u00b6 Pachyderm supports this type of splitting for lines or JSON blobs as well. See the examples below. Split a json file on json blobs by putting each json blob into a separate file. $ pachctl put file users@master -f user_data.json --split json --target-file-datums 1 Split a json file on json blobs by putting three json blobs into each split file. $ pachctl put file users@master -f user_data.json --split json --target-file-datums 3 Split a file on lines by putting each 100-bytes chunk into the split files. $ pachctl put file users@master -f user_data.txt --split line --target-file-bytes 100 Specifying a Header \u00b6 If your data has a common header, you can specify it manually by using pachctl put file with the --header-records flag. You can use this functionality with JSON and CSV data. To specify a header, complete the following steps: Create a new or use an existing data file. For example, the user_data.csv from the section above with the following header: NUMBER,EMAIL,IP_ADDRESS Create a new repository or use an existing one: $ pachctl create repo users Put your file into the repository by separating the header from other lines: $ pachctl put file users@master -f user_data.csv --split = csv --header-records = 1 --target-file-datums = 1 Verify that the file was added and split: $ pachctl list file users@master:/user_data.csv Example: NAME TYPE SIZE /user_data.csv/0000000000000000 file 70B /user_data.csv/0000000000000001 file 66B /user_data.csv/0000000000000002 file 64B /user_data.csv/0000000000000003 file 61B /user_data.csv/0000000000000004 file 62B /user_data.csv/0000000000000005 file 68B /user_data.csv/0000000000000006 file 59B /user_data.csv/0000000000000007 file 59B /user_data.csv/0000000000000008 file 71B /user_data.csv/0000000000000009 file 65B Get the first file from the repository: $ pachctl get file users@master:/user_data.csv/0000000000000000 NUMBER,EMAIL,IP_ADDRESS 1 ,cyukhtin0@stumbleupon.com,144.155.176.12 1. Get all files: $ pachctl get file users@master:/user_data.csv/* NUMBER,EMAIL,IP_ADDRESS 1,cyukhtin0@stumbleupon.com,144.155.176.12 2,csisneros1@over-blog.com,26.119.26.5 3,jeye2@instagram.com,13.165.230.106 4,rnollet3@hexun.com,58.52.147.83 5,bposkitt4@irs.gov,51.247.120.167 6,vvenmore5@hubpages.com,161.189.245.212 7,lcoyte6@ask.com,56.13.147.134 8,atuke7@psu.edu,78.178.247.163 9,nmorrell8@howstuffworks.com,28.172.10.170 10,afynn9@google.com.au,166.14.112.65 For more information, type pachctl put file --help . Ingesting PostgresSQL data \u00b6 Pachyderm supports direct data ingestion from PostgreSQL. You need first extract your database into a script file by using pg_dump and then add the data from the file into Pachyderm by running the pachctl put file with the --split flag. When you use pachctl put file --split sql ... , Pachyderm splits your pgdump file into three parts - the header, rows, and the footer. The header contains all the SQL statements in the pgdump file that set up the schema and tables. The rows are split into individual files, or if you specify the --target-file-datums or --target-file-bytes , multiple rows per file. The footer contains the remaining SQL statements for setting up the tables. The header and footer are stored in the directory that contains the rows. If you request a get file on that directory, you get just the header and footer. If you request an individual file, you see the header, the row or rows, and the footer. If you request all the files with a glob pattern, for example, /directoryname/* , you receive the header, all the rows, and the footer recreating the full pgdump . Therefore, you can construct full or partial pgdump files so that you can load full or partial datasets. To put your PostgreSQL data into Pachyderm, complete the following steps: Generate a pgdump file: Example: $ pg_dump -t users -f users.pgdump View the pgdump file: Example $ cat users.pgdump -- -- PostgreSQL database dump -- -- Dumped from database version 9 .5.12 -- Dumped by pg_dump version 9 .5.12 SET statement_timeout = 0 ; SET lock_timeout = 0 ; SET client_encoding = 'UTF8' ; SET standard_conforming_strings = on ; SELECT pg_catalog.set_config ( 'search_path' , '' , false ) ; SET check_function_bodies = false ; SET client_min_messages = warning ; SET row_security = off ; SET default_tablespace = '' ; SET default_with_oids = false ; -- -- Name: users ; Type: TABLE ; Schema: public ; Owner: postgres -- CREATE TABLE public.users ( id integer NOT NULL, name text NOT NULL, saying text NOT NULL ) ; ALTER TABLE public.users OWNER TO postgres ; -- -- Data for Name: users ; Type: TABLE DATA ; Schema: public ; Owner: postgres -- COPY public.users ( id, name, saying ) FROM stdin ; 0 wile E Coyote ... 1 road runner \\\\ . \\. -- -- PostgreSQL database dump complete -- Ingest the SQL data by using the pachctl put file command with the --split file: $ pachctl put file data@master -f users.pgdump --split sql $ pachctl put file data@master:users --split sql -f users.pgdump View the information about your repository: $ pachctl list file data@master NAME TYPE SIZE users dir 914B The users.pgdump file is added to the master branch in the data repository. View the information about the users.pgdump file: $ pachctl list file data@master:users NAME TYPE SIZE /users/0000000000000000 file 20B /users/0000000000000001 file 18B In your pipeline, where you have started and forked PostgreSQL, you can load the data by running the following or a similar script: $ cat /pfs/data/users/* | sudo -u postgres psql By using the glob pattern /* , this code loads each raw PostgreSQL chunk into your PostgreSQL instance for processing by your pipeline. Tip For this use case, you might want to use --target-file-datums or --target-file-bytes because these commands enable your queries to run against many rows at a time.","title":"Splitting Data for Distributed Processing"},{"location":"how-tos/splitting-data/splitting/#splitting-data-for-distributed-processing","text":"Before you read this section, make sure that you understand the concepts described in Distributed Computing . Pachyderm enables you to parallelize computations over data as long as that data can be split up into multiple datums . However, in many cases, you might have a dataset that you want or need to commit into Pachyderm as a single file rather than a bunch of smaller files that are easily mapped to datums, such as one file per record. For such cases, Pachyderm provides an easy way to prepare your dataset for subsequent distributed computing by splitting it upon uploading to a Pachyderm repository. In this example, you have a dataset that consists of information about your users and a repository called user . This data is in CSV format in a single file called user_data.csv with one record per line: $ head user_data.csv 1,cyukhtin0@stumbleupon.com,144.155.176.12 2,csisneros1@over-blog.com,26.119.26.5 3,jeye2@instagram.com,13.165.230.106 4,rnollet3@hexun.com,58.52.147.83 5,bposkitt4@irs.gov,51.247.120.167 6,vvenmore5@hubpages.com,161.189.245.212 7,lcoyte6@ask.com,56.13.147.134 8,atuke7@psu.edu,78.178.247.163 9,nmorrell8@howstuffworks.com,28.172.10.170 10,afynn9@google.com.au,166.14.112.65 If you put this data into Pachyderm as a single file, Pachyderm processes them a single datum. It cannot process each of these user records in parallel as separate datums . Potentially, you can manually separate these user records into standalone files before you commit them into the users repository or through a pipeline stage dedicated to this splitting task. However, Pachyderm provides an optimized way of completing this task. The put file API includes an option for splitting the file into separate datums automatically. You can use the --split flag with the put file command. To complete this example, follow the steps below: Create a users repository by running: $ pachctl create repo users Create a file called user_data.csv with the contents listed above. Put your user_data.csv file into Pachyderm and automatically split it into separate datums for each line: $ pachctl put file users@master -f user_data.csv --split line --target-file-datums 1 The --split line argument specifies that Pachyderm splits this file into lines, and the --target-file-datums 1 argument specifies that each resulting file must include at most one datum or one line. View the list of files in the master branch of the users repository: $ pachctl list file users@master NAME TYPE SIZE user_data.csv dir 5 .346 KiB If you run pachctl list file command for the master branch in the users repository, Pachyderm still shows the user_data.csv entity to you as one entity in the repo However, this entity is now a directory that contains all of the split records. To view the detailed information about the user_data.csv file, run the command with the file name specified after a colon: $ pachctl list file users@master:user_data.csv NAME TYPE SIZE user_data.csv/0000000000000000 file 43 B user_data.csv/0000000000000001 file 39 B user_data.csv/0000000000000002 file 37 B user_data.csv/0000000000000003 file 34 B user_data.csv/0000000000000004 file 35 B user_data.csv/0000000000000005 file 41 B user_data.csv/0000000000000006 file 32 B etc... Then, a pipeline that takes the repo users as input with a glob pattern of /user_data.csv/* processes each user record, such as each line in the CSV file in parallel.","title":"Splitting Data for Distributed Processing"},{"location":"how-tos/splitting-data/splitting/#json-and-text-file-splitting-examples","text":"Pachyderm supports this type of splitting for lines or JSON blobs as well. See the examples below. Split a json file on json blobs by putting each json blob into a separate file. $ pachctl put file users@master -f user_data.json --split json --target-file-datums 1 Split a json file on json blobs by putting three json blobs into each split file. $ pachctl put file users@master -f user_data.json --split json --target-file-datums 3 Split a file on lines by putting each 100-bytes chunk into the split files. $ pachctl put file users@master -f user_data.txt --split line --target-file-bytes 100","title":"JSON and Text File Splitting Examples"},{"location":"how-tos/splitting-data/splitting/#specifying-a-header","text":"If your data has a common header, you can specify it manually by using pachctl put file with the --header-records flag. You can use this functionality with JSON and CSV data. To specify a header, complete the following steps: Create a new or use an existing data file. For example, the user_data.csv from the section above with the following header: NUMBER,EMAIL,IP_ADDRESS Create a new repository or use an existing one: $ pachctl create repo users Put your file into the repository by separating the header from other lines: $ pachctl put file users@master -f user_data.csv --split = csv --header-records = 1 --target-file-datums = 1 Verify that the file was added and split: $ pachctl list file users@master:/user_data.csv Example: NAME TYPE SIZE /user_data.csv/0000000000000000 file 70B /user_data.csv/0000000000000001 file 66B /user_data.csv/0000000000000002 file 64B /user_data.csv/0000000000000003 file 61B /user_data.csv/0000000000000004 file 62B /user_data.csv/0000000000000005 file 68B /user_data.csv/0000000000000006 file 59B /user_data.csv/0000000000000007 file 59B /user_data.csv/0000000000000008 file 71B /user_data.csv/0000000000000009 file 65B Get the first file from the repository: $ pachctl get file users@master:/user_data.csv/0000000000000000 NUMBER,EMAIL,IP_ADDRESS 1 ,cyukhtin0@stumbleupon.com,144.155.176.12 1. Get all files: $ pachctl get file users@master:/user_data.csv/* NUMBER,EMAIL,IP_ADDRESS 1,cyukhtin0@stumbleupon.com,144.155.176.12 2,csisneros1@over-blog.com,26.119.26.5 3,jeye2@instagram.com,13.165.230.106 4,rnollet3@hexun.com,58.52.147.83 5,bposkitt4@irs.gov,51.247.120.167 6,vvenmore5@hubpages.com,161.189.245.212 7,lcoyte6@ask.com,56.13.147.134 8,atuke7@psu.edu,78.178.247.163 9,nmorrell8@howstuffworks.com,28.172.10.170 10,afynn9@google.com.au,166.14.112.65 For more information, type pachctl put file --help .","title":"Specifying a Header"},{"location":"how-tos/splitting-data/splitting/#ingesting-postgressql-data","text":"Pachyderm supports direct data ingestion from PostgreSQL. You need first extract your database into a script file by using pg_dump and then add the data from the file into Pachyderm by running the pachctl put file with the --split flag. When you use pachctl put file --split sql ... , Pachyderm splits your pgdump file into three parts - the header, rows, and the footer. The header contains all the SQL statements in the pgdump file that set up the schema and tables. The rows are split into individual files, or if you specify the --target-file-datums or --target-file-bytes , multiple rows per file. The footer contains the remaining SQL statements for setting up the tables. The header and footer are stored in the directory that contains the rows. If you request a get file on that directory, you get just the header and footer. If you request an individual file, you see the header, the row or rows, and the footer. If you request all the files with a glob pattern, for example, /directoryname/* , you receive the header, all the rows, and the footer recreating the full pgdump . Therefore, you can construct full or partial pgdump files so that you can load full or partial datasets. To put your PostgreSQL data into Pachyderm, complete the following steps: Generate a pgdump file: Example: $ pg_dump -t users -f users.pgdump View the pgdump file: Example $ cat users.pgdump -- -- PostgreSQL database dump -- -- Dumped from database version 9 .5.12 -- Dumped by pg_dump version 9 .5.12 SET statement_timeout = 0 ; SET lock_timeout = 0 ; SET client_encoding = 'UTF8' ; SET standard_conforming_strings = on ; SELECT pg_catalog.set_config ( 'search_path' , '' , false ) ; SET check_function_bodies = false ; SET client_min_messages = warning ; SET row_security = off ; SET default_tablespace = '' ; SET default_with_oids = false ; -- -- Name: users ; Type: TABLE ; Schema: public ; Owner: postgres -- CREATE TABLE public.users ( id integer NOT NULL, name text NOT NULL, saying text NOT NULL ) ; ALTER TABLE public.users OWNER TO postgres ; -- -- Data for Name: users ; Type: TABLE DATA ; Schema: public ; Owner: postgres -- COPY public.users ( id, name, saying ) FROM stdin ; 0 wile E Coyote ... 1 road runner \\\\ . \\. -- -- PostgreSQL database dump complete -- Ingest the SQL data by using the pachctl put file command with the --split file: $ pachctl put file data@master -f users.pgdump --split sql $ pachctl put file data@master:users --split sql -f users.pgdump View the information about your repository: $ pachctl list file data@master NAME TYPE SIZE users dir 914B The users.pgdump file is added to the master branch in the data repository. View the information about the users.pgdump file: $ pachctl list file data@master:users NAME TYPE SIZE /users/0000000000000000 file 20B /users/0000000000000001 file 18B In your pipeline, where you have started and forked PostgreSQL, you can load the data by running the following or a similar script: $ cat /pfs/data/users/* | sudo -u postgres psql By using the glob pattern /* , this code loads each raw PostgreSQL chunk into your PostgreSQL instance for processing by your pipeline. Tip For this use case, you might want to use --target-file-datums or --target-file-bytes because these commands enable your queries to run against many rows at a time.","title":"Ingesting PostgresSQL data"},{"location":"pachub/pachub_getting_started/","text":"Getting Started with Pachyderm Hub \u00b6 Pachyderm Hub is a platform for data scientists where you can version-control your data, build analysis pipelines, and track the provenance of your data science workflow. This section walks you through the steps of creating a cluster in Pachyderm Hub so that you do not need to worry about the underlying infrastructure and can get started using Pachyderm right away. Pachyderm Hub enables you to preview Pachyderm functionality free of charge by removing the burden of deploying Pachyderm locally or in a third-party cloud platform. Currently, Pachyderm Hub is in beta so clusters cannot be turned into production clusters and should only be used for easy development and testing. Production-grade functionality will be supported in later releases. Note: We'd like to hear your feedback! Let us know what you think about Pachyderm Hub and help us make it better. Join our Slack channel . How it Works \u00b6 To get started, complete the following steps: Log in \u00b6 Pachyderm Hub uses GitHub OAuth as an identity provider. Therefore, to start using Pachyderm Hub, you need to log in by authorizing Pachyderm Hub with your GitHub account. If you do not have a GitHub account yet, create one by following the steps described in Join GitHub . To log in to Pachyderm Hub, complete the following steps: Go to hub.pachyderm.com . Click Try for free . Authorize Pachyderm Hub with your GitHub account by typing your GitHub user name and password. Proceed to Step 1 . Step 1: Create a Cluster \u00b6 To get started, create a Pachyderm cluster on which your pipelines will run. A Pachyderm cluster runs on top of the underlying cloud infrastructure. In Pachyderm Hub, you can create a one-node cluster that you can use for a limited time. To create a Pachyderm cluster, complete the following steps: If you have not yet done so, log in to Pachyderm Hub. Click Create . Type a name for your cluster. For example, test1 . Click Create . Your cluster is provisioned instantly! Note: Pachyderm has a set number of pre-warmed clusters. If you see your cluster is in a starting state, you might have to wait a few minutes for it to be ready. Proceed to Step 2 . Step 2 - Connect to Your Cluster \u00b6 Pachyderm Hub enables you to access your cluster through a command-line interface (CLI) called pachctl and the web interface called the Dashboard. Although you can perform most simple actions directly in the dashboard, pachctl provides full functionality. Most likely, you will use pachctl for any operation beyound the most basic workflow. recommends that you use pachctl for all data operations and the dashboard to view your data and graphical representation of your pipelines. After you create a cluster, you need to go to the terminal on your computer and configure your CLI to connect to your cluster by installing pachctl and configuring your Pachyderm context. For more information about Pachyderm contexts, see Connect by using a Pachyderm Context . Your pachctl version must match the version of the Pachyderm cluster that you deployed on Pachyderm Hub. Pachyderm Hub uses the latest release of Pachyderm so we recommend that you use the same version for pachctl . To set the correct Pachyderm context, you need to use the hostname of your cluster that is available in the Pachyderm Hub UI under Connect . Note: kubectl commands are not supported for the clusters deployed on Pachyderm Hub. To connect to your cluster, complete the following steps: On your local computer, open a terminal window. Install pachctl for your platform. For example, if you are using macOS, run: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 If you are using another operating system, see Install pachctl . If you already have pachctl installed, skip this step, or you might need to update your version of pachctl . For example, if you use macOS and brew , run: $ brew upgrade pachyderm/tap/pachctl@1.9 Verify your pachctl version: $ pachctl version --client-only 1 .9.7 Configure a Pachyderm context and log in to your cluster by using a one-time authentication token: In the Pachyderm Hub UI, click Connect next to your cluster. Copy, paste, and run the commands in the instructions in your terminal. These commands create a new Pachyderm context with your cluster details on your machine. Note: If you get the following error, that means that your authentication token has expired: error authenticating with Pachyderm cluster: /pachyderm_auth/auth-codes/ e14ccfafb35d4768f4a73b2dc9238b365492b88e98b76929d82ef0c6079e0027 not found To get a new token, refresh the page. Then, use the new token to authenticate. Verify that you have set the correct context: $ pachctl config get active-context Verify that you can run pachctl commands on your cluster: Create a repo called test : $ pachctl create repo test Verify that the repo was created: $ pachctl list repo NAME CREATED SIZE ( MASTER ) ACCESS LEVEL test 3 seconds ago 0B OWNER Go to the dashboard and verify that you can see the repo in the dashboard: 1. In the Pachyderm Hub UI, click Dashboard next to your cluster. The dashboard opens in a new window. Next Steps \u00b6 Congratulations! You have successfully deployed and configured a Pachyderm cluster in Pachyderm Hub. Now, you can try out our Beginners tutorial that walks you through the Pachyderm basics. Beginner Tutorial","title":"Getting Started with Pachyderm Hub"},{"location":"pachub/pachub_getting_started/#getting-started-with-pachyderm-hub","text":"Pachyderm Hub is a platform for data scientists where you can version-control your data, build analysis pipelines, and track the provenance of your data science workflow. This section walks you through the steps of creating a cluster in Pachyderm Hub so that you do not need to worry about the underlying infrastructure and can get started using Pachyderm right away. Pachyderm Hub enables you to preview Pachyderm functionality free of charge by removing the burden of deploying Pachyderm locally or in a third-party cloud platform. Currently, Pachyderm Hub is in beta so clusters cannot be turned into production clusters and should only be used for easy development and testing. Production-grade functionality will be supported in later releases. Note: We'd like to hear your feedback! Let us know what you think about Pachyderm Hub and help us make it better. Join our Slack channel .","title":"Getting Started with Pachyderm Hub"},{"location":"pachub/pachub_getting_started/#how-it-works","text":"To get started, complete the following steps:","title":"How it Works"},{"location":"pachub/pachub_getting_started/#log-in","text":"Pachyderm Hub uses GitHub OAuth as an identity provider. Therefore, to start using Pachyderm Hub, you need to log in by authorizing Pachyderm Hub with your GitHub account. If you do not have a GitHub account yet, create one by following the steps described in Join GitHub . To log in to Pachyderm Hub, complete the following steps: Go to hub.pachyderm.com . Click Try for free . Authorize Pachyderm Hub with your GitHub account by typing your GitHub user name and password. Proceed to Step 1 .","title":"Log in"},{"location":"pachub/pachub_getting_started/#step-1-create-a-cluster","text":"To get started, create a Pachyderm cluster on which your pipelines will run. A Pachyderm cluster runs on top of the underlying cloud infrastructure. In Pachyderm Hub, you can create a one-node cluster that you can use for a limited time. To create a Pachyderm cluster, complete the following steps: If you have not yet done so, log in to Pachyderm Hub. Click Create . Type a name for your cluster. For example, test1 . Click Create . Your cluster is provisioned instantly! Note: Pachyderm has a set number of pre-warmed clusters. If you see your cluster is in a starting state, you might have to wait a few minutes for it to be ready. Proceed to Step 2 .","title":"Step 1: Create a Cluster"},{"location":"pachub/pachub_getting_started/#step-2-connect-to-your-cluster","text":"Pachyderm Hub enables you to access your cluster through a command-line interface (CLI) called pachctl and the web interface called the Dashboard. Although you can perform most simple actions directly in the dashboard, pachctl provides full functionality. Most likely, you will use pachctl for any operation beyound the most basic workflow. recommends that you use pachctl for all data operations and the dashboard to view your data and graphical representation of your pipelines. After you create a cluster, you need to go to the terminal on your computer and configure your CLI to connect to your cluster by installing pachctl and configuring your Pachyderm context. For more information about Pachyderm contexts, see Connect by using a Pachyderm Context . Your pachctl version must match the version of the Pachyderm cluster that you deployed on Pachyderm Hub. Pachyderm Hub uses the latest release of Pachyderm so we recommend that you use the same version for pachctl . To set the correct Pachyderm context, you need to use the hostname of your cluster that is available in the Pachyderm Hub UI under Connect . Note: kubectl commands are not supported for the clusters deployed on Pachyderm Hub. To connect to your cluster, complete the following steps: On your local computer, open a terminal window. Install pachctl for your platform. For example, if you are using macOS, run: $ brew tap pachyderm/tap && brew install pachyderm/tap/pachctl@1.9 If you are using another operating system, see Install pachctl . If you already have pachctl installed, skip this step, or you might need to update your version of pachctl . For example, if you use macOS and brew , run: $ brew upgrade pachyderm/tap/pachctl@1.9 Verify your pachctl version: $ pachctl version --client-only 1 .9.7 Configure a Pachyderm context and log in to your cluster by using a one-time authentication token: In the Pachyderm Hub UI, click Connect next to your cluster. Copy, paste, and run the commands in the instructions in your terminal. These commands create a new Pachyderm context with your cluster details on your machine. Note: If you get the following error, that means that your authentication token has expired: error authenticating with Pachyderm cluster: /pachyderm_auth/auth-codes/ e14ccfafb35d4768f4a73b2dc9238b365492b88e98b76929d82ef0c6079e0027 not found To get a new token, refresh the page. Then, use the new token to authenticate. Verify that you have set the correct context: $ pachctl config get active-context Verify that you can run pachctl commands on your cluster: Create a repo called test : $ pachctl create repo test Verify that the repo was created: $ pachctl list repo NAME CREATED SIZE ( MASTER ) ACCESS LEVEL test 3 seconds ago 0B OWNER Go to the dashboard and verify that you can see the repo in the dashboard: 1. In the Pachyderm Hub UI, click Dashboard next to your cluster. The dashboard opens in a new window.","title":"Step 2 - Connect to Your Cluster"},{"location":"pachub/pachub_getting_started/#next-steps","text":"Congratulations! You have successfully deployed and configured a Pachyderm cluster in Pachyderm Hub. Now, you can try out our Beginners tutorial that walks you through the Pachyderm basics. Beginner Tutorial","title":"Next Steps"},{"location":"reference/clients/","text":"Pachyderm language clients \u00b6 Go Client \u00b6 The Go client is officially supported by the Pachyderm team. It implements almost all of the functionality that is provided with the pachctl CLI tool, and, thus, you can easily integrated operations like put file into your applications. For more info, check out the godocs . Note - A compatible version of grpc is needed when using the Go client. You can deduce the compatible version from our vendor.json file, where you will see something like: { \"checksumSHA1\": \"mEyChIkG797MtkrJQXW8X/qZ0l0=\", \"path\": \"google.golang.org/grpc\", \"revision\": \"21f8ed309495401e6fd79b3a9fd549582aed1b4c\", \"revisionTime\": \"2017-01-27T15:26:01Z\" }, You can then get this version via: go get google.golang.org/grpc cd $GOPATH/src/google.golang.org/grpc git checkout 21f8ed309495401e6fd79b3a9fd549582aed1b4c Python Client \u00b6 The Python client is officially supported by the Pachyderm team. It implements almost all of the functionality that is provided with the pachctl CLI tool, and, thus, you can easily integrated operations like put file into your applications. For more info, check out the repo . Scala Client \u00b6 Our users are currently working on a Scala client for Pachyderm. Please contact us if you are interested in helping with this or testing it out. Other languages \u00b6 Pachyderm uses a simple protocol buffer API . Protobufs support a bunch of other languages , any of which can be used to programmatically use Pachyderm. We haven\u2019t built clients for them yet, but it\u2019s not too hard. It\u2019s an easy way to contribute to Pachyderm if you\u2019re looking to get involved.","title":"Pachyderm Language Clients"},{"location":"reference/clients/#pachyderm-language-clients","text":"","title":"Pachyderm language clients"},{"location":"reference/clients/#go-client","text":"The Go client is officially supported by the Pachyderm team. It implements almost all of the functionality that is provided with the pachctl CLI tool, and, thus, you can easily integrated operations like put file into your applications. For more info, check out the godocs . Note - A compatible version of grpc is needed when using the Go client. You can deduce the compatible version from our vendor.json file, where you will see something like: { \"checksumSHA1\": \"mEyChIkG797MtkrJQXW8X/qZ0l0=\", \"path\": \"google.golang.org/grpc\", \"revision\": \"21f8ed309495401e6fd79b3a9fd549582aed1b4c\", \"revisionTime\": \"2017-01-27T15:26:01Z\" }, You can then get this version via: go get google.golang.org/grpc cd $GOPATH/src/google.golang.org/grpc git checkout 21f8ed309495401e6fd79b3a9fd549582aed1b4c","title":"Go Client"},{"location":"reference/clients/#python-client","text":"The Python client is officially supported by the Pachyderm team. It implements almost all of the functionality that is provided with the pachctl CLI tool, and, thus, you can easily integrated operations like put file into your applications. For more info, check out the repo .","title":"Python Client"},{"location":"reference/clients/#scala-client","text":"Our users are currently working on a Scala client for Pachyderm. Please contact us if you are interested in helping with this or testing it out.","title":"Scala Client"},{"location":"reference/clients/#other-languages","text":"Pachyderm uses a simple protocol buffer API . Protobufs support a bunch of other languages , any of which can be used to programmatically use Pachyderm. We haven\u2019t built clients for them yet, but it\u2019s not too hard. It\u2019s an easy way to contribute to Pachyderm if you\u2019re looking to get involved.","title":"Other languages"},{"location":"reference/config_spec/","text":"Config Specification \u00b6 This document outlines the fields in pachyderm configs. This should act as a reference. If you wish to change a config value, you should do so via pachctl config . JSON format \u00b6 { \"user_id\" : string , \"v2\" : { \"active_context\" : string , \"contexts\" : { string: { \"pachd_address\" : string , \"server_cas\" : string , \"session_token\" : string , \"active_transaction\" : string }, ... }, \"metrics\" : bool } } If a field is not set, it will be omitted from JSON entirely. Following is an example of a simple config: { \"user_id\" : \"514cbe16-e615-46fe-92d9-3156f12885d7\" , \"v2\" : { \"active_context\" : \"default\" , \"contexts\" : { \"default\" : {} }, \"metrics\" : true } } Following is a walk-through of all the fields. User ID \u00b6 A UUID giving a unique ID for this user for metrics. Metrics \u00b6 Whether metrics is enabled. Active Context \u00b6 v2.active_context specifies the name of the currently actively pachyderm context, as specified in v2.contexts . Contexts \u00b6 A map of context names to their configurations. Pachyderm contexts are akin to kubernetes contexts (and in fact reference the kubernetes context that they're associated with.) Pachd Address \u00b6 A host:port specification for connecting to pachd. If this is set, pachyderm will directly connect to the cluster, rather than resorting to kubernetes' port forwarding. If you can set this (because there's no firewall between you and the cluster), you should, as kubernetes' port forwarder is not designed to handle large amounts of data. Server CAs \u00b6 Trusted root certificates for the cluster, formatted as a base64-encoded PEM. This is only set when TLS is enabled. Session token \u00b6 A secret token identifying the current pachctl user within their pachyderm cluster. This is included in all RPCs sent by pachctl, and used to determine if pachctl actions are authorized. This is only set when auth is enabled. Active transaction \u00b6 The currently active transaction for batching together pachctl commands. This can be set or cleared via many of the pachctl * transaction commands.","title":"Pachyderm Config Specification"},{"location":"reference/config_spec/#config-specification","text":"This document outlines the fields in pachyderm configs. This should act as a reference. If you wish to change a config value, you should do so via pachctl config .","title":"Config Specification"},{"location":"reference/config_spec/#json-format","text":"{ \"user_id\" : string , \"v2\" : { \"active_context\" : string , \"contexts\" : { string: { \"pachd_address\" : string , \"server_cas\" : string , \"session_token\" : string , \"active_transaction\" : string }, ... }, \"metrics\" : bool } } If a field is not set, it will be omitted from JSON entirely. Following is an example of a simple config: { \"user_id\" : \"514cbe16-e615-46fe-92d9-3156f12885d7\" , \"v2\" : { \"active_context\" : \"default\" , \"contexts\" : { \"default\" : {} }, \"metrics\" : true } } Following is a walk-through of all the fields.","title":"JSON format"},{"location":"reference/config_spec/#user-id","text":"A UUID giving a unique ID for this user for metrics.","title":"User ID"},{"location":"reference/config_spec/#metrics","text":"Whether metrics is enabled.","title":"Metrics"},{"location":"reference/config_spec/#active-context","text":"v2.active_context specifies the name of the currently actively pachyderm context, as specified in v2.contexts .","title":"Active Context"},{"location":"reference/config_spec/#contexts","text":"A map of context names to their configurations. Pachyderm contexts are akin to kubernetes contexts (and in fact reference the kubernetes context that they're associated with.)","title":"Contexts"},{"location":"reference/config_spec/#pachd-address","text":"A host:port specification for connecting to pachd. If this is set, pachyderm will directly connect to the cluster, rather than resorting to kubernetes' port forwarding. If you can set this (because there's no firewall between you and the cluster), you should, as kubernetes' port forwarder is not designed to handle large amounts of data.","title":"Pachd Address"},{"location":"reference/config_spec/#server-cas","text":"Trusted root certificates for the cluster, formatted as a base64-encoded PEM. This is only set when TLS is enabled.","title":"Server CAs"},{"location":"reference/config_spec/#session-token","text":"A secret token identifying the current pachctl user within their pachyderm cluster. This is included in all RPCs sent by pachctl, and used to determine if pachctl actions are authorized. This is only set when auth is enabled.","title":"Session token"},{"location":"reference/config_spec/#active-transaction","text":"The currently active transaction for batching together pachctl commands. This can be set or cleared via many of the pachctl * transaction commands.","title":"Active transaction"},{"location":"reference/pipeline_spec/","text":"Pipeline Specification \u00b6 This document discusses each of the fields present in a pipeline specification. To see how to use a pipeline spec to create a pipeline, refer to the pachctl create pipeline section. JSON Manifest Format \u00b6 { \"pipeline\" : { \"name\" : string }, \"description\" : string , \"transform\" : { \"image\" : string , \"cmd\" : [ string ], \"stdin\" : [ string ], \"err_cmd\" : [ string ], \"err_stdin\" : [ string ], \"env\" : { string: string }, \"secrets\" : [ { \"name\" : string , \"mount_path\" : string }, { \"name\" : string , \"env_var\" : string , \"key\" : string } ], \"image_pull_secrets\" : [ string ], \"accept_return_code\" : [ int ], \"debug\" : bool , \"user\" : string , \"working_dir\" : string , }, \"parallelism_spec\" : { // Set at most one of the following: \"constant\" : int , \"coefficient\" : number }, \"hashtree_spec\" : { \"constant\" : int , }, \"resource_requests\" : { \"memory\" : string , \"cpu\" : number , \"disk\" : string , }, \"resource_limits\" : { \"memory\" : string , \"cpu\" : number , \"gpu\" : { \"type\" : string , \"number\" : int } \"disk\" : string , }, \"datum_timeout\" : string , \"datum_tries\" : int , \"job_timeout\" : string , \"input\" : { < \"pfs\" , \"cross\" , \"union\" , \"cron\" , or \"git\" see below> }, \"output_branch\" : string , \"egress\" : { \"URL\" : \"s3://bucket/dir\" }, \"standby\" : bool , \"cache_size\" : string , \"enable_stats\" : bool , \"service\" : { \"internal_port\" : int , \"external_port\" : int }, \"spout\" : { \"overwrite\" : bool \\\\ Optionally , you can combine a spout with a service: \"service\" : { \"internal_port\" : int , \"external_port\" : int , \"annotations\" : { \"foo\" : \"bar\" } } }, \"max_queue_size\" : int , \"chunk_spec\" : { \"number\" : int , \"size_bytes\" : int }, \"scheduling_spec\" : { \"node_selector\" : { string: string }, \"priority_class_name\" : string }, \"pod_spec\" : string , \"pod_patch\" : string , } ------------------------------------ \"pfs\" input ------------------------------------ \"pfs\" : { \"name\" : string , \"repo\" : string , \"branch\" : string , \"glob\" : string , \"lazy\" bool , \"empty_files\" : bool } ------------------------------------ \"cross\" or \"union\" input ------------------------------------ \"cross\" or \"union\" : [ { \"pfs\" : { \"name\" : string , \"repo\" : string , \"branch\" : string , \"glob\" : string , \"lazy\" bool , \"empty_files\" : bool } }, { \"pfs\" : { \"name\" : string , \"repo\" : string , \"branch\" : string , \"glob\" : string , \"lazy\" bool , \"empty_files\" : bool } } etc... ] ------------------------------------ \"cron\" input ------------------------------------ \"cron\" : { \"name\" : string , \"spec\" : string , \"repo\" : string , \"start\" : time , \"overwrite\" : bool } ------------------------------------ \"git\" input ------------------------------------ \"git\" : { \"URL\" : string , \"name\" : string , \"branch\" : string } In practice, you rarely need to specify all the fields. Most fields either come with sensible defaults or can be empty. The following text is an example of a minimum spec: { \"pipeline\" : { \"name\" : \"wordcount\" }, \"transform\" : { \"image\" : \"wordcount-image\" , \"cmd\" : [ \"/binary\" , \"/pfs/data\" , \"/pfs/out\" ] }, \"input\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } } } Name (required) \u00b6 pipeline.name is the name of the pipeline that you are creating. Each pipeline needs to have a unique name. Pipeline names must meet the following prerequisites: Include only alphanumeric characters, _ and - . Begin or end with only alphanumeric characters (not _ or - ). Not exceed 50 characters in length. Description (optional) \u00b6 description is an optional text field where you can add information about the pipeline. Transform (required) \u00b6 transform.image is the name of the Docker image that your jobs use. transform.cmd is the command passed to the Docker run invocation. Similarly to Docker, cmd is not run inside a shell which means that wildcard globbing ( * ), pipes ( | ), and file redirects ( > and >> ) do not work. To specify these settings, you can set cmd to be a shell of your choice, such as sh and pass a shell script to stdin . transform.stdin is an array of lines that are sent to your command on stdin . Lines do not have to end in newline characters. transform.err_cmd is an optional command that is executed on failed datums. If the err_cmd is successful and returns 0 error code, it does not prevent the job from succeeding. This behavior means that transform.err_cmd can be used to ignore failed datums while still writing successful datums to the output repo, instead of failing the whole job when some datums fail. The transform.err_cmd command has the same limitations as transform.cmd . transform.err_stdin is an array of lines that are sent to your error command on stdin . Lines do not have to end in newline characters. transform.env is a key-value map of environment variables that Pachyderm injects into the container. Note: There are environment variables that are automatically injected into the container, for a comprehensive list of them see the Environment Variables section below. transform.secrets is an array of secrets. You can use the secrets to embed sensitive data, such as credentials. The secrets reference Kubernetes secrets by name and specify a path to map the secrets or an environment variable ( env_var ) that the value should be bound to. Secrets must set name which should be the name of a secret in Kubernetes. Secrets must also specify either mount_path or env_var and key . See more information about Kubernetes secrets here . transform.image_pull_secrets is an array of image pull secrets, image pull secrets are similar to secrets except that they are mounted before the containers are created so they can be used to provide credentials for image pulling. For example, if you are using a private Docker registry for your images, you can specify it by running the following command: $ kubectl create secret docker-registry myregistrykey --docker-server = DOCKER_REGISTRY_SERVER --docker-username = DOCKER_USER --docker-password = DOCKER_PASSWORD --docker-email = DOCKER_EMAIL And then, notify your pipeline about it by using \"image_pull_secrets\": [ \"myregistrykey\" ] . Read more about image pull secrets here . transform.accept_return_code is an array of return codes, such as exit codes from your Docker command that are considered acceptable. If your Docker command exits with one of the codes in this array, it is considered a successful run to set job status. 0 is always considered a successful exit code. transform.debug turns on added debug logging for the pipeline. transform.user sets the user that your code runs as, this can also be accomplished with a USER directive in your Dockerfile . transform.working_dir sets the directory that your command runs from. You can also specify the WORKDIR directive in your Dockerfile . transform.dockerfile is the path to the Dockerfile used with the --build flag. This defaults to ./Dockerfile . Parallelism Spec (optional) \u00b6 parallelism_spec describes how Pachyderm parallelizes your pipeline. Currently, Pachyderm has two parallelism strategies: constant and coefficient . If you set the constant field, Pachyderm starts the number of workers that you specify. For example, set \"constant\":10 to use 10 workers. If you set the coefficient field, Pachyderm starts a number of workers that is a multiple of your Kubernetes cluster\u2019s size. For example, if your Kubernetes cluster has 10 nodes, and you set \"coefficient\": 0.5 , Pachyderm starts five workers. If you set it to 2.0, Pachyderm starts 20 workers (two per Kubernetes node). The default if left unset is \"constant=1\". Resource Requests (optional) \u00b6 resource_requests describes the amount of resources you expect the workers for a given pipeline to consume. Knowing this in advance lets Pachyderm schedule big jobs on separate machines, so that they do not conflict and either slow down or die. The memory field is a string that describes the amount of memory, in bytes, each worker needs (with allowed SI suffixes (M, K, G, Mi, Ki, Gi, and so on). For example, a worker that needs to read a 1GB file into memory might set \"memory\": \"1.2G\" with a little extra for the code to use in addition to the file. Workers for this pipeline will be placed on machines with at least 1.2GB of free memory, and other large workers will be prevented from using it (if they also set their resource_requests ). The cpu field is a number that describes the amount of CPU time in cpu seconds/real seconds that each worker needs. Setting \"cpu\": 0.5 indicates that the worker should get 500ms of CPU time per second. Setting \"cpu\": 2 indicates that the worker gets 2000ms of CPU time per second. In other words, it is using 2 CPUs, though worker threads might spend 500ms on four physical CPUs instead of one second on two physical CPUs. The disk field is a string that describes the amount of ephemeral disk space, in bytes, each worker needs with allowed SI suffixes (M, K, G, Mi, Ki, Gi, and so on). In both cases, the resource requests are not upper bounds. If the worker uses more memory than it is requested, it does not mean that it will be shut down. However, if the whole node runs out of memory, Kubernetes starts deleting pods that have been placed on it and exceeded their memory request, to reclaim memory. To prevent deletion of your worker node, you must set your memory request to a sufficiently large value. However, if the total memory requested by all workers in the system is too large, Kubernetes cannot schedule new workers because no machine has enough unclaimed memory. cpu works similarly, but for CPU time. By default, workers are scheduled with an effective resource request of 0 (to avoid scheduling problems that prevent users from being unable to run pipelines). This means that if a node runs out of memory, any such worker might be terminated. For more information about resource requests and limits see the Kubernetes docs on the subject. Resource Limits (optional) \u00b6 resource_limits describes the upper threshold of allowed resources a given worker can consume. If a worker exceeds this value, it will be evicted. The gpu field is a number that describes how many GPUs each worker needs. Only whole number are supported, Kubernetes does not allow multiplexing of GPUs. Unlike the other resource fields, GPUs only have meaning in Limits, by requesting a GPU the worker will have sole access to that GPU while it is running. It's recommended to enable standby if you are using GPUs so other processes in the cluster will have access to the GPUs while the pipeline has nothing to process. For more information about scheduling GPUs see the Kubernetes docs on the subject. Datum Timeout (optional) \u00b6 datum_timeout is a string (e.g. 1s , 5m , or 15h ) that determines the maximum execution time allowed per datum. So no matter what your parallelism or number of datums, no single datum is allowed to exceed this value. Datum Tries (optional) \u00b6 datum_tries is a int (e.g. 1 , 2 , or 3 ) that determines the number of retries that a job should attempt given failure was observed. Only failed datums are retries in retry attempt. The the operation succeeds in retry attempts then job is successful, otherwise the job is marked as failure. Job Timeout (optional) \u00b6 job_timeout is a string (e.g. 1s , 5m , or 15h ) that determines the maximum execution time allowed for a job. It differs from datum_timeout in that the limit gets applied across all workers and all datums. That means that you'll need to keep in mind the parallelism, total number of datums, and execution time per datum when setting this value. Keep in mind that the number of datums may change over jobs. Some new commits may have a bunch of new files (and so new datums). Some may have fewer. Input (required) \u00b6 input specifies repos that will be visible to the jobs during runtime. Commits to these repos will automatically trigger the pipeline to create new jobs to process them. Input is a recursive type, there are multiple different kinds of inputs which can be combined together. The input object is a container for the different input types with a field for each, only one of these fields be set for any instantiation of the object. { \"pfs\": pfs_input, \"union\": union_input, \"cross\": cross_input, \"cron\": cron_input } PFS Input \u00b6 Note: Atom inputs were renamed to PFS inputs in version 1.8.1. If you are using an older version of Pachyderm, replace every instance of pfs with atom in the code below. PFS inputs are the simplest inputs, they take input from a single branch on a single repo. { \"name\": string, \"repo\": string, \"branch\": string, \"glob\": string, \"lazy\" bool, \"empty_files\": bool } input.pfs.name is the name of the input. An input with the name XXX is visible under the path /pfs/XXX when a job runs. Input names must be unique if the inputs are crossed, but they may be duplicated between PFSInput s that are combined by using the union operator. This is because when PFSInput s are combined, you only ever see a datum from one input at a time. Overlapping the names of combined inputs allows you to write simpler code since you no longer need to consider which input directory a particular datum comes from. If an input's name is not specified, it defaults to the name of the repo. Therefore, if you have two crossed inputs from the same repo, you must give at least one of them a unique name. input.pfs.repo is the repo to be used for the input. input.pfs.branch is the branch to watch for commits. If left blank, master is used by default. input.pfs.glob is a glob pattern that is used to determine how the input data is partitioned. It is explained in detail in the next section. input.pfs.lazy controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true , data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. Some applications do not work with pipes. For example, pipes do not support applications that makes syscalls such as Seek . Applications that can work with pipes must use them since they are more performant. The difference will be especially notable if the job only reads a subset of the files that are available to it. Note: lazy currently does not support datums that contain more than 10000 files. input.pfs.empty_files controls how files are exposed to jobs. If set to true , it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks. Union Input \u00b6 Union inputs take the union of other inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /* . Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only filesystm objects in these repositories. | inputA | inputB | inputA \u222a inputB | | ------ | ------ | --------------- | | foo | fizz | foo | | bar | buzz | fizz | | | | bar | | | | buzz | The union inputs do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... or /pfs/inputB/... , but never both at the same time. When you write code to address this behavior, make sure that your code first determines which input directory is present. Starting with Pachyderm 1.5.3, we recommend that you give your inputs the same Name . That way your code only needs to handle data being present in that directory. This only works if your code does not need to be aware of which of the underlying inputs the data comes from. input.union is an array of inputs to combine. The inputs do not have to be pfs inputs. They can also be union and cross inputs. Although, there is no reason to take a union of unions because union is associative. Cross Input \u00b6 Cross inputs create the cross product of other inputs. In other words, a cross input creates tuples of the datums in the inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /* . Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only filesystm objects in these repositories. | inputA | inputB | inputA \u2a2f inputB | | ------ | ------ | --------------- | | foo | fizz | (foo, fizz) | | bar | buzz | (foo, buzz) | | | | (bar, fizz) | | | | (bar, buzz) | The cross inputs above do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... and /pfs/inputB/... . input.cross is an array of inputs to cross. The inputs do not have to be pfs inputs. They can also be union and cross inputs. Although, there is no reason to take a union of unions because union is associative. Cron Input \u00b6 Cron inputs allow you to trigger pipelines based on time. A Cron input is based on the Unix utility called cron . When you create a pipeline with one or more Cron inputs, pachd creates a repo for each of them. The start time for Cron input is specified in its spec. When a Cron input triggers, pachd commits a single file, named by the current RFC 3339 timestamp to the repo which contains the time which satisfied the spec. { \"name\": string, \"spec\": string, \"repo\": string, \"start\": time, \"overwrite\": bool } input.cron.name is the name for the input. Its semantics is similar to those of input.pfs.name . Except that it is not optional. input.cron.spec is a cron expression which specifies the schedule on which to trigger the pipeline. To learn more about how to write schedules, see the Wikipedia page on cron . Pachyderm supports non-standard schedules, such as \"@daily\" . input.cron.repo is the repo which Pachyderm creates for the input. This parameter is optional. If you do not specify this parameter, then \"<pipeline-name>_<input-name>\" is used by default. input.cron.start is the time to start counting from for the input. This parameter is optional. If you do not specify this parameter, then the time when the pipeline was created is used by default. Specifying a time enables you to run on matching times from the past or skip times from the present and only start running on matching times in the future. Format the time value according to RFC 3339 . input.cron.overwrite is a flag to specify whether you want the timestamp file to be overwritten on each tick. This parameter is optional, and if you do not specify it, it defaults to simply writing new files on each tick. By default, pachd expects only the new information to be written out on each tick and combines that data with the data from the previous ticks. If \"overwrite\" is set to true , it expects the full dataset to be written out for each tick and replaces previous outputs with the new data written out. Git Input (alpha feature) \u00b6 Git inputs allow you to pull code from a public git URL and execute that code as part of your pipeline. A pipeline with a Git Input will get triggered (i.e. will see a new input commit and will spawn a job) whenever you commit to your git repository. Note: This only works on cloud deployments, not local clusters. input.git.URL must be a URL of the form: https://github.com/foo/bar.git input.git.name is the name for the input, its semantics are similar to those of input.pfs.name . It is optional. input.git.branch is the name of the git branch to use as input Git inputs also require some additional configuration. In order for new commits on your git repository to correspond to new commits on the Pachyderm Git Input repo, we need to setup a git webhook. At the moment, only GitHub is supported. (Though if you ask nicely, we can add support for GitLab or BitBucket). Create your Pachyderm pipeline with the Git Input. To get the URL of the webhook to your cluster, do pachctl inspect pipeline on your pipeline. You should see a Githook URL field with a URL set. Note - this will only work if you've deployed to a cloud provider (e.g. AWS, GKE). If you see pending as the value (and you've deployed on a cloud provider), it's possible that the service is still being provisioned. You can check kubectl get svc to make sure you see the githook service running. To setup the GitHub webhook, navigate to: https://github.com/<your_org>/<your_repo>/settings/hooks/new Or navigate to webhooks under settings. Then you'll want to copy the Githook URL into the 'Payload URL' field. Output Branch (optional) \u00b6 This is the branch where the pipeline outputs new commits. By default, it's \"master\". Egress (optional) \u00b6 egress allows you to push the results of a Pipeline to an external data store such as s3, Google Cloud Storage or Azure Storage. Data will be pushed after the user code has finished running but before the job is marked as successful. For more information, see Exporting Data by using egress Standby (optional) \u00b6 standby indicates that the pipeline should be put into \"standby\" when there's no data for it to process. A pipeline in standby will have no pods running and thus will consume no resources, it's state will be displayed as \"standby\". Standby replaces scale_down_threshold from releases prior to 1.7.1. Cache Size (optional) \u00b6 cache_size controls how much cache a pipeline's sidecar containers use. In general, your pipeline's performance will increase with the cache size, but only up to a certain point depending on your workload. Every worker in every pipeline has a limited-functionality pachd server running adjacent to it, which proxies PFS reads and writes (this prevents thundering herds when jobs start and end, which is when all of a pipeline's workers are reading from and writing to PFS simultaneously). Part of what these \"sidecar\" pachd servers do is cache PFS reads. If a pipeline has a cross input, and a worker is downloading the same datum from one branch of the input repeatedly, then the cache can speed up processing significantly. Enable Stats (optional) \u00b6 enable_stats turns on stat tracking for the pipeline. This will cause the pipeline to commit to a second branch in its output repo called \"stats\" . This branch will have information about each datum that is processed including: timing information, size information, logs and a /pfs snapshot. This information can be accessed through the inspect datum and list datum pachctl commands and through the webUI. Note: enabling stats will use extra storage for logs and timing information. However it will not use as much extra storage as it appears to due to the fact that snapshots of the /pfs directory, which are generally the largest thing stored, don't actually require extra storage because the data is already stored in the input repos. Service (alpha feature, optional) \u00b6 service specifies that the pipeline should be treated as a long running service rather than a data transformation. This means that transform.cmd is not expected to exit, if it does it will be restarted. Furthermore, the service is exposed outside the container using a Kubernetes service. \"internal_port\" should be a port that the user code binds to inside the container, \"external_port\" is the port on which it is exposed through the NodePorts functionality of Kubernetes services. After a service has been created, you should be able to access it at http://<kubernetes-host>:<external_port> . Spout (optional) \u00b6 spout is a type of pipeline that processes streaming data. Unlike a union or cross pipeline, a spout pipeline does not have a PFS input. Instead, it opens a Linux named pipe into the source of the streaming data. Your pipeline can be either a spout or a service and not both. Therefore, if you added the service as a top-level object in your pipeline, you cannot add spout . However, you can expose a service from inside of a spout pipeline by specifying it as a field in the spout spec. Then, Kubernetes creates a service endpoint that you can expose externally. You can get the information about the service by running kubectl get services . For more information, see Spouts . Max Queue Size (optional) \u00b6 max_queue_size specifies that maximum number of datums that a worker should hold in its processing queue at a given time (after processing its entire queue, a worker \"checkpoints\" its progress by writing to persistent storage). The default value is 1 which means workers will only hold onto the value that they're currently processing. Increasing this value can improve pipeline performance, as that allows workers to simultaneously download, process and upload different datums at the same time (and reduces the total time spent on checkpointing). Decreasing this value can make jobs more robust to failed workers, as work gets checkpointed more often, and a failing worker will not lose as much progress. Setting this value too high can also cause problems if you have lazy inputs, as there's a cap of 10,000 lazy files per worker and multiple datums that are running all count against this limit. Chunk Spec (optional) \u00b6 chunk_spec specifies how a pipeline should chunk its datums. chunk_spec.number if nonzero, specifies that each chunk should contain number datums. Chunks may contain fewer if the total number of datums don't divide evenly. chunk_spec.size_bytes , if nonzero, specifies a target size for each chunk of datums. Chunks may be larger or smaller than size_bytes , but will usually be pretty close to size_bytes in size. Scheduling Spec (optional) \u00b6 scheduling_spec specifies how the pods for a pipeline should be scheduled. scheduling_spec.node_selector allows you to select which nodes your pipeline will run on. Refer to the Kubernetes docs on node selectors for more information about how this works. scheduling_spec.priority_class_name allows you to select the prioriy class for the pipeline, which will how Kubernetes chooses to schedule and deschedule the pipeline. Refer to the Kubernetes docs on priority and preemption for more information about how this works. Pod Spec (optional) \u00b6 pod_spec is an advanced option that allows you to set fields in the pod spec that haven't been explicitly exposed in the rest of the pipeline spec. A good way to figure out what JSON you should pass is to create a pod in Kubernetes with the proper settings, then do: kubectl get po/<pod-name> -o json | jq .spec this will give you a correctly formated piece of JSON, you should then remove the extraneous fields that Kubernetes injects or that can be set else where. The JSON is applied after the other parameters for the pod_spec have already been set as a JSON Merge Patch . This means that you can modify things such as the storage and user containers. Pod Patch (optional) \u00b6 pod_patch is similar to pod_spec above but is applied as a JSON Patch . Note, this means that the process outlined above of modifying an existing pod spec and then manually blanking unchanged fields won't work, you'll need to create a correctly formatted patch by diffing the two pod specs. The Input Glob Pattern \u00b6 Each PFS input needs to specify a glob pattern . Pachyderm uses the glob pattern to determine how many \"datums\" an input consists of. Datums are the unit of parallelism in Pachyderm. That is, Pachyderm attempts to process datums in parallel whenever possible. Intuitively, you may think of the input repo as a file system, and you are applying the glob pattern to the root of the file system. The files and directories that match the glob pattern are considered datums. For instance, let's say your input repo has the following structure: /foo-1 /foo-2 /bar /bar-1 /bar-2 Now let's consider what the following glob patterns would match respectively: / : this pattern matches / , the root directory itself, meaning all the data would be a single large datum. /* : this pattern matches everything under the root directory given us 3 datums: /foo-1. , /foo-2. , and everything under the directory /bar . /bar/* : this pattern matches files only under the /bar directory: /bar-1 and /bar-2 /foo* : this pattern matches files under the root directory that start with the characters foo /*/* : this pattern matches everything that's two levels deep relative to the root: /bar/bar-1 and /bar/bar-2 The datums are defined as whichever files or directories match by the glob pattern. For instance, if we used /* , then the job will process three datums (potentially in parallel): /foo-1 , /foo-2 , and /bar . Both the bar-1 and bar-2 files within the directory bar would be grouped together and always processed by the same worker. PPS Mounts and File Access \u00b6 Mount Paths \u00b6 The root mount point is at /pfs , which contains: /pfs/input_name which is where you would find the datum. Each input will be found here by its name, which defaults to the repo name if not specified. /pfs/out which is where you write any output. Environment Variables \u00b6 There are several environment variables that get injected into the user code before it runs. They are: PACH_JOB_ID the id the currently run job. PACH_OUTPUT_COMMIT_ID the id of the commit being outputted to. For each input there will be an environment variable with the same name defined to the path of the file for that input. For example if you are accessing an input called foo from the path /pfs/foo which contains a file called bar then the environment variable foo will have the value /pfs/foo/bar . The path in the environment variable is the path which matched the glob pattern, even if the file is a directory, ie if your glob pattern is /* it would match a directory /bar , the value of $foo would then be /pfs/foo/bar . With a glob pattern of /*/* you would match the files contained in /bar and thus the value of foo would be /pfs/foo/bar/quux . For each input there will be an environment variable named input_COMMIT indicating the id of the commit being used for that input. In addition to these environment variables Kubernetes also injects others for Services that are running inside the cluster. These allow you to connect to those outside services, which can be powerful but also can be hard to reason about, as processing might be retried multiple times. For example if your code writes a row to a database that row may be written multiple times due to retries. Interaction with outside services should be idempotent to prevent unexpected behavior. Furthermore, one of the running services that your code can connect to is Pachyderm itself, this is generally not recommended as very little of the Pachyderm API is idempotent, but in some specific cases it can be a viable approach.","title":"Pipeline Specification"},{"location":"reference/pipeline_spec/#pipeline-specification","text":"This document discusses each of the fields present in a pipeline specification. To see how to use a pipeline spec to create a pipeline, refer to the pachctl create pipeline section.","title":"Pipeline Specification"},{"location":"reference/pipeline_spec/#json-manifest-format","text":"{ \"pipeline\" : { \"name\" : string }, \"description\" : string , \"transform\" : { \"image\" : string , \"cmd\" : [ string ], \"stdin\" : [ string ], \"err_cmd\" : [ string ], \"err_stdin\" : [ string ], \"env\" : { string: string }, \"secrets\" : [ { \"name\" : string , \"mount_path\" : string }, { \"name\" : string , \"env_var\" : string , \"key\" : string } ], \"image_pull_secrets\" : [ string ], \"accept_return_code\" : [ int ], \"debug\" : bool , \"user\" : string , \"working_dir\" : string , }, \"parallelism_spec\" : { // Set at most one of the following: \"constant\" : int , \"coefficient\" : number }, \"hashtree_spec\" : { \"constant\" : int , }, \"resource_requests\" : { \"memory\" : string , \"cpu\" : number , \"disk\" : string , }, \"resource_limits\" : { \"memory\" : string , \"cpu\" : number , \"gpu\" : { \"type\" : string , \"number\" : int } \"disk\" : string , }, \"datum_timeout\" : string , \"datum_tries\" : int , \"job_timeout\" : string , \"input\" : { < \"pfs\" , \"cross\" , \"union\" , \"cron\" , or \"git\" see below> }, \"output_branch\" : string , \"egress\" : { \"URL\" : \"s3://bucket/dir\" }, \"standby\" : bool , \"cache_size\" : string , \"enable_stats\" : bool , \"service\" : { \"internal_port\" : int , \"external_port\" : int }, \"spout\" : { \"overwrite\" : bool \\\\ Optionally , you can combine a spout with a service: \"service\" : { \"internal_port\" : int , \"external_port\" : int , \"annotations\" : { \"foo\" : \"bar\" } } }, \"max_queue_size\" : int , \"chunk_spec\" : { \"number\" : int , \"size_bytes\" : int }, \"scheduling_spec\" : { \"node_selector\" : { string: string }, \"priority_class_name\" : string }, \"pod_spec\" : string , \"pod_patch\" : string , } ------------------------------------ \"pfs\" input ------------------------------------ \"pfs\" : { \"name\" : string , \"repo\" : string , \"branch\" : string , \"glob\" : string , \"lazy\" bool , \"empty_files\" : bool } ------------------------------------ \"cross\" or \"union\" input ------------------------------------ \"cross\" or \"union\" : [ { \"pfs\" : { \"name\" : string , \"repo\" : string , \"branch\" : string , \"glob\" : string , \"lazy\" bool , \"empty_files\" : bool } }, { \"pfs\" : { \"name\" : string , \"repo\" : string , \"branch\" : string , \"glob\" : string , \"lazy\" bool , \"empty_files\" : bool } } etc... ] ------------------------------------ \"cron\" input ------------------------------------ \"cron\" : { \"name\" : string , \"spec\" : string , \"repo\" : string , \"start\" : time , \"overwrite\" : bool } ------------------------------------ \"git\" input ------------------------------------ \"git\" : { \"URL\" : string , \"name\" : string , \"branch\" : string } In practice, you rarely need to specify all the fields. Most fields either come with sensible defaults or can be empty. The following text is an example of a minimum spec: { \"pipeline\" : { \"name\" : \"wordcount\" }, \"transform\" : { \"image\" : \"wordcount-image\" , \"cmd\" : [ \"/binary\" , \"/pfs/data\" , \"/pfs/out\" ] }, \"input\" : { \"pfs\" : { \"repo\" : \"data\" , \"glob\" : \"/*\" } } }","title":"JSON Manifest Format"},{"location":"reference/pipeline_spec/#name-required","text":"pipeline.name is the name of the pipeline that you are creating. Each pipeline needs to have a unique name. Pipeline names must meet the following prerequisites: Include only alphanumeric characters, _ and - . Begin or end with only alphanumeric characters (not _ or - ). Not exceed 50 characters in length.","title":"Name (required)"},{"location":"reference/pipeline_spec/#description-optional","text":"description is an optional text field where you can add information about the pipeline.","title":"Description (optional)"},{"location":"reference/pipeline_spec/#transform-required","text":"transform.image is the name of the Docker image that your jobs use. transform.cmd is the command passed to the Docker run invocation. Similarly to Docker, cmd is not run inside a shell which means that wildcard globbing ( * ), pipes ( | ), and file redirects ( > and >> ) do not work. To specify these settings, you can set cmd to be a shell of your choice, such as sh and pass a shell script to stdin . transform.stdin is an array of lines that are sent to your command on stdin . Lines do not have to end in newline characters. transform.err_cmd is an optional command that is executed on failed datums. If the err_cmd is successful and returns 0 error code, it does not prevent the job from succeeding. This behavior means that transform.err_cmd can be used to ignore failed datums while still writing successful datums to the output repo, instead of failing the whole job when some datums fail. The transform.err_cmd command has the same limitations as transform.cmd . transform.err_stdin is an array of lines that are sent to your error command on stdin . Lines do not have to end in newline characters. transform.env is a key-value map of environment variables that Pachyderm injects into the container. Note: There are environment variables that are automatically injected into the container, for a comprehensive list of them see the Environment Variables section below. transform.secrets is an array of secrets. You can use the secrets to embed sensitive data, such as credentials. The secrets reference Kubernetes secrets by name and specify a path to map the secrets or an environment variable ( env_var ) that the value should be bound to. Secrets must set name which should be the name of a secret in Kubernetes. Secrets must also specify either mount_path or env_var and key . See more information about Kubernetes secrets here . transform.image_pull_secrets is an array of image pull secrets, image pull secrets are similar to secrets except that they are mounted before the containers are created so they can be used to provide credentials for image pulling. For example, if you are using a private Docker registry for your images, you can specify it by running the following command: $ kubectl create secret docker-registry myregistrykey --docker-server = DOCKER_REGISTRY_SERVER --docker-username = DOCKER_USER --docker-password = DOCKER_PASSWORD --docker-email = DOCKER_EMAIL And then, notify your pipeline about it by using \"image_pull_secrets\": [ \"myregistrykey\" ] . Read more about image pull secrets here . transform.accept_return_code is an array of return codes, such as exit codes from your Docker command that are considered acceptable. If your Docker command exits with one of the codes in this array, it is considered a successful run to set job status. 0 is always considered a successful exit code. transform.debug turns on added debug logging for the pipeline. transform.user sets the user that your code runs as, this can also be accomplished with a USER directive in your Dockerfile . transform.working_dir sets the directory that your command runs from. You can also specify the WORKDIR directive in your Dockerfile . transform.dockerfile is the path to the Dockerfile used with the --build flag. This defaults to ./Dockerfile .","title":"Transform (required)"},{"location":"reference/pipeline_spec/#parallelism-spec-optional","text":"parallelism_spec describes how Pachyderm parallelizes your pipeline. Currently, Pachyderm has two parallelism strategies: constant and coefficient . If you set the constant field, Pachyderm starts the number of workers that you specify. For example, set \"constant\":10 to use 10 workers. If you set the coefficient field, Pachyderm starts a number of workers that is a multiple of your Kubernetes cluster\u2019s size. For example, if your Kubernetes cluster has 10 nodes, and you set \"coefficient\": 0.5 , Pachyderm starts five workers. If you set it to 2.0, Pachyderm starts 20 workers (two per Kubernetes node). The default if left unset is \"constant=1\".","title":"Parallelism Spec (optional)"},{"location":"reference/pipeline_spec/#resource-requests-optional","text":"resource_requests describes the amount of resources you expect the workers for a given pipeline to consume. Knowing this in advance lets Pachyderm schedule big jobs on separate machines, so that they do not conflict and either slow down or die. The memory field is a string that describes the amount of memory, in bytes, each worker needs (with allowed SI suffixes (M, K, G, Mi, Ki, Gi, and so on). For example, a worker that needs to read a 1GB file into memory might set \"memory\": \"1.2G\" with a little extra for the code to use in addition to the file. Workers for this pipeline will be placed on machines with at least 1.2GB of free memory, and other large workers will be prevented from using it (if they also set their resource_requests ). The cpu field is a number that describes the amount of CPU time in cpu seconds/real seconds that each worker needs. Setting \"cpu\": 0.5 indicates that the worker should get 500ms of CPU time per second. Setting \"cpu\": 2 indicates that the worker gets 2000ms of CPU time per second. In other words, it is using 2 CPUs, though worker threads might spend 500ms on four physical CPUs instead of one second on two physical CPUs. The disk field is a string that describes the amount of ephemeral disk space, in bytes, each worker needs with allowed SI suffixes (M, K, G, Mi, Ki, Gi, and so on). In both cases, the resource requests are not upper bounds. If the worker uses more memory than it is requested, it does not mean that it will be shut down. However, if the whole node runs out of memory, Kubernetes starts deleting pods that have been placed on it and exceeded their memory request, to reclaim memory. To prevent deletion of your worker node, you must set your memory request to a sufficiently large value. However, if the total memory requested by all workers in the system is too large, Kubernetes cannot schedule new workers because no machine has enough unclaimed memory. cpu works similarly, but for CPU time. By default, workers are scheduled with an effective resource request of 0 (to avoid scheduling problems that prevent users from being unable to run pipelines). This means that if a node runs out of memory, any such worker might be terminated. For more information about resource requests and limits see the Kubernetes docs on the subject.","title":"Resource Requests (optional)"},{"location":"reference/pipeline_spec/#resource-limits-optional","text":"resource_limits describes the upper threshold of allowed resources a given worker can consume. If a worker exceeds this value, it will be evicted. The gpu field is a number that describes how many GPUs each worker needs. Only whole number are supported, Kubernetes does not allow multiplexing of GPUs. Unlike the other resource fields, GPUs only have meaning in Limits, by requesting a GPU the worker will have sole access to that GPU while it is running. It's recommended to enable standby if you are using GPUs so other processes in the cluster will have access to the GPUs while the pipeline has nothing to process. For more information about scheduling GPUs see the Kubernetes docs on the subject.","title":"Resource Limits (optional)"},{"location":"reference/pipeline_spec/#datum-timeout-optional","text":"datum_timeout is a string (e.g. 1s , 5m , or 15h ) that determines the maximum execution time allowed per datum. So no matter what your parallelism or number of datums, no single datum is allowed to exceed this value.","title":"Datum Timeout (optional)"},{"location":"reference/pipeline_spec/#datum-tries-optional","text":"datum_tries is a int (e.g. 1 , 2 , or 3 ) that determines the number of retries that a job should attempt given failure was observed. Only failed datums are retries in retry attempt. The the operation succeeds in retry attempts then job is successful, otherwise the job is marked as failure.","title":"Datum Tries (optional)"},{"location":"reference/pipeline_spec/#job-timeout-optional","text":"job_timeout is a string (e.g. 1s , 5m , or 15h ) that determines the maximum execution time allowed for a job. It differs from datum_timeout in that the limit gets applied across all workers and all datums. That means that you'll need to keep in mind the parallelism, total number of datums, and execution time per datum when setting this value. Keep in mind that the number of datums may change over jobs. Some new commits may have a bunch of new files (and so new datums). Some may have fewer.","title":"Job Timeout (optional)"},{"location":"reference/pipeline_spec/#input-required","text":"input specifies repos that will be visible to the jobs during runtime. Commits to these repos will automatically trigger the pipeline to create new jobs to process them. Input is a recursive type, there are multiple different kinds of inputs which can be combined together. The input object is a container for the different input types with a field for each, only one of these fields be set for any instantiation of the object. { \"pfs\": pfs_input, \"union\": union_input, \"cross\": cross_input, \"cron\": cron_input }","title":"Input (required)"},{"location":"reference/pipeline_spec/#pfs-input","text":"Note: Atom inputs were renamed to PFS inputs in version 1.8.1. If you are using an older version of Pachyderm, replace every instance of pfs with atom in the code below. PFS inputs are the simplest inputs, they take input from a single branch on a single repo. { \"name\": string, \"repo\": string, \"branch\": string, \"glob\": string, \"lazy\" bool, \"empty_files\": bool } input.pfs.name is the name of the input. An input with the name XXX is visible under the path /pfs/XXX when a job runs. Input names must be unique if the inputs are crossed, but they may be duplicated between PFSInput s that are combined by using the union operator. This is because when PFSInput s are combined, you only ever see a datum from one input at a time. Overlapping the names of combined inputs allows you to write simpler code since you no longer need to consider which input directory a particular datum comes from. If an input's name is not specified, it defaults to the name of the repo. Therefore, if you have two crossed inputs from the same repo, you must give at least one of them a unique name. input.pfs.repo is the repo to be used for the input. input.pfs.branch is the branch to watch for commits. If left blank, master is used by default. input.pfs.glob is a glob pattern that is used to determine how the input data is partitioned. It is explained in detail in the next section. input.pfs.lazy controls how the data is exposed to jobs. The default is false which means the job eagerly downloads the data it needs to process and exposes it as normal files on disk. If lazy is set to true , data is exposed as named pipes instead, and no data is downloaded until the job opens the pipe and reads it. If the pipe is never opened, then no data is downloaded. Some applications do not work with pipes. For example, pipes do not support applications that makes syscalls such as Seek . Applications that can work with pipes must use them since they are more performant. The difference will be especially notable if the job only reads a subset of the files that are available to it. Note: lazy currently does not support datums that contain more than 10000 files. input.pfs.empty_files controls how files are exposed to jobs. If set to true , it causes files from this PFS to be presented as empty files. This is useful in shuffle pipelines where you want to read the names of files and reorganize them by using symlinks.","title":"PFS Input"},{"location":"reference/pipeline_spec/#union-input","text":"Union inputs take the union of other inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /* . Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only filesystm objects in these repositories. | inputA | inputB | inputA \u222a inputB | | ------ | ------ | --------------- | | foo | fizz | foo | | bar | buzz | fizz | | | | bar | | | | buzz | The union inputs do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... or /pfs/inputB/... , but never both at the same time. When you write code to address this behavior, make sure that your code first determines which input directory is present. Starting with Pachyderm 1.5.3, we recommend that you give your inputs the same Name . That way your code only needs to handle data being present in that directory. This only works if your code does not need to be aware of which of the underlying inputs the data comes from. input.union is an array of inputs to combine. The inputs do not have to be pfs inputs. They can also be union and cross inputs. Although, there is no reason to take a union of unions because union is associative.","title":"Union Input"},{"location":"reference/pipeline_spec/#cross-input","text":"Cross inputs create the cross product of other inputs. In other words, a cross input creates tuples of the datums in the inputs. In the example below, each input includes individual datums, such as if foo and bar were in the same repository with the glob pattern set to /* . Alternatively, each of these datums might have come from separate repositories with the glob pattern set to / and being the only filesystm objects in these repositories. | inputA | inputB | inputA \u2a2f inputB | | ------ | ------ | --------------- | | foo | fizz | (foo, fizz) | | bar | buzz | (foo, buzz) | | | | (bar, fizz) | | | | (bar, buzz) | The cross inputs above do not take a name and maintain the names of the sub-inputs. In the example above, you would see files under /pfs/inputA/... and /pfs/inputB/... . input.cross is an array of inputs to cross. The inputs do not have to be pfs inputs. They can also be union and cross inputs. Although, there is no reason to take a union of unions because union is associative.","title":"Cross Input"},{"location":"reference/pipeline_spec/#cron-input","text":"Cron inputs allow you to trigger pipelines based on time. A Cron input is based on the Unix utility called cron . When you create a pipeline with one or more Cron inputs, pachd creates a repo for each of them. The start time for Cron input is specified in its spec. When a Cron input triggers, pachd commits a single file, named by the current RFC 3339 timestamp to the repo which contains the time which satisfied the spec. { \"name\": string, \"spec\": string, \"repo\": string, \"start\": time, \"overwrite\": bool } input.cron.name is the name for the input. Its semantics is similar to those of input.pfs.name . Except that it is not optional. input.cron.spec is a cron expression which specifies the schedule on which to trigger the pipeline. To learn more about how to write schedules, see the Wikipedia page on cron . Pachyderm supports non-standard schedules, such as \"@daily\" . input.cron.repo is the repo which Pachyderm creates for the input. This parameter is optional. If you do not specify this parameter, then \"<pipeline-name>_<input-name>\" is used by default. input.cron.start is the time to start counting from for the input. This parameter is optional. If you do not specify this parameter, then the time when the pipeline was created is used by default. Specifying a time enables you to run on matching times from the past or skip times from the present and only start running on matching times in the future. Format the time value according to RFC 3339 . input.cron.overwrite is a flag to specify whether you want the timestamp file to be overwritten on each tick. This parameter is optional, and if you do not specify it, it defaults to simply writing new files on each tick. By default, pachd expects only the new information to be written out on each tick and combines that data with the data from the previous ticks. If \"overwrite\" is set to true , it expects the full dataset to be written out for each tick and replaces previous outputs with the new data written out.","title":"Cron Input"},{"location":"reference/pipeline_spec/#git-input-alpha-feature","text":"Git inputs allow you to pull code from a public git URL and execute that code as part of your pipeline. A pipeline with a Git Input will get triggered (i.e. will see a new input commit and will spawn a job) whenever you commit to your git repository. Note: This only works on cloud deployments, not local clusters. input.git.URL must be a URL of the form: https://github.com/foo/bar.git input.git.name is the name for the input, its semantics are similar to those of input.pfs.name . It is optional. input.git.branch is the name of the git branch to use as input Git inputs also require some additional configuration. In order for new commits on your git repository to correspond to new commits on the Pachyderm Git Input repo, we need to setup a git webhook. At the moment, only GitHub is supported. (Though if you ask nicely, we can add support for GitLab or BitBucket). Create your Pachyderm pipeline with the Git Input. To get the URL of the webhook to your cluster, do pachctl inspect pipeline on your pipeline. You should see a Githook URL field with a URL set. Note - this will only work if you've deployed to a cloud provider (e.g. AWS, GKE). If you see pending as the value (and you've deployed on a cloud provider), it's possible that the service is still being provisioned. You can check kubectl get svc to make sure you see the githook service running. To setup the GitHub webhook, navigate to: https://github.com/<your_org>/<your_repo>/settings/hooks/new Or navigate to webhooks under settings. Then you'll want to copy the Githook URL into the 'Payload URL' field.","title":"Git Input (alpha feature)"},{"location":"reference/pipeline_spec/#output-branch-optional","text":"This is the branch where the pipeline outputs new commits. By default, it's \"master\".","title":"Output Branch (optional)"},{"location":"reference/pipeline_spec/#egress-optional","text":"egress allows you to push the results of a Pipeline to an external data store such as s3, Google Cloud Storage or Azure Storage. Data will be pushed after the user code has finished running but before the job is marked as successful. For more information, see Exporting Data by using egress","title":"Egress (optional)"},{"location":"reference/pipeline_spec/#standby-optional","text":"standby indicates that the pipeline should be put into \"standby\" when there's no data for it to process. A pipeline in standby will have no pods running and thus will consume no resources, it's state will be displayed as \"standby\". Standby replaces scale_down_threshold from releases prior to 1.7.1.","title":"Standby (optional)"},{"location":"reference/pipeline_spec/#cache-size-optional","text":"cache_size controls how much cache a pipeline's sidecar containers use. In general, your pipeline's performance will increase with the cache size, but only up to a certain point depending on your workload. Every worker in every pipeline has a limited-functionality pachd server running adjacent to it, which proxies PFS reads and writes (this prevents thundering herds when jobs start and end, which is when all of a pipeline's workers are reading from and writing to PFS simultaneously). Part of what these \"sidecar\" pachd servers do is cache PFS reads. If a pipeline has a cross input, and a worker is downloading the same datum from one branch of the input repeatedly, then the cache can speed up processing significantly.","title":"Cache Size (optional)"},{"location":"reference/pipeline_spec/#enable-stats-optional","text":"enable_stats turns on stat tracking for the pipeline. This will cause the pipeline to commit to a second branch in its output repo called \"stats\" . This branch will have information about each datum that is processed including: timing information, size information, logs and a /pfs snapshot. This information can be accessed through the inspect datum and list datum pachctl commands and through the webUI. Note: enabling stats will use extra storage for logs and timing information. However it will not use as much extra storage as it appears to due to the fact that snapshots of the /pfs directory, which are generally the largest thing stored, don't actually require extra storage because the data is already stored in the input repos.","title":"Enable Stats (optional)"},{"location":"reference/pipeline_spec/#service-alpha-feature-optional","text":"service specifies that the pipeline should be treated as a long running service rather than a data transformation. This means that transform.cmd is not expected to exit, if it does it will be restarted. Furthermore, the service is exposed outside the container using a Kubernetes service. \"internal_port\" should be a port that the user code binds to inside the container, \"external_port\" is the port on which it is exposed through the NodePorts functionality of Kubernetes services. After a service has been created, you should be able to access it at http://<kubernetes-host>:<external_port> .","title":"Service (alpha feature, optional)"},{"location":"reference/pipeline_spec/#spout-optional","text":"spout is a type of pipeline that processes streaming data. Unlike a union or cross pipeline, a spout pipeline does not have a PFS input. Instead, it opens a Linux named pipe into the source of the streaming data. Your pipeline can be either a spout or a service and not both. Therefore, if you added the service as a top-level object in your pipeline, you cannot add spout . However, you can expose a service from inside of a spout pipeline by specifying it as a field in the spout spec. Then, Kubernetes creates a service endpoint that you can expose externally. You can get the information about the service by running kubectl get services . For more information, see Spouts .","title":"Spout (optional)"},{"location":"reference/pipeline_spec/#max-queue-size-optional","text":"max_queue_size specifies that maximum number of datums that a worker should hold in its processing queue at a given time (after processing its entire queue, a worker \"checkpoints\" its progress by writing to persistent storage). The default value is 1 which means workers will only hold onto the value that they're currently processing. Increasing this value can improve pipeline performance, as that allows workers to simultaneously download, process and upload different datums at the same time (and reduces the total time spent on checkpointing). Decreasing this value can make jobs more robust to failed workers, as work gets checkpointed more often, and a failing worker will not lose as much progress. Setting this value too high can also cause problems if you have lazy inputs, as there's a cap of 10,000 lazy files per worker and multiple datums that are running all count against this limit.","title":"Max Queue Size (optional)"},{"location":"reference/pipeline_spec/#chunk-spec-optional","text":"chunk_spec specifies how a pipeline should chunk its datums. chunk_spec.number if nonzero, specifies that each chunk should contain number datums. Chunks may contain fewer if the total number of datums don't divide evenly. chunk_spec.size_bytes , if nonzero, specifies a target size for each chunk of datums. Chunks may be larger or smaller than size_bytes , but will usually be pretty close to size_bytes in size.","title":"Chunk Spec (optional)"},{"location":"reference/pipeline_spec/#scheduling-spec-optional","text":"scheduling_spec specifies how the pods for a pipeline should be scheduled. scheduling_spec.node_selector allows you to select which nodes your pipeline will run on. Refer to the Kubernetes docs on node selectors for more information about how this works. scheduling_spec.priority_class_name allows you to select the prioriy class for the pipeline, which will how Kubernetes chooses to schedule and deschedule the pipeline. Refer to the Kubernetes docs on priority and preemption for more information about how this works.","title":"Scheduling Spec (optional)"},{"location":"reference/pipeline_spec/#pod-spec-optional","text":"pod_spec is an advanced option that allows you to set fields in the pod spec that haven't been explicitly exposed in the rest of the pipeline spec. A good way to figure out what JSON you should pass is to create a pod in Kubernetes with the proper settings, then do: kubectl get po/<pod-name> -o json | jq .spec this will give you a correctly formated piece of JSON, you should then remove the extraneous fields that Kubernetes injects or that can be set else where. The JSON is applied after the other parameters for the pod_spec have already been set as a JSON Merge Patch . This means that you can modify things such as the storage and user containers.","title":"Pod Spec (optional)"},{"location":"reference/pipeline_spec/#pod-patch-optional","text":"pod_patch is similar to pod_spec above but is applied as a JSON Patch . Note, this means that the process outlined above of modifying an existing pod spec and then manually blanking unchanged fields won't work, you'll need to create a correctly formatted patch by diffing the two pod specs.","title":"Pod Patch (optional)"},{"location":"reference/pipeline_spec/#the-input-glob-pattern","text":"Each PFS input needs to specify a glob pattern . Pachyderm uses the glob pattern to determine how many \"datums\" an input consists of. Datums are the unit of parallelism in Pachyderm. That is, Pachyderm attempts to process datums in parallel whenever possible. Intuitively, you may think of the input repo as a file system, and you are applying the glob pattern to the root of the file system. The files and directories that match the glob pattern are considered datums. For instance, let's say your input repo has the following structure: /foo-1 /foo-2 /bar /bar-1 /bar-2 Now let's consider what the following glob patterns would match respectively: / : this pattern matches / , the root directory itself, meaning all the data would be a single large datum. /* : this pattern matches everything under the root directory given us 3 datums: /foo-1. , /foo-2. , and everything under the directory /bar . /bar/* : this pattern matches files only under the /bar directory: /bar-1 and /bar-2 /foo* : this pattern matches files under the root directory that start with the characters foo /*/* : this pattern matches everything that's two levels deep relative to the root: /bar/bar-1 and /bar/bar-2 The datums are defined as whichever files or directories match by the glob pattern. For instance, if we used /* , then the job will process three datums (potentially in parallel): /foo-1 , /foo-2 , and /bar . Both the bar-1 and bar-2 files within the directory bar would be grouped together and always processed by the same worker.","title":"The Input Glob Pattern"},{"location":"reference/pipeline_spec/#pps-mounts-and-file-access","text":"","title":"PPS Mounts and File Access"},{"location":"reference/pipeline_spec/#mount-paths","text":"The root mount point is at /pfs , which contains: /pfs/input_name which is where you would find the datum. Each input will be found here by its name, which defaults to the repo name if not specified. /pfs/out which is where you write any output.","title":"Mount Paths"},{"location":"reference/pipeline_spec/#environment-variables","text":"There are several environment variables that get injected into the user code before it runs. They are: PACH_JOB_ID the id the currently run job. PACH_OUTPUT_COMMIT_ID the id of the commit being outputted to. For each input there will be an environment variable with the same name defined to the path of the file for that input. For example if you are accessing an input called foo from the path /pfs/foo which contains a file called bar then the environment variable foo will have the value /pfs/foo/bar . The path in the environment variable is the path which matched the glob pattern, even if the file is a directory, ie if your glob pattern is /* it would match a directory /bar , the value of $foo would then be /pfs/foo/bar . With a glob pattern of /*/* you would match the files contained in /bar and thus the value of foo would be /pfs/foo/bar/quux . For each input there will be an environment variable named input_COMMIT indicating the id of the commit being used for that input. In addition to these environment variables Kubernetes also injects others for Services that are running inside the cluster. These allow you to connect to those outside services, which can be powerful but also can be hard to reason about, as processing might be retried multiple times. For example if your code writes a row to a database that row may be written multiple times due to retries. Interaction with outside services should be idempotent to prevent unexpected behavior. Furthermore, one of the running services that your code can connect to is Pachyderm itself, this is generally not recommended as very little of the Pachyderm API is idempotent, but in some specific cases it can be a viable approach.","title":"Environment Variables"},{"location":"reference/s3gateway_api/","text":"S3Gateway API \u00b6 This outlines the HTTP API exposed by the s3gateway and any peculiarities relative to S3. The operations largely mirror those documented in S3's official docs . Generally, you would not call these endpoints directly, but rather use a tool or library designed to work with S3-like APIs. Because of that, some working knowledge of S3 and HTTP is assumed. Operations on buckets \u00b6 Buckets are represented via branch.repo , e.g. the master.images bucket corresponds to the master branch of the images repo. Creating buckets \u00b6 Route: PUT /branch.repo/ . If the repo does not exist, it is created. If the branch does not exist, it is likewise created. As per S3's behavior in some regions (but not all), trying to create the same bucket twice will return a BucketAlreadyOwnedByYou error. Deleting buckets \u00b6 Route: DELETE /branch.repo/ . Deletes the branch. If it is the last branch in the repo, the repo is also deleted. Unlike S3, you can delete non-empty branches. Listing buckets \u00b6 Route: GET / . Lists all of the branches across all of the repos as S3 buckets. Object operations \u00b6 Object operations act upon the HEAD commit of branches. Authorization-gated PFS branches are not supported. Writing objects \u00b6 Route: PUT /branch.repo/filepath . Writes the PFS file at filepath in an atomic commit on the HEAD of branch . Any existing file content is overwritten. Unlike S3, there is no limit to upload size. The s3gateway does not support multipart uploads, but you can use this endpoint to upload very large files. We recommend setting the Content-MD5 request header - especially for larger files - to ensure data integrity. Some S3 libraries and clients will detect that our s3gateway does not support multipart uploads and automatically fallback to using this endpoint. Notably, this includes minio. Removing objects \u00b6 Route: DELETE /branch.repo/filepath . Deletes the PFS file filepath in an atomic commit on the HEAD of branch . Listing objects \u00b6 Route: GET /branch.repo/ Only S3's list objects v1 is supported. PFS directories are represented via CommonPrefixes . This largely mirrors how S3 is used in practice, but leads to a couple of differences: * If you set the delimiter parameter, it must be / . * Empty directories are included in listed results. With regard to listed results: * Due to PFS peculiarities, the LastModified field references when the most recent commit to the branch happened, which may or may not have modified the specific object listed. * The HTTP ETag field does not use MD5, but is a cryptographically secure hash of the file contents. * The S3 StorageClass and Owner fields always have the same filler value. Getting objects \u00b6 Route: GET /branch.repo/filepath . There is support for range queries and conditional requests, however error response bodies for bad requests using these headers are not standard S3 XML. With regard to HTTP response headers: * Due to PFS peculiarities, the HTTP Last-Modified header references when the most recent commit to the branch happened, which may or may not have modified this specific object. * The HTTP ETag does not use MD5, but is a cryptographically secure hash of the file contents.","title":"S3 Gateway API Reference"},{"location":"reference/s3gateway_api/#s3gateway-api","text":"This outlines the HTTP API exposed by the s3gateway and any peculiarities relative to S3. The operations largely mirror those documented in S3's official docs . Generally, you would not call these endpoints directly, but rather use a tool or library designed to work with S3-like APIs. Because of that, some working knowledge of S3 and HTTP is assumed.","title":"S3Gateway API"},{"location":"reference/s3gateway_api/#operations-on-buckets","text":"Buckets are represented via branch.repo , e.g. the master.images bucket corresponds to the master branch of the images repo.","title":"Operations on buckets"},{"location":"reference/s3gateway_api/#creating-buckets","text":"Route: PUT /branch.repo/ . If the repo does not exist, it is created. If the branch does not exist, it is likewise created. As per S3's behavior in some regions (but not all), trying to create the same bucket twice will return a BucketAlreadyOwnedByYou error.","title":"Creating buckets"},{"location":"reference/s3gateway_api/#deleting-buckets","text":"Route: DELETE /branch.repo/ . Deletes the branch. If it is the last branch in the repo, the repo is also deleted. Unlike S3, you can delete non-empty branches.","title":"Deleting buckets"},{"location":"reference/s3gateway_api/#listing-buckets","text":"Route: GET / . Lists all of the branches across all of the repos as S3 buckets.","title":"Listing buckets"},{"location":"reference/s3gateway_api/#object-operations","text":"Object operations act upon the HEAD commit of branches. Authorization-gated PFS branches are not supported.","title":"Object operations"},{"location":"reference/s3gateway_api/#writing-objects","text":"Route: PUT /branch.repo/filepath . Writes the PFS file at filepath in an atomic commit on the HEAD of branch . Any existing file content is overwritten. Unlike S3, there is no limit to upload size. The s3gateway does not support multipart uploads, but you can use this endpoint to upload very large files. We recommend setting the Content-MD5 request header - especially for larger files - to ensure data integrity. Some S3 libraries and clients will detect that our s3gateway does not support multipart uploads and automatically fallback to using this endpoint. Notably, this includes minio.","title":"Writing objects"},{"location":"reference/s3gateway_api/#removing-objects","text":"Route: DELETE /branch.repo/filepath . Deletes the PFS file filepath in an atomic commit on the HEAD of branch .","title":"Removing objects"},{"location":"reference/s3gateway_api/#listing-objects","text":"Route: GET /branch.repo/ Only S3's list objects v1 is supported. PFS directories are represented via CommonPrefixes . This largely mirrors how S3 is used in practice, but leads to a couple of differences: * If you set the delimiter parameter, it must be / . * Empty directories are included in listed results. With regard to listed results: * Due to PFS peculiarities, the LastModified field references when the most recent commit to the branch happened, which may or may not have modified the specific object listed. * The HTTP ETag field does not use MD5, but is a cryptographically secure hash of the file contents. * The S3 StorageClass and Owner fields always have the same filler value.","title":"Listing objects"},{"location":"reference/s3gateway_api/#getting-objects","text":"Route: GET /branch.repo/filepath . There is support for range queries and conditional requests, however error response bodies for bad requests using these headers are not standard S3 XML. With regard to HTTP response headers: * Due to PFS peculiarities, the HTTP Last-Modified header references when the most recent commit to the branch happened, which may or may not have modified this specific object. * The HTTP ETag does not use MD5, but is a cryptographically secure hash of the file contents.","title":"Getting objects"},{"location":"reference/pachctl/","text":"Pachctl Command Line Tool \u00b6 This document describes Pachyderm Command Line Interface (CLI) tool pachctl . .. toctree:: :maxdepth: 1 pachctl.md pachctl_auth.md pachctl_auth_activate.md pachctl_auth_check.md pachctl_auth_deactivate.md pachctl_auth_get-auth-token.md pachctl_auth_get-config.md pachctl_auth_get.md pachctl_auth_list-admins.md pachctl_auth_login.md pachctl_auth_logout.md pachctl_auth_modify-admins.md pachctl_auth_set-config.md pachctl_auth_set.md pachctl_auth_use-auth-token.md pachctl_auth_whoami.md pachctl_completion.md pachctl_config.md pachctl_config_delete.md pachctl_config_delete_context.md pachctl_config_get.md pachctl_config_get_active-context.md pachctl_config_get_context.md pachctl_config_get_metrics.md pachctl_config_list.md pachctl_config_list_context.md pachctl_config_set.md pachctl_config_set_active-context.md pachctl_config_set_context.md pachctl_config_set_metrics.md pachctl_config_update.md pachctl_config_update_context.md pachctl_copy.md pachctl_copy_file.md pachctl_create.md pachctl_create_branch.md pachctl_create_pipeline.md pachctl_create_repo.md pachctl_debug.md pachctl_debug_binary.md pachctl_debug_dump.md pachctl_debug_pprof.md pachctl_debug_profile.md pachctl_delete.md pachctl_delete_all.md pachctl_delete_branch.md pachctl_delete_commit.md pachctl_delete_file.md pachctl_delete_job.md pachctl_delete_pipeline.md pachctl_delete_repo.md pachctl_delete_transaction.md pachctl_deploy.md pachctl_deploy_amazon.md pachctl_deploy_custom.md pachctl_deploy_export-images.md pachctl_deploy_google.md pachctl_deploy_import-images.md pachctl_deploy_list-images.md pachctl_deploy_local.md pachctl_deploy_microsoft.md pachctl_deploy_storage.md pachctl_deploy_storage_amazon.md pachctl_deploy_storage_google.md pachctl_deploy_storage_microsoft.md pachctl_diff.md pachctl_diff_file.md pachctl_edit.md pachctl_edit_pipeline.md pachctl_enterprise.md pachctl_enterprise_activate.md pachctl_enterprise_get-state.md pachctl_extract.md pachctl_extract_pipeline.md pachctl_finish.md pachctl_finish_commit.md pachctl_finish_transaction.md pachctl_flush.md pachctl_flush_commit.md pachctl_flush_job.md pachctl_fsck.md pachctl_garbage-collect.md pachctl_get.md pachctl_get_file.md pachctl_get_object.md pachctl_get_tag.md pachctl_glob.md pachctl_glob_file.md pachctl_inspect.md pachctl_inspect_cluster.md pachctl_inspect_commit.md pachctl_inspect_datum.md pachctl_inspect_file.md pachctl_inspect_job.md pachctl_inspect_pipeline.md pachctl_inspect_repo.md pachctl_inspect_transaction.md pachctl_list.md pachctl_list_branch.md pachctl_list_commit.md pachctl_list_datum.md pachctl_list_file.md pachctl_list_job.md pachctl_list_pipeline.md pachctl_list_repo.md pachctl_list_transaction.md pachctl_logs.md pachctl_mount.md pachctl_port-forward.md pachctl_put.md pachctl_put_file.md pachctl_restart.md pachctl_restart_datum.md pachctl_restore.md pachctl_resume.md pachctl_resume_transaction.md pachctl_run.md pachctl_run_pipeline.md pachctl_start.md pachctl_start_commit.md pachctl_start_pipeline.md pachctl_start_transaction.md pachctl_stop.md pachctl_stop_job.md pachctl_stop_pipeline.md pachctl_stop_transaction.md pachctl_subscribe.md pachctl_subscribe_commit.md pachctl_undeploy.md pachctl_unmount.md pachctl_update-dash.md pachctl_update.md pachctl_update_pipeline.md pachctl_update_repo.md pachctl_version.md","title":"Home"},{"location":"reference/pachctl/#pachctl-command-line-tool","text":"This document describes Pachyderm Command Line Interface (CLI) tool pachctl . .. toctree:: :maxdepth: 1 pachctl.md pachctl_auth.md pachctl_auth_activate.md pachctl_auth_check.md pachctl_auth_deactivate.md pachctl_auth_get-auth-token.md pachctl_auth_get-config.md pachctl_auth_get.md pachctl_auth_list-admins.md pachctl_auth_login.md pachctl_auth_logout.md pachctl_auth_modify-admins.md pachctl_auth_set-config.md pachctl_auth_set.md pachctl_auth_use-auth-token.md pachctl_auth_whoami.md pachctl_completion.md pachctl_config.md pachctl_config_delete.md pachctl_config_delete_context.md pachctl_config_get.md pachctl_config_get_active-context.md pachctl_config_get_context.md pachctl_config_get_metrics.md pachctl_config_list.md pachctl_config_list_context.md pachctl_config_set.md pachctl_config_set_active-context.md pachctl_config_set_context.md pachctl_config_set_metrics.md pachctl_config_update.md pachctl_config_update_context.md pachctl_copy.md pachctl_copy_file.md pachctl_create.md pachctl_create_branch.md pachctl_create_pipeline.md pachctl_create_repo.md pachctl_debug.md pachctl_debug_binary.md pachctl_debug_dump.md pachctl_debug_pprof.md pachctl_debug_profile.md pachctl_delete.md pachctl_delete_all.md pachctl_delete_branch.md pachctl_delete_commit.md pachctl_delete_file.md pachctl_delete_job.md pachctl_delete_pipeline.md pachctl_delete_repo.md pachctl_delete_transaction.md pachctl_deploy.md pachctl_deploy_amazon.md pachctl_deploy_custom.md pachctl_deploy_export-images.md pachctl_deploy_google.md pachctl_deploy_import-images.md pachctl_deploy_list-images.md pachctl_deploy_local.md pachctl_deploy_microsoft.md pachctl_deploy_storage.md pachctl_deploy_storage_amazon.md pachctl_deploy_storage_google.md pachctl_deploy_storage_microsoft.md pachctl_diff.md pachctl_diff_file.md pachctl_edit.md pachctl_edit_pipeline.md pachctl_enterprise.md pachctl_enterprise_activate.md pachctl_enterprise_get-state.md pachctl_extract.md pachctl_extract_pipeline.md pachctl_finish.md pachctl_finish_commit.md pachctl_finish_transaction.md pachctl_flush.md pachctl_flush_commit.md pachctl_flush_job.md pachctl_fsck.md pachctl_garbage-collect.md pachctl_get.md pachctl_get_file.md pachctl_get_object.md pachctl_get_tag.md pachctl_glob.md pachctl_glob_file.md pachctl_inspect.md pachctl_inspect_cluster.md pachctl_inspect_commit.md pachctl_inspect_datum.md pachctl_inspect_file.md pachctl_inspect_job.md pachctl_inspect_pipeline.md pachctl_inspect_repo.md pachctl_inspect_transaction.md pachctl_list.md pachctl_list_branch.md pachctl_list_commit.md pachctl_list_datum.md pachctl_list_file.md pachctl_list_job.md pachctl_list_pipeline.md pachctl_list_repo.md pachctl_list_transaction.md pachctl_logs.md pachctl_mount.md pachctl_port-forward.md pachctl_put.md pachctl_put_file.md pachctl_restart.md pachctl_restart_datum.md pachctl_restore.md pachctl_resume.md pachctl_resume_transaction.md pachctl_run.md pachctl_run_pipeline.md pachctl_start.md pachctl_start_commit.md pachctl_start_pipeline.md pachctl_start_transaction.md pachctl_stop.md pachctl_stop_job.md pachctl_stop_pipeline.md pachctl_stop_transaction.md pachctl_subscribe.md pachctl_subscribe_commit.md pachctl_undeploy.md pachctl_unmount.md pachctl_update-dash.md pachctl_update.md pachctl_update_pipeline.md pachctl_update_repo.md pachctl_version.md","title":"Pachctl Command Line Tool"},{"location":"reference/pachctl/pachctl/","text":"pachctl \u00b6 Synopsis \u00b6 Access the Pachyderm API. Environment variables: PACHD_ADDRESS= : , the pachd server to connect to (e.g. 127.0.0.1:30650). PACH_CONFIG= , the path where pachctl will attempt to load your pach config. JAEGER_ENDPOINT= : , the Jaeger server to connect to, if PACH_TRACE is set PACH_TRACE={true,false}, If true, and JAEGER_ENDPOINT is set, attach a Jaeger trace to any outgoing RPCs Options \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl"},{"location":"reference/pachctl/pachctl/#pachctl","text":"","title":"pachctl"},{"location":"reference/pachctl/pachctl/#synopsis","text":"Access the Pachyderm API. Environment variables: PACHD_ADDRESS= : , the pachd server to connect to (e.g. 127.0.0.1:30650). PACH_CONFIG= , the path where pachctl will attempt to load your pach config. JAEGER_ENDPOINT= : , the Jaeger server to connect to, if PACH_TRACE is set PACH_TRACE={true,false}, If true, and JAEGER_ENDPOINT is set, attach a Jaeger trace to any outgoing RPCs","title":"Synopsis"},{"location":"reference/pachctl/pachctl/#options","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options"},{"location":"reference/pachctl/pachctl_auth/","text":"pachctl auth \u00b6 Auth commands manage access to data in a Pachyderm cluster Synopsis \u00b6 Auth commands manage access to data in a Pachyderm cluster Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth"},{"location":"reference/pachctl/pachctl_auth/#pachctl-auth","text":"Auth commands manage access to data in a Pachyderm cluster","title":"pachctl auth"},{"location":"reference/pachctl/pachctl_auth/#synopsis","text":"Auth commands manage access to data in a Pachyderm cluster","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_activate/","text":"pachctl auth activate \u00b6 Activate Pachyderm's auth system Synopsis \u00b6 Activate Pachyderm's auth system, and restrict access to existing data to the user running the command (or the argument to --initial-admin), who will be the first cluster admin pachctl auth activate Options \u00b6 --initial-admin string The subject (robot user or github user) who will be the first cluster admin; the user running 'activate' will identify as this user once auth is active. If you set 'initial-admin' to a robot user, pachctl will print that robot user's Pachyderm token; this token is effectively a root token, and if it's lost you will be locked out of your cluster Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth activate"},{"location":"reference/pachctl/pachctl_auth_activate/#pachctl-auth-activate","text":"Activate Pachyderm's auth system","title":"pachctl auth activate"},{"location":"reference/pachctl/pachctl_auth_activate/#synopsis","text":"Activate Pachyderm's auth system, and restrict access to existing data to the user running the command (or the argument to --initial-admin), who will be the first cluster admin pachctl auth activate","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_activate/#options","text":"--initial-admin string The subject (robot user or github user) who will be the first cluster admin; the user running 'activate' will identify as this user once auth is active. If you set 'initial-admin' to a robot user, pachctl will print that robot user's Pachyderm token; this token is effectively a root token, and if it's lost you will be locked out of your cluster","title":"Options"},{"location":"reference/pachctl/pachctl_auth_activate/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_check/","text":"pachctl auth check \u00b6 Check whether you have reader/writer/etc-level access to 'repo' Synopsis \u00b6 Check whether you have reader/writer/etc-level access to 'repo'. For example, 'pachctl auth check reader private-data' prints \"true\" if the you have at least \"reader\" access to the repo \"private-data\" (you could be a reader, writer, or owner). Unlike pachctl auth get , you do not need to have access to 'repo' to discover your own access level. pachctl auth check (none|reader|writer|owner) <repo> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth check"},{"location":"reference/pachctl/pachctl_auth_check/#pachctl-auth-check","text":"Check whether you have reader/writer/etc-level access to 'repo'","title":"pachctl auth check"},{"location":"reference/pachctl/pachctl_auth_check/#synopsis","text":"Check whether you have reader/writer/etc-level access to 'repo'. For example, 'pachctl auth check reader private-data' prints \"true\" if the you have at least \"reader\" access to the repo \"private-data\" (you could be a reader, writer, or owner). Unlike pachctl auth get , you do not need to have access to 'repo' to discover your own access level. pachctl auth check (none|reader|writer|owner) <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_check/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_deactivate/","text":"pachctl auth deactivate \u00b6 Delete all ACLs, tokens, and admins, and deactivate Pachyderm auth Synopsis \u00b6 Deactivate Pachyderm's auth system, which will delete ALL auth tokens, ACLs and admins, and expose all data in the cluster to any user with cluster access. Use with caution. pachctl auth deactivate Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth deactivate"},{"location":"reference/pachctl/pachctl_auth_deactivate/#pachctl-auth-deactivate","text":"Delete all ACLs, tokens, and admins, and deactivate Pachyderm auth","title":"pachctl auth deactivate"},{"location":"reference/pachctl/pachctl_auth_deactivate/#synopsis","text":"Deactivate Pachyderm's auth system, which will delete ALL auth tokens, ACLs and admins, and expose all data in the cluster to any user with cluster access. Use with caution. pachctl auth deactivate","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_deactivate/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_get-auth-token/","text":"pachctl auth get-auth-token \u00b6 Get an auth token that authenticates the holder as \"username\" Synopsis \u00b6 Get an auth token that authenticates the holder as \"username\"; this can only be called by cluster admins pachctl auth get-auth-token <username> Options \u00b6 -q, --quiet if set, only print the resulting token (if successful). This is useful for scripting, as the output can be piped to use-auth-token Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth get auth token"},{"location":"reference/pachctl/pachctl_auth_get-auth-token/#pachctl-auth-get-auth-token","text":"Get an auth token that authenticates the holder as \"username\"","title":"pachctl auth get-auth-token"},{"location":"reference/pachctl/pachctl_auth_get-auth-token/#synopsis","text":"Get an auth token that authenticates the holder as \"username\"; this can only be called by cluster admins pachctl auth get-auth-token <username>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_get-auth-token/#options","text":"-q, --quiet if set, only print the resulting token (if successful). This is useful for scripting, as the output can be piped to use-auth-token","title":"Options"},{"location":"reference/pachctl/pachctl_auth_get-auth-token/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_get-config/","text":"pachctl auth get-config \u00b6 Retrieve Pachyderm's current auth configuration Synopsis \u00b6 Retrieve Pachyderm's current auth configuration pachctl auth get-config Options \u00b6 -o, --output-format string output format (\"json\" or \"yaml\") (default \"json\") Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth get config"},{"location":"reference/pachctl/pachctl_auth_get-config/#pachctl-auth-get-config","text":"Retrieve Pachyderm's current auth configuration","title":"pachctl auth get-config"},{"location":"reference/pachctl/pachctl_auth_get-config/#synopsis","text":"Retrieve Pachyderm's current auth configuration pachctl auth get-config","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_get-config/#options","text":"-o, --output-format string output format (\"json\" or \"yaml\") (default \"json\")","title":"Options"},{"location":"reference/pachctl/pachctl_auth_get-config/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_get-otp/","text":"pachctl auth get-otp \u00b6 Get a one-time password that authenticates the holder as \"username\" Synopsis \u00b6 Get a one-time password that authenticates the holder as \"username\" pachctl auth get-otp <username> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth get otp"},{"location":"reference/pachctl/pachctl_auth_get-otp/#pachctl-auth-get-otp","text":"Get a one-time password that authenticates the holder as \"username\"","title":"pachctl auth get-otp"},{"location":"reference/pachctl/pachctl_auth_get-otp/#synopsis","text":"Get a one-time password that authenticates the holder as \"username\" pachctl auth get-otp <username>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_get-otp/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_get/","text":"pachctl auth get \u00b6 Get the ACL for 'repo' or the access that 'username' has to 'repo' Synopsis \u00b6 Get the ACL for 'repo' or the access that 'username' has to 'repo'. For example, 'pachctl auth get github-alice private-data' prints \"reader\", \"writer\", \"owner\", or \"none\", depending on the privileges that \"github-alice\" has in \"repo\". Currently all Pachyderm authentication uses GitHub OAuth, so 'username' must be a GitHub username pachctl auth get [<username>] <repo> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth get"},{"location":"reference/pachctl/pachctl_auth_get/#pachctl-auth-get","text":"Get the ACL for 'repo' or the access that 'username' has to 'repo'","title":"pachctl auth get"},{"location":"reference/pachctl/pachctl_auth_get/#synopsis","text":"Get the ACL for 'repo' or the access that 'username' has to 'repo'. For example, 'pachctl auth get github-alice private-data' prints \"reader\", \"writer\", \"owner\", or \"none\", depending on the privileges that \"github-alice\" has in \"repo\". Currently all Pachyderm authentication uses GitHub OAuth, so 'username' must be a GitHub username pachctl auth get [<username>] <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_get/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_list-admins/","text":"pachctl auth list-admins \u00b6 List the current cluster admins Synopsis \u00b6 List the current cluster admins pachctl auth list-admins Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth list admins"},{"location":"reference/pachctl/pachctl_auth_list-admins/#pachctl-auth-list-admins","text":"List the current cluster admins","title":"pachctl auth list-admins"},{"location":"reference/pachctl/pachctl_auth_list-admins/#synopsis","text":"List the current cluster admins pachctl auth list-admins","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_list-admins/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_login/","text":"pachctl auth login \u00b6 Log in to Pachyderm Synopsis \u00b6 Login to Pachyderm. Any resources that have been restricted to the account you have with your ID provider (e.g. GitHub, Okta) account will subsequently be accessible. pachctl auth login Options \u00b6 -o, --one-time-password If set, authenticate with a Dash-provided One-Time Password, rather than via GitHub Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth login"},{"location":"reference/pachctl/pachctl_auth_login/#pachctl-auth-login","text":"Log in to Pachyderm","title":"pachctl auth login"},{"location":"reference/pachctl/pachctl_auth_login/#synopsis","text":"Login to Pachyderm. Any resources that have been restricted to the account you have with your ID provider (e.g. GitHub, Okta) account will subsequently be accessible. pachctl auth login","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_login/#options","text":"-o, --one-time-password If set, authenticate with a Dash-provided One-Time Password, rather than via GitHub","title":"Options"},{"location":"reference/pachctl/pachctl_auth_login/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_logout/","text":"pachctl auth logout \u00b6 Log out of Pachyderm by deleting your local credential Synopsis \u00b6 Log out of Pachyderm by deleting your local credential. Note that it's not necessary to log out before logging in with another account (simply run 'pachctl auth login' twice) but 'logout' can be useful on shared workstations. pachctl auth logout Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth logout"},{"location":"reference/pachctl/pachctl_auth_logout/#pachctl-auth-logout","text":"Log out of Pachyderm by deleting your local credential","title":"pachctl auth logout"},{"location":"reference/pachctl/pachctl_auth_logout/#synopsis","text":"Log out of Pachyderm by deleting your local credential. Note that it's not necessary to log out before logging in with another account (simply run 'pachctl auth login' twice) but 'logout' can be useful on shared workstations. pachctl auth logout","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_logout/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_modify-admins/","text":"pachctl auth modify-admins \u00b6 Modify the current cluster admins Synopsis \u00b6 Modify the current cluster admins. --add accepts a comma-separated list of users to grant admin status, and --remove accepts a comma-separated list of users to revoke admin status pachctl auth modify-admins Options \u00b6 --add strings Comma-separated list of users to grant admin status --remove strings Comma-separated list of users revoke admin status Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth modify admins"},{"location":"reference/pachctl/pachctl_auth_modify-admins/#pachctl-auth-modify-admins","text":"Modify the current cluster admins","title":"pachctl auth modify-admins"},{"location":"reference/pachctl/pachctl_auth_modify-admins/#synopsis","text":"Modify the current cluster admins. --add accepts a comma-separated list of users to grant admin status, and --remove accepts a comma-separated list of users to revoke admin status pachctl auth modify-admins","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_modify-admins/#options","text":"--add strings Comma-separated list of users to grant admin status --remove strings Comma-separated list of users revoke admin status","title":"Options"},{"location":"reference/pachctl/pachctl_auth_modify-admins/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_set-config/","text":"pachctl auth set-config \u00b6 Set Pachyderm's current auth configuration Synopsis \u00b6 Set Pachyderm's current auth configuration pachctl auth set-config Options \u00b6 -f, --file string input file (to use as the new config (default \"-\") Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth set config"},{"location":"reference/pachctl/pachctl_auth_set-config/#pachctl-auth-set-config","text":"Set Pachyderm's current auth configuration","title":"pachctl auth set-config"},{"location":"reference/pachctl/pachctl_auth_set-config/#synopsis","text":"Set Pachyderm's current auth configuration pachctl auth set-config","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_set-config/#options","text":"-f, --file string input file (to use as the new config (default \"-\")","title":"Options"},{"location":"reference/pachctl/pachctl_auth_set-config/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_set/","text":"pachctl auth set \u00b6 Set the scope of access that 'username' has to 'repo' Synopsis \u00b6 Set the scope of access that 'username' has to 'repo'. For example, 'pachctl auth set github-alice none private-data' prevents \"github-alice\" from interacting with the \"private-data\" repo in any way (the default). Similarly, 'pachctl auth set github-alice reader private-data' would let \"github-alice\" read from \"private-data\" but not create commits (writer) or modify the repo's access permissions (owner). Currently all Pachyderm authentication uses GitHub OAuth, so 'username' must be a GitHub username pachctl auth set <username> (none|reader|writer|owner) <repo> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth set"},{"location":"reference/pachctl/pachctl_auth_set/#pachctl-auth-set","text":"Set the scope of access that 'username' has to 'repo'","title":"pachctl auth set"},{"location":"reference/pachctl/pachctl_auth_set/#synopsis","text":"Set the scope of access that 'username' has to 'repo'. For example, 'pachctl auth set github-alice none private-data' prevents \"github-alice\" from interacting with the \"private-data\" repo in any way (the default). Similarly, 'pachctl auth set github-alice reader private-data' would let \"github-alice\" read from \"private-data\" but not create commits (writer) or modify the repo's access permissions (owner). Currently all Pachyderm authentication uses GitHub OAuth, so 'username' must be a GitHub username pachctl auth set <username> (none|reader|writer|owner) <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_set/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_use-auth-token/","text":"pachctl auth use-auth-token \u00b6 Read a Pachyderm auth token from stdin, and write it to the current user's Pachyderm config file Synopsis \u00b6 Read a Pachyderm auth token from stdin, and write it to the current user's Pachyderm config file pachctl auth use-auth-token Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth use auth token"},{"location":"reference/pachctl/pachctl_auth_use-auth-token/#pachctl-auth-use-auth-token","text":"Read a Pachyderm auth token from stdin, and write it to the current user's Pachyderm config file","title":"pachctl auth use-auth-token"},{"location":"reference/pachctl/pachctl_auth_use-auth-token/#synopsis","text":"Read a Pachyderm auth token from stdin, and write it to the current user's Pachyderm config file pachctl auth use-auth-token","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_use-auth-token/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_auth_whoami/","text":"pachctl auth whoami \u00b6 Print your Pachyderm identity Synopsis \u00b6 Print your Pachyderm identity. pachctl auth whoami Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl auth whoami"},{"location":"reference/pachctl/pachctl_auth_whoami/#pachctl-auth-whoami","text":"Print your Pachyderm identity","title":"pachctl auth whoami"},{"location":"reference/pachctl/pachctl_auth_whoami/#synopsis","text":"Print your Pachyderm identity. pachctl auth whoami","title":"Synopsis"},{"location":"reference/pachctl/pachctl_auth_whoami/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_completion/","text":"pachctl completion \u00b6 Print or install the bash completion code. Synopsis \u00b6 Print or install the bash completion code. This should be placed as the file pachctl in the bash completion directory (by default this is /etc/bash_completion.d . If bash-completion was installed via homebrew, this would be $(brew --prefix)/etc/bash_completion.d .) pachctl completion Options \u00b6 --install Install the completion. --path /etc/bash_completion.d/ Path to install the completion to. This will default to /etc/bash_completion.d/ if unspecified. (default \"/etc/bash_completion.d/pachctl\") Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl completion"},{"location":"reference/pachctl/pachctl_completion/#pachctl-completion","text":"Print or install the bash completion code.","title":"pachctl completion"},{"location":"reference/pachctl/pachctl_completion/#synopsis","text":"Print or install the bash completion code. This should be placed as the file pachctl in the bash completion directory (by default this is /etc/bash_completion.d . If bash-completion was installed via homebrew, this would be $(brew --prefix)/etc/bash_completion.d .) pachctl completion","title":"Synopsis"},{"location":"reference/pachctl/pachctl_completion/#options","text":"--install Install the completion. --path /etc/bash_completion.d/ Path to install the completion to. This will default to /etc/bash_completion.d/ if unspecified. (default \"/etc/bash_completion.d/pachctl\")","title":"Options"},{"location":"reference/pachctl/pachctl_completion/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config/","text":"pachctl config \u00b6 Manages the pachyderm config. Synopsis \u00b6 Gets/sets pachyderm config values. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config"},{"location":"reference/pachctl/pachctl_config/#pachctl-config","text":"Manages the pachyderm config.","title":"pachctl config"},{"location":"reference/pachctl/pachctl_config/#synopsis","text":"Gets/sets pachyderm config values.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_delete/","text":"pachctl config delete \u00b6 Commands for deleting pachyderm config values Synopsis \u00b6 Commands for deleting pachyderm config values Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config delete"},{"location":"reference/pachctl/pachctl_config_delete/#pachctl-config-delete","text":"Commands for deleting pachyderm config values","title":"pachctl config delete"},{"location":"reference/pachctl/pachctl_config_delete/#synopsis","text":"Commands for deleting pachyderm config values","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_delete/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_delete_context/","text":"pachctl config delete context \u00b6 Deletes a context. Synopsis \u00b6 Deletes a context. pachctl config delete context Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config delete context"},{"location":"reference/pachctl/pachctl_config_delete_context/#pachctl-config-delete-context","text":"Deletes a context.","title":"pachctl config delete context"},{"location":"reference/pachctl/pachctl_config_delete_context/#synopsis","text":"Deletes a context. pachctl config delete context","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_delete_context/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_get/","text":"pachctl config get \u00b6 Commands for getting pachyderm config values Synopsis \u00b6 Commands for getting pachyderm config values Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config get"},{"location":"reference/pachctl/pachctl_config_get/#pachctl-config-get","text":"Commands for getting pachyderm config values","title":"pachctl config get"},{"location":"reference/pachctl/pachctl_config_get/#synopsis","text":"Commands for getting pachyderm config values","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_get/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_get_active-context/","text":"pachctl config get active-context \u00b6 Gets the currently active context. Synopsis \u00b6 Gets the currently active context. pachctl config get active-context Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config get active context"},{"location":"reference/pachctl/pachctl_config_get_active-context/#pachctl-config-get-active-context","text":"Gets the currently active context.","title":"pachctl config get active-context"},{"location":"reference/pachctl/pachctl_config_get_active-context/#synopsis","text":"Gets the currently active context. pachctl config get active-context","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_get_active-context/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_get_context/","text":"pachctl config get context \u00b6 Gets a context. Synopsis \u00b6 Gets the config of a context by its name. pachctl config get context Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config get context"},{"location":"reference/pachctl/pachctl_config_get_context/#pachctl-config-get-context","text":"Gets a context.","title":"pachctl config get context"},{"location":"reference/pachctl/pachctl_config_get_context/#synopsis","text":"Gets the config of a context by its name. pachctl config get context","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_get_context/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_get_metrics/","text":"pachctl config get metrics \u00b6 Gets whether metrics are enabled. Synopsis \u00b6 Gets whether metrics are enabled. pachctl config get metrics Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config get metrics"},{"location":"reference/pachctl/pachctl_config_get_metrics/#pachctl-config-get-metrics","text":"Gets whether metrics are enabled.","title":"pachctl config get metrics"},{"location":"reference/pachctl/pachctl_config_get_metrics/#synopsis","text":"Gets whether metrics are enabled. pachctl config get metrics","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_get_metrics/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_list/","text":"pachctl config list \u00b6 Commands for listing pachyderm config values Synopsis \u00b6 Commands for listing pachyderm config values Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config list"},{"location":"reference/pachctl/pachctl_config_list/#pachctl-config-list","text":"Commands for listing pachyderm config values","title":"pachctl config list"},{"location":"reference/pachctl/pachctl_config_list/#synopsis","text":"Commands for listing pachyderm config values","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_list/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_list_context/","text":"pachctl config list context \u00b6 Lists contexts. Synopsis \u00b6 Lists contexts. pachctl config list context Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config list context"},{"location":"reference/pachctl/pachctl_config_list_context/#pachctl-config-list-context","text":"Lists contexts.","title":"pachctl config list context"},{"location":"reference/pachctl/pachctl_config_list_context/#synopsis","text":"Lists contexts. pachctl config list context","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_list_context/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_set/","text":"pachctl config set \u00b6 Commands for setting pachyderm config values Synopsis \u00b6 Commands for setting pachyderm config values Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config set"},{"location":"reference/pachctl/pachctl_config_set/#pachctl-config-set","text":"Commands for setting pachyderm config values","title":"pachctl config set"},{"location":"reference/pachctl/pachctl_config_set/#synopsis","text":"Commands for setting pachyderm config values","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_set/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_set_active-context/","text":"pachctl config set active-context \u00b6 Sets the currently active context. Synopsis \u00b6 Sets the currently active context. pachctl config set active-context Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config set active context"},{"location":"reference/pachctl/pachctl_config_set_active-context/#pachctl-config-set-active-context","text":"Sets the currently active context.","title":"pachctl config set active-context"},{"location":"reference/pachctl/pachctl_config_set_active-context/#synopsis","text":"Sets the currently active context. pachctl config set active-context","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_set_active-context/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_set_context/","text":"pachctl config set context \u00b6 Set a context. Synopsis \u00b6 Set a context config from a given name and JSON stdin. pachctl config set context Options \u00b6 --overwrite Overwrite a context if it already exists. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config set context"},{"location":"reference/pachctl/pachctl_config_set_context/#pachctl-config-set-context","text":"Set a context.","title":"pachctl config set context"},{"location":"reference/pachctl/pachctl_config_set_context/#synopsis","text":"Set a context config from a given name and JSON stdin. pachctl config set context","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_set_context/#options","text":"--overwrite Overwrite a context if it already exists.","title":"Options"},{"location":"reference/pachctl/pachctl_config_set_context/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_set_metrics/","text":"pachctl config set metrics \u00b6 Sets whether metrics are enabled. Synopsis \u00b6 Sets whether metrics are enabled. pachctl config set metrics Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config set metrics"},{"location":"reference/pachctl/pachctl_config_set_metrics/#pachctl-config-set-metrics","text":"Sets whether metrics are enabled.","title":"pachctl config set metrics"},{"location":"reference/pachctl/pachctl_config_set_metrics/#synopsis","text":"Sets whether metrics are enabled. pachctl config set metrics","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_set_metrics/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_update/","text":"pachctl config update \u00b6 Commands for updating pachyderm config values Synopsis \u00b6 Commands for updating pachyderm config values Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config update"},{"location":"reference/pachctl/pachctl_config_update/#pachctl-config-update","text":"Commands for updating pachyderm config values","title":"pachctl config update"},{"location":"reference/pachctl/pachctl_config_update/#synopsis","text":"Commands for updating pachyderm config values","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_update/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_config_update_context/","text":"pachctl config update context \u00b6 Updates a context. Synopsis \u00b6 Updates an existing context config from a given name. pachctl config update context Options \u00b6 --auth-info string Set a new auth info. --cluster-name string Set a new cluster name. --namespace string Set a new namespace. --pachd-address string Set a new name pachd address. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl config update context"},{"location":"reference/pachctl/pachctl_config_update_context/#pachctl-config-update-context","text":"Updates a context.","title":"pachctl config update context"},{"location":"reference/pachctl/pachctl_config_update_context/#synopsis","text":"Updates an existing context config from a given name. pachctl config update context","title":"Synopsis"},{"location":"reference/pachctl/pachctl_config_update_context/#options","text":"--auth-info string Set a new auth info. --cluster-name string Set a new cluster name. --namespace string Set a new namespace. --pachd-address string Set a new name pachd address.","title":"Options"},{"location":"reference/pachctl/pachctl_config_update_context/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_copy/","text":"pachctl copy \u00b6 Copy a Pachyderm resource. Synopsis \u00b6 Copy a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl copy"},{"location":"reference/pachctl/pachctl_copy/#pachctl-copy","text":"Copy a Pachyderm resource.","title":"pachctl copy"},{"location":"reference/pachctl/pachctl_copy/#synopsis","text":"Copy a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_copy/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_copy_file/","text":"pachctl copy file \u00b6 Copy files between pfs paths. Synopsis \u00b6 Copy files between pfs paths. pachctl copy file <src-repo>@<src-branch-or-commit>:<src-path> <dst-repo>@<dst-branch-or-commit>:<dst-path> Options \u00b6 -o, --overwrite Overwrite the existing content of the file, either from previous commits or previous calls to 'put file' within this commit. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl copy file"},{"location":"reference/pachctl/pachctl_copy_file/#pachctl-copy-file","text":"Copy files between pfs paths.","title":"pachctl copy file"},{"location":"reference/pachctl/pachctl_copy_file/#synopsis","text":"Copy files between pfs paths. pachctl copy file <src-repo>@<src-branch-or-commit>:<src-path> <dst-repo>@<dst-branch-or-commit>:<dst-path>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_copy_file/#options","text":"-o, --overwrite Overwrite the existing content of the file, either from previous commits or previous calls to 'put file' within this commit.","title":"Options"},{"location":"reference/pachctl/pachctl_copy_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_create/","text":"pachctl create \u00b6 Create a new instance of a Pachyderm resource. Synopsis \u00b6 Create a new instance of a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl create"},{"location":"reference/pachctl/pachctl_create/#pachctl-create","text":"Create a new instance of a Pachyderm resource.","title":"pachctl create"},{"location":"reference/pachctl/pachctl_create/#synopsis","text":"Create a new instance of a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_create/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_create_branch/","text":"pachctl create branch \u00b6 Create a new branch, or update an existing branch, on a repo. Synopsis \u00b6 Create a new branch, or update an existing branch, on a repo, starting a commit on the branch will also create it, so there's often no need to call this. pachctl create branch <repo>@<branch-or-commit> Options \u00b6 --head string The head of the newly created branch. -p, --provenance []string The provenance for the branch. format: <repo>@<branch-or-commit> (default []) Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl create branch"},{"location":"reference/pachctl/pachctl_create_branch/#pachctl-create-branch","text":"Create a new branch, or update an existing branch, on a repo.","title":"pachctl create branch"},{"location":"reference/pachctl/pachctl_create_branch/#synopsis","text":"Create a new branch, or update an existing branch, on a repo, starting a commit on the branch will also create it, so there's often no need to call this. pachctl create branch <repo>@<branch-or-commit>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_create_branch/#options","text":"--head string The head of the newly created branch. -p, --provenance []string The provenance for the branch. format: <repo>@<branch-or-commit> (default [])","title":"Options"},{"location":"reference/pachctl/pachctl_create_branch/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_create_pipeline/","text":"pachctl create pipeline \u00b6 Create a new pipeline. Synopsis \u00b6 Create a new pipeline from a pipeline specification. For details on the format, see http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html . pachctl create pipeline Options \u00b6 -b, --build If true, build and push local docker images into the docker registry. -f, --file string The JSON file containing the pipeline, it can be a url or local file. - reads from stdin. (default \"-\") -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default \"docker.io\") -u, --username string The username to push images as, defaults to your docker username. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl create pipeline"},{"location":"reference/pachctl/pachctl_create_pipeline/#pachctl-create-pipeline","text":"Create a new pipeline.","title":"pachctl create pipeline"},{"location":"reference/pachctl/pachctl_create_pipeline/#synopsis","text":"Create a new pipeline from a pipeline specification. For details on the format, see http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html . pachctl create pipeline","title":"Synopsis"},{"location":"reference/pachctl/pachctl_create_pipeline/#options","text":"-b, --build If true, build and push local docker images into the docker registry. -f, --file string The JSON file containing the pipeline, it can be a url or local file. - reads from stdin. (default \"-\") -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default \"docker.io\") -u, --username string The username to push images as, defaults to your docker username.","title":"Options"},{"location":"reference/pachctl/pachctl_create_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_create_repo/","text":"pachctl create repo \u00b6 Create a new repo. Synopsis \u00b6 Create a new repo. pachctl create repo <repo> Options \u00b6 -d, --description string A description of the repo. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl create repo"},{"location":"reference/pachctl/pachctl_create_repo/#pachctl-create-repo","text":"Create a new repo.","title":"pachctl create repo"},{"location":"reference/pachctl/pachctl_create_repo/#synopsis","text":"Create a new repo. pachctl create repo <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_create_repo/#options","text":"-d, --description string A description of the repo.","title":"Options"},{"location":"reference/pachctl/pachctl_create_repo/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_debug/","text":"pachctl debug \u00b6 Debug commands for analyzing a running cluster. Synopsis \u00b6 Debug commands for analyzing a running cluster. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl debug"},{"location":"reference/pachctl/pachctl_debug/#pachctl-debug","text":"Debug commands for analyzing a running cluster.","title":"pachctl debug"},{"location":"reference/pachctl/pachctl_debug/#synopsis","text":"Debug commands for analyzing a running cluster.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_debug/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_debug_binary/","text":"pachctl debug binary \u00b6 Return the binary the server is running. Synopsis \u00b6 Return the binary the server is running. pachctl debug binary Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl debug binary"},{"location":"reference/pachctl/pachctl_debug_binary/#pachctl-debug-binary","text":"Return the binary the server is running.","title":"pachctl debug binary"},{"location":"reference/pachctl/pachctl_debug_binary/#synopsis","text":"Return the binary the server is running. pachctl debug binary","title":"Synopsis"},{"location":"reference/pachctl/pachctl_debug_binary/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_debug_dump/","text":"pachctl debug dump \u00b6 Return a dump of running goroutines. Synopsis \u00b6 Return a dump of running goroutines. pachctl debug dump Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl debug dump"},{"location":"reference/pachctl/pachctl_debug_dump/#pachctl-debug-dump","text":"Return a dump of running goroutines.","title":"pachctl debug dump"},{"location":"reference/pachctl/pachctl_debug_dump/#synopsis","text":"Return a dump of running goroutines. pachctl debug dump","title":"Synopsis"},{"location":"reference/pachctl/pachctl_debug_dump/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_debug_pprof/","text":"pachctl debug pprof \u00b6 Analyze a profile of pachd in pprof. Synopsis \u00b6 Analyze a profile of pachd in pprof. pachctl debug pprof <profile> Options \u00b6 --binary-file string File to write the binary to. (default \"binary\") -d, --duration duration Duration to run a CPU profile for. (default 1m0s) --profile-file string File to write the profile to. (default \"profile\") Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl debug pprof"},{"location":"reference/pachctl/pachctl_debug_pprof/#pachctl-debug-pprof","text":"Analyze a profile of pachd in pprof.","title":"pachctl debug pprof"},{"location":"reference/pachctl/pachctl_debug_pprof/#synopsis","text":"Analyze a profile of pachd in pprof. pachctl debug pprof <profile>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_debug_pprof/#options","text":"--binary-file string File to write the binary to. (default \"binary\") -d, --duration duration Duration to run a CPU profile for. (default 1m0s) --profile-file string File to write the profile to. (default \"profile\")","title":"Options"},{"location":"reference/pachctl/pachctl_debug_pprof/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_debug_profile/","text":"pachctl debug profile \u00b6 Return a profile from the server. Synopsis \u00b6 Return a profile from the server. pachctl debug profile <profile> Options \u00b6 -d, --duration duration Duration to run a CPU profile for. (default 1m0s) Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl debug profile"},{"location":"reference/pachctl/pachctl_debug_profile/#pachctl-debug-profile","text":"Return a profile from the server.","title":"pachctl debug profile"},{"location":"reference/pachctl/pachctl_debug_profile/#synopsis","text":"Return a profile from the server. pachctl debug profile <profile>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_debug_profile/#options","text":"-d, --duration duration Duration to run a CPU profile for. (default 1m0s)","title":"Options"},{"location":"reference/pachctl/pachctl_debug_profile/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete/","text":"pachctl delete \u00b6 Delete an existing Pachyderm resource. Synopsis \u00b6 Delete an existing Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete"},{"location":"reference/pachctl/pachctl_delete/#pachctl-delete","text":"Delete an existing Pachyderm resource.","title":"pachctl delete"},{"location":"reference/pachctl/pachctl_delete/#synopsis","text":"Delete an existing Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_all/","text":"pachctl delete all \u00b6 Delete everything. Synopsis \u00b6 Delete all repos, commits, files, pipelines and jobs. This resets the cluster to its initial state. pachctl delete all Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete all"},{"location":"reference/pachctl/pachctl_delete_all/#pachctl-delete-all","text":"Delete everything.","title":"pachctl delete all"},{"location":"reference/pachctl/pachctl_delete_all/#synopsis","text":"Delete all repos, commits, files, pipelines and jobs. This resets the cluster to its initial state. pachctl delete all","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_all/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_branch/","text":"pachctl delete branch \u00b6 Delete a branch Synopsis \u00b6 Delete a branch, while leaving the commits intact pachctl delete branch <repo>@<branch-or-commit> Options \u00b6 -f, --force remove the branch regardless of errors; use with care Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete branch"},{"location":"reference/pachctl/pachctl_delete_branch/#pachctl-delete-branch","text":"Delete a branch","title":"pachctl delete branch"},{"location":"reference/pachctl/pachctl_delete_branch/#synopsis","text":"Delete a branch, while leaving the commits intact pachctl delete branch <repo>@<branch-or-commit>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_branch/#options","text":"-f, --force remove the branch regardless of errors; use with care","title":"Options"},{"location":"reference/pachctl/pachctl_delete_branch/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_commit/","text":"pachctl delete commit \u00b6 Delete an input commit. Synopsis \u00b6 Delete an input commit. An input is a commit which is not the output of a pipeline. pachctl delete commit <repo>@<branch-or-commit> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete commit"},{"location":"reference/pachctl/pachctl_delete_commit/#pachctl-delete-commit","text":"Delete an input commit.","title":"pachctl delete commit"},{"location":"reference/pachctl/pachctl_delete_commit/#synopsis","text":"Delete an input commit. An input is a commit which is not the output of a pipeline. pachctl delete commit <repo>@<branch-or-commit>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_commit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_file/","text":"pachctl delete file \u00b6 Delete a file. Synopsis \u00b6 Delete a file. pachctl delete file <repo>@<branch-or-commit>:<path/in/pfs> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete file"},{"location":"reference/pachctl/pachctl_delete_file/#pachctl-delete-file","text":"Delete a file.","title":"pachctl delete file"},{"location":"reference/pachctl/pachctl_delete_file/#synopsis","text":"Delete a file. pachctl delete file <repo>@<branch-or-commit>:<path/in/pfs>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_job/","text":"pachctl delete job \u00b6 Delete a job. Synopsis \u00b6 Delete a job. pachctl delete job <job> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete job"},{"location":"reference/pachctl/pachctl_delete_job/#pachctl-delete-job","text":"Delete a job.","title":"pachctl delete job"},{"location":"reference/pachctl/pachctl_delete_job/#synopsis","text":"Delete a job. pachctl delete job <job>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_job/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_pipeline/","text":"pachctl delete pipeline \u00b6 Delete a pipeline. Synopsis \u00b6 Delete a pipeline. pachctl delete pipeline (<pipeline>|--all) Options \u00b6 --all delete all pipelines -f, --force delete the pipeline regardless of errors; use with care Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete pipeline"},{"location":"reference/pachctl/pachctl_delete_pipeline/#pachctl-delete-pipeline","text":"Delete a pipeline.","title":"pachctl delete pipeline"},{"location":"reference/pachctl/pachctl_delete_pipeline/#synopsis","text":"Delete a pipeline. pachctl delete pipeline (<pipeline>|--all)","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_pipeline/#options","text":"--all delete all pipelines -f, --force delete the pipeline regardless of errors; use with care","title":"Options"},{"location":"reference/pachctl/pachctl_delete_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_repo/","text":"pachctl delete repo \u00b6 Delete a repo. Synopsis \u00b6 Delete a repo. pachctl delete repo <repo> Options \u00b6 --all remove all repos -f, --force remove the repo regardless of errors; use with care Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete repo"},{"location":"reference/pachctl/pachctl_delete_repo/#pachctl-delete-repo","text":"Delete a repo.","title":"pachctl delete repo"},{"location":"reference/pachctl/pachctl_delete_repo/#synopsis","text":"Delete a repo. pachctl delete repo <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_repo/#options","text":"--all remove all repos -f, --force remove the repo regardless of errors; use with care","title":"Options"},{"location":"reference/pachctl/pachctl_delete_repo/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_delete_transaction/","text":"pachctl delete transaction \u00b6 Cancel and delete an existing transaction. Synopsis \u00b6 Cancel and delete an existing transaction. pachctl delete transaction [<transaction>] Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl delete transaction"},{"location":"reference/pachctl/pachctl_delete_transaction/#pachctl-delete-transaction","text":"Cancel and delete an existing transaction.","title":"pachctl delete transaction"},{"location":"reference/pachctl/pachctl_delete_transaction/#synopsis","text":"Cancel and delete an existing transaction. pachctl delete transaction [<transaction>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_delete_transaction/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy/","text":"pachctl deploy \u00b6 Deploy a Pachyderm cluster. Synopsis \u00b6 Deploy a Pachyderm cluster. Options \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl deploy"},{"location":"reference/pachctl/pachctl_deploy/#pachctl-deploy","text":"Deploy a Pachyderm cluster.","title":"pachctl deploy"},{"location":"reference/pachctl/pachctl_deploy/#synopsis","text":"Deploy a Pachyderm cluster.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy/#options","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd)","title":"Options"},{"location":"reference/pachctl/pachctl_deploy/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_amazon/","text":"pachctl deploy amazon \u00b6 Deploy a Pachyderm cluster running on AWS. Synopsis \u00b6 Deploy a Pachyderm cluster running on AWS. : An S3 bucket where Pachyderm will store PFS data. : The AWS region where Pachyderm is being deployed (e.g. us-west-1) : Size of EBS volumes, in GB (assumed to all be the same). pachctl deploy amazon <bucket-name> <region> <disk-size> Options \u00b6 --cloudfront-distribution string Deploying on AWS with cloudfront is currently an alpha feature. No security restrictions have beenapplied to cloudfront, making all data public (obscured but not secured) --credentials string Use the format \"<id>,<secret>[,<token>]\". You can get a token by running \"aws sts get-session-token\". --iam-role string Use the given IAM role for authorization, as opposed to using static credentials. The given role will be applied as the annotation iam.amazonaws.com/role, this used with a Kubernetes IAM role management system such as kube2iam allows you to give pachd credentials in a more secure way. --vault string Use the format \"<address/hostport>,<role>,<token>\". Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy amazon"},{"location":"reference/pachctl/pachctl_deploy_amazon/#pachctl-deploy-amazon","text":"Deploy a Pachyderm cluster running on AWS.","title":"pachctl deploy amazon"},{"location":"reference/pachctl/pachctl_deploy_amazon/#synopsis","text":"Deploy a Pachyderm cluster running on AWS. : An S3 bucket where Pachyderm will store PFS data. : The AWS region where Pachyderm is being deployed (e.g. us-west-1) : Size of EBS volumes, in GB (assumed to all be the same). pachctl deploy amazon <bucket-name> <region> <disk-size>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_amazon/#options","text":"--cloudfront-distribution string Deploying on AWS with cloudfront is currently an alpha feature. No security restrictions have beenapplied to cloudfront, making all data public (obscured but not secured) --credentials string Use the format \"<id>,<secret>[,<token>]\". You can get a token by running \"aws sts get-session-token\". --iam-role string Use the given IAM role for authorization, as opposed to using static credentials. The given role will be applied as the annotation iam.amazonaws.com/role, this used with a Kubernetes IAM role management system such as kube2iam allows you to give pachd credentials in a more secure way. --vault string Use the format \"<address/hostport>,<role>,<token>\".","title":"Options"},{"location":"reference/pachctl/pachctl_deploy_amazon/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_custom/","text":"pachctl deploy custom \u00b6 Deploy a custom Pachyderm cluster configuration Synopsis \u00b6 Deploy a custom Pachyderm cluster configuration. If is \\\"s3\\\", then the arguments are: pachctl deploy custom --persistent-disk <persistent disk backend> --object-store <object store backend> <persistent disk args> <object store args> Options \u00b6 --isS3V2 Enable S3V2 client --object-store string (required) Backend providing an object-storage API to pachyderm. One of: s3, gcs, or azure-blob. (default \"s3\") --persistent-disk string (required) Backend providing persistent local volumes to stateful pods. One of: aws, google, or azure. (default \"aws\") -s, --secure Enable secure access to a Minio server. Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy custom"},{"location":"reference/pachctl/pachctl_deploy_custom/#pachctl-deploy-custom","text":"Deploy a custom Pachyderm cluster configuration","title":"pachctl deploy custom"},{"location":"reference/pachctl/pachctl_deploy_custom/#synopsis","text":"Deploy a custom Pachyderm cluster configuration. If is \\\"s3\\\", then the arguments are: pachctl deploy custom --persistent-disk <persistent disk backend> --object-store <object store backend> <persistent disk args> <object store args>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_custom/#options","text":"--isS3V2 Enable S3V2 client --object-store string (required) Backend providing an object-storage API to pachyderm. One of: s3, gcs, or azure-blob. (default \"s3\") --persistent-disk string (required) Backend providing persistent local volumes to stateful pods. One of: aws, google, or azure. (default \"aws\") -s, --secure Enable secure access to a Minio server.","title":"Options"},{"location":"reference/pachctl/pachctl_deploy_custom/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_export-images/","text":"pachctl deploy export-images \u00b6 Export a tarball (to stdout) containing all of the images in a deployment. Synopsis \u00b6 Export a tarball (to stdout) containing all of the images in a deployment. pachctl deploy export-images <output-file> Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy export images"},{"location":"reference/pachctl/pachctl_deploy_export-images/#pachctl-deploy-export-images","text":"Export a tarball (to stdout) containing all of the images in a deployment.","title":"pachctl deploy export-images"},{"location":"reference/pachctl/pachctl_deploy_export-images/#synopsis","text":"Export a tarball (to stdout) containing all of the images in a deployment. pachctl deploy export-images <output-file>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_export-images/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_google/","text":"pachctl deploy google \u00b6 Deploy a Pachyderm cluster running on Google Cloud Platform. Synopsis \u00b6 Deploy a Pachyderm cluster running on Google Cloud Platform. : A Google Cloud Storage bucket where Pachyderm will store PFS data. : Size of Google Compute Engine persistent disks in GB (assumed to all be the same). : A file containing the private key for the account (downloaded from Google Compute Engine). pachctl deploy google <bucket-name> <disk-size> [<credentials-file>] Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy google"},{"location":"reference/pachctl/pachctl_deploy_google/#pachctl-deploy-google","text":"Deploy a Pachyderm cluster running on Google Cloud Platform.","title":"pachctl deploy google"},{"location":"reference/pachctl/pachctl_deploy_google/#synopsis","text":"Deploy a Pachyderm cluster running on Google Cloud Platform. : A Google Cloud Storage bucket where Pachyderm will store PFS data. : Size of Google Compute Engine persistent disks in GB (assumed to all be the same). : A file containing the private key for the account (downloaded from Google Compute Engine). pachctl deploy google <bucket-name> <disk-size> [<credentials-file>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_google/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_import-images/","text":"pachctl deploy import-images \u00b6 Import a tarball (from stdin) containing all of the images in a deployment and push them to a private registry. Synopsis \u00b6 Import a tarball (from stdin) containing all of the images in a deployment and push them to a private registry. pachctl deploy import-images <input-file> Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy import images"},{"location":"reference/pachctl/pachctl_deploy_import-images/#pachctl-deploy-import-images","text":"Import a tarball (from stdin) containing all of the images in a deployment and push them to a private registry.","title":"pachctl deploy import-images"},{"location":"reference/pachctl/pachctl_deploy_import-images/#synopsis","text":"Import a tarball (from stdin) containing all of the images in a deployment and push them to a private registry. pachctl deploy import-images <input-file>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_import-images/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_list-images/","text":"pachctl deploy list-images \u00b6 Output the list of images in a deployment. Synopsis \u00b6 Output the list of images in a deployment. pachctl deploy list-images Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy list images"},{"location":"reference/pachctl/pachctl_deploy_list-images/#pachctl-deploy-list-images","text":"Output the list of images in a deployment.","title":"pachctl deploy list-images"},{"location":"reference/pachctl/pachctl_deploy_list-images/#synopsis","text":"Output the list of images in a deployment. pachctl deploy list-images","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_list-images/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_local/","text":"pachctl deploy local \u00b6 Deploy a single-node Pachyderm cluster with local metadata storage. Synopsis \u00b6 Deploy a single-node Pachyderm cluster with local metadata storage. pachctl deploy local Options \u00b6 -d, --dev Deploy pachd with local version tags, disable metrics, expose Pachyderm's object/block API, and use an insecure authentication mechanism (do not set on any cluster with sensitive data) --host-path string Location on the host machine where PFS metadata will be stored. (default \"/var/pachyderm\") Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy local"},{"location":"reference/pachctl/pachctl_deploy_local/#pachctl-deploy-local","text":"Deploy a single-node Pachyderm cluster with local metadata storage.","title":"pachctl deploy local"},{"location":"reference/pachctl/pachctl_deploy_local/#synopsis","text":"Deploy a single-node Pachyderm cluster with local metadata storage. pachctl deploy local","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_local/#options","text":"-d, --dev Deploy pachd with local version tags, disable metrics, expose Pachyderm's object/block API, and use an insecure authentication mechanism (do not set on any cluster with sensitive data) --host-path string Location on the host machine where PFS metadata will be stored. (default \"/var/pachyderm\")","title":"Options"},{"location":"reference/pachctl/pachctl_deploy_local/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_microsoft/","text":"pachctl deploy microsoft \u00b6 Deploy a Pachyderm cluster running on Microsoft Azure. Synopsis \u00b6 Deploy a Pachyderm cluster running on Microsoft Azure. : An Azure container where Pachyderm will store PFS data. : Size of persistent volumes, in GB (assumed to all be the same). pachctl deploy microsoft <container> <account-name> <account-key> <disk-size> Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy microsoft"},{"location":"reference/pachctl/pachctl_deploy_microsoft/#pachctl-deploy-microsoft","text":"Deploy a Pachyderm cluster running on Microsoft Azure.","title":"pachctl deploy microsoft"},{"location":"reference/pachctl/pachctl_deploy_microsoft/#synopsis","text":"Deploy a Pachyderm cluster running on Microsoft Azure. : An Azure container where Pachyderm will store PFS data. : Size of persistent volumes, in GB (assumed to all be the same). pachctl deploy microsoft <container> <account-name> <account-key> <disk-size>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_microsoft/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_storage/","text":"pachctl deploy storage \u00b6 Deploy credentials for a particular storage provider. Synopsis \u00b6 Deploy credentials for a particular storage provider, so that Pachyderm can ingress data from and egress data to it. Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy storage"},{"location":"reference/pachctl/pachctl_deploy_storage/#pachctl-deploy-storage","text":"Deploy credentials for a particular storage provider.","title":"pachctl deploy storage"},{"location":"reference/pachctl/pachctl_deploy_storage/#synopsis","text":"Deploy credentials for a particular storage provider, so that Pachyderm can ingress data from and egress data to it.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_storage/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_storage_amazon/","text":"pachctl deploy storage amazon \u00b6 Deploy credentials for the Amazon S3 storage provider. Synopsis \u00b6 Deploy credentials for the Amazon S3 storage provider, so that Pachyderm can ingress data from and egress data to it. pachctl deploy storage amazon <region> <access-key-id> <secret-access-key> [<session-token>] Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy storage amazon"},{"location":"reference/pachctl/pachctl_deploy_storage_amazon/#pachctl-deploy-storage-amazon","text":"Deploy credentials for the Amazon S3 storage provider.","title":"pachctl deploy storage amazon"},{"location":"reference/pachctl/pachctl_deploy_storage_amazon/#synopsis","text":"Deploy credentials for the Amazon S3 storage provider, so that Pachyderm can ingress data from and egress data to it. pachctl deploy storage amazon <region> <access-key-id> <secret-access-key> [<session-token>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_storage_amazon/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_storage_google/","text":"pachctl deploy storage google \u00b6 Deploy credentials for the Google Cloud storage provider. Synopsis \u00b6 Deploy credentials for the Google Cloud storage provider, so that Pachyderm can ingress data from and egress data to it. pachctl deploy storage google <credentials-file> Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy storage google"},{"location":"reference/pachctl/pachctl_deploy_storage_google/#pachctl-deploy-storage-google","text":"Deploy credentials for the Google Cloud storage provider.","title":"pachctl deploy storage google"},{"location":"reference/pachctl/pachctl_deploy_storage_google/#synopsis","text":"Deploy credentials for the Google Cloud storage provider, so that Pachyderm can ingress data from and egress data to it. pachctl deploy storage google <credentials-file>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_storage_google/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_deploy_storage_microsoft/","text":"pachctl deploy storage microsoft \u00b6 Deploy credentials for the Azure storage provider. Synopsis \u00b6 Deploy credentials for the Azure storage provider, so that Pachyderm can ingress data from and egress data to it. pachctl deploy storage microsoft <account-name> <account-key> Options inherited from parent commands \u00b6 --block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Pachctl deploy storage microsoft"},{"location":"reference/pachctl/pachctl_deploy_storage_microsoft/#pachctl-deploy-storage-microsoft","text":"Deploy credentials for the Azure storage provider.","title":"pachctl deploy storage microsoft"},{"location":"reference/pachctl/pachctl_deploy_storage_microsoft/#synopsis","text":"Deploy credentials for the Azure storage provider, so that Pachyderm can ingress data from and egress data to it. pachctl deploy storage microsoft <account-name> <account-key>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_deploy_storage_microsoft/#options-inherited-from-parent-commands","text":"--block-cache-size string Size of pachd's in-memory cache for PFS files. Size is specified in bytes, with allowed SI suffixes (M, K, G, Mi, Ki, Gi, etc). -c, --context string Name of the context to add to the pachyderm config. --dash-image string Image URL for pachyderm dashboard --dashboard-only Only deploy the Pachyderm UI (experimental), without the rest of pachyderm. This is for launching the UI adjacent to an existing Pachyderm cluster. After deployment, run \"pachctl port-forward\" to connect --dry-run Don't actually deploy pachyderm to Kubernetes, instead just print the manifest. --dynamic-etcd-nodes int Deploy etcd as a StatefulSet with the given number of pods. The persistent volumes used by these pods are provisioned dynamically. Note that StatefulSet is currently a beta kubernetes feature, which might be unavailable in older versions of kubernetes. --etcd-cpu-request string (rarely set) The size of etcd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --etcd-memory-request string (rarely set) The size of etcd's memory request. Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --etcd-storage-class string If set, the name of an existing StorageClass to use for etcd storage. Ignored if --static-etcd-volume is set. --expose-object-api If set, instruct pachd to serve its object/block API on its public port (not safe with auth enabled, do not set in production). --image-pull-secret string A secret in Kubernetes that's needed to pull from your private registry. --local-roles Use namespace-local roles instead of cluster roles. Ignored if --no-rbac is set. --log-level string The level of log messages to print options are, from least to most verbose: \"error\", \"info\", \"debug\". (default \"info\") --namespace string Kubernetes namespace to deploy Pachyderm to. --new-hash-tree-flag (feature flag) Do not set, used for testing --no-color Turn off colors. --no-dashboard Don't deploy the Pachyderm UI alongside Pachyderm (experimental). --no-expose-docker-socket Don't expose the Docker socket to worker containers. This limits the privileges of workers which prevents them from automatically setting the container's working dir and user. --no-guaranteed Don't use guaranteed QoS for etcd and pachd deployments. Turning this on (turning guaranteed QoS off) can lead to more stable local clusters (such as a on Minikube), it should normally be used for production clusters. --no-rbac Don't deploy RBAC roles for Pachyderm. (for k8s versions prior to 1.8) -o, --output string Output format. One of: json|yaml (default \"json\") --pachd-cpu-request string (rarely set) The size of Pachd's CPU request, which we give to Kubernetes. Size is in cores (with partial cores allowed and encouraged). --pachd-memory-request string (rarely set) The size of PachD's memory request in addition to its block cache (set via --block-cache-size). Size is in bytes, with SI suffixes (M, K, G, Mi, Ki, Gi, etc). --registry string The registry to pull images from. --shards int (rarely set) The maximum number of pachd nodes allowed in the cluster; increasing this number blindly can result in degraded performance. (default 16) --static-etcd-volume string Deploy etcd as a ReplicationController with one pod. The pod uses the given persistent volume. --tls string string of the form \"<cert path>,<key path>\" of the signed TLS certificate and private key that Pachd should use for TLS authentication (enables TLS-encrypted communication with Pachd) -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_diff/","text":"pachctl diff \u00b6 Show the differences between two Pachyderm resources. Synopsis \u00b6 Show the differences between two Pachyderm resources. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl diff"},{"location":"reference/pachctl/pachctl_diff/#pachctl-diff","text":"Show the differences between two Pachyderm resources.","title":"pachctl diff"},{"location":"reference/pachctl/pachctl_diff/#synopsis","text":"Show the differences between two Pachyderm resources.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_diff/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_diff_file/","text":"pachctl diff file \u00b6 Return a diff of two file trees. Synopsis \u00b6 Return a diff of two file trees. pachctl diff file <new-repo>@<new-branch-or-commit>:<new-path> [<old-repo>@<old-branch-or-commit>:<old-path>] Examples \u00b6 # Return the diff of the file \"path\" of the repo \"foo\" between the head of the # \"master\" branch and its parent. $ pachctl diff file foo@master:path # Return the diff between the master branches of repos foo and bar at paths # path1 and path2, respectively. $ pachctl diff file foo@master:path1 bar@master:path2 Options \u00b6 --diff-command string Use a program other than git to diff files. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --name-only Show only the names of changed files. --no-pager Don't pipe output into a pager (i.e. less). -s, --shallow Don't descend into sub directories. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl diff file"},{"location":"reference/pachctl/pachctl_diff_file/#pachctl-diff-file","text":"Return a diff of two file trees.","title":"pachctl diff file"},{"location":"reference/pachctl/pachctl_diff_file/#synopsis","text":"Return a diff of two file trees. pachctl diff file <new-repo>@<new-branch-or-commit>:<new-path> [<old-repo>@<old-branch-or-commit>:<old-path>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_diff_file/#examples","text":"# Return the diff of the file \"path\" of the repo \"foo\" between the head of the # \"master\" branch and its parent. $ pachctl diff file foo@master:path # Return the diff between the master branches of repos foo and bar at paths # path1 and path2, respectively. $ pachctl diff file foo@master:path1 bar@master:path2","title":"Examples"},{"location":"reference/pachctl/pachctl_diff_file/#options","text":"--diff-command string Use a program other than git to diff files. --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --name-only Show only the names of changed files. --no-pager Don't pipe output into a pager (i.e. less). -s, --shallow Don't descend into sub directories.","title":"Options"},{"location":"reference/pachctl/pachctl_diff_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_edit/","text":"pachctl edit \u00b6 Edit the value of an existing Pachyderm resource. Synopsis \u00b6 Edit the value of an existing Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl edit"},{"location":"reference/pachctl/pachctl_edit/#pachctl-edit","text":"Edit the value of an existing Pachyderm resource.","title":"pachctl edit"},{"location":"reference/pachctl/pachctl_edit/#synopsis","text":"Edit the value of an existing Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_edit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_edit_pipeline/","text":"pachctl edit pipeline \u00b6 Edit the manifest for a pipeline in your text editor. Synopsis \u00b6 Edit the manifest for a pipeline in your text editor. pachctl edit pipeline <pipeline> Options \u00b6 --editor string Editor to use for modifying the manifest. --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl edit pipeline"},{"location":"reference/pachctl/pachctl_edit_pipeline/#pachctl-edit-pipeline","text":"Edit the manifest for a pipeline in your text editor.","title":"pachctl edit pipeline"},{"location":"reference/pachctl/pachctl_edit_pipeline/#synopsis","text":"Edit the manifest for a pipeline in your text editor. pachctl edit pipeline <pipeline>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_edit_pipeline/#options","text":"--editor string Editor to use for modifying the manifest. --reprocess If true, reprocess datums that were already processed by previous version of the pipeline.","title":"Options"},{"location":"reference/pachctl/pachctl_edit_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_enterprise/","text":"pachctl enterprise \u00b6 Enterprise commands enable Pachyderm Enterprise features Synopsis \u00b6 Enterprise commands enable Pachyderm Enterprise features Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl enterprise"},{"location":"reference/pachctl/pachctl_enterprise/#pachctl-enterprise","text":"Enterprise commands enable Pachyderm Enterprise features","title":"pachctl enterprise"},{"location":"reference/pachctl/pachctl_enterprise/#synopsis","text":"Enterprise commands enable Pachyderm Enterprise features","title":"Synopsis"},{"location":"reference/pachctl/pachctl_enterprise/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_enterprise_activate/","text":"pachctl enterprise activate \u00b6 Activate the enterprise features of Pachyderm with an activation code Synopsis \u00b6 Activate the enterprise features of Pachyderm with an activation code pachctl enterprise activate <activation-code> Options \u00b6 --expires string A timestamp indicating when the token provided above should expire (formatted as an RFC 3339/ISO 8601 datetime). This is only applied if it's earlier than the signed expiration time encoded in 'activation-code', and therefore is only useful for testing. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl enterprise activate"},{"location":"reference/pachctl/pachctl_enterprise_activate/#pachctl-enterprise-activate","text":"Activate the enterprise features of Pachyderm with an activation code","title":"pachctl enterprise activate"},{"location":"reference/pachctl/pachctl_enterprise_activate/#synopsis","text":"Activate the enterprise features of Pachyderm with an activation code pachctl enterprise activate <activation-code>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_enterprise_activate/#options","text":"--expires string A timestamp indicating when the token provided above should expire (formatted as an RFC 3339/ISO 8601 datetime). This is only applied if it's earlier than the signed expiration time encoded in 'activation-code', and therefore is only useful for testing.","title":"Options"},{"location":"reference/pachctl/pachctl_enterprise_activate/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_enterprise_get-state/","text":"pachctl enterprise get-state \u00b6 Check whether the Pachyderm cluster has enterprise features activated Synopsis \u00b6 Check whether the Pachyderm cluster has enterprise features activated pachctl enterprise get-state Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl enterprise get state"},{"location":"reference/pachctl/pachctl_enterprise_get-state/#pachctl-enterprise-get-state","text":"Check whether the Pachyderm cluster has enterprise features activated","title":"pachctl enterprise get-state"},{"location":"reference/pachctl/pachctl_enterprise_get-state/#synopsis","text":"Check whether the Pachyderm cluster has enterprise features activated pachctl enterprise get-state","title":"Synopsis"},{"location":"reference/pachctl/pachctl_enterprise_get-state/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_extract/","text":"pachctl extract \u00b6 Extract Pachyderm state to stdout or an object store bucket. Synopsis \u00b6 Extract Pachyderm state to stdout or an object store bucket. pachctl extract Examples \u00b6 # Extract into a local file: $ pachctl extract > backup # Extract to s3: $ pachctl extract -u s3://bucket/backup Options \u00b6 --no-objects don't extract from object storage, only extract data from etcd -u, --url string An object storage url (i.e. s3://...) to extract to. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl extract"},{"location":"reference/pachctl/pachctl_extract/#pachctl-extract","text":"Extract Pachyderm state to stdout or an object store bucket.","title":"pachctl extract"},{"location":"reference/pachctl/pachctl_extract/#synopsis","text":"Extract Pachyderm state to stdout or an object store bucket. pachctl extract","title":"Synopsis"},{"location":"reference/pachctl/pachctl_extract/#examples","text":"# Extract into a local file: $ pachctl extract > backup # Extract to s3: $ pachctl extract -u s3://bucket/backup","title":"Examples"},{"location":"reference/pachctl/pachctl_extract/#options","text":"--no-objects don't extract from object storage, only extract data from etcd -u, --url string An object storage url (i.e. s3://...) to extract to.","title":"Options"},{"location":"reference/pachctl/pachctl_extract/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_extract_pipeline/","text":"pachctl extract pipeline \u00b6 Return the manifest used to create a pipeline. Synopsis \u00b6 Return the manifest used to create a pipeline. pachctl extract pipeline <pipeline> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl extract pipeline"},{"location":"reference/pachctl/pachctl_extract_pipeline/#pachctl-extract-pipeline","text":"Return the manifest used to create a pipeline.","title":"pachctl extract pipeline"},{"location":"reference/pachctl/pachctl_extract_pipeline/#synopsis","text":"Return the manifest used to create a pipeline. pachctl extract pipeline <pipeline>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_extract_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_finish/","text":"pachctl finish \u00b6 Finish a Pachyderm resource. Synopsis \u00b6 Finish a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl finish"},{"location":"reference/pachctl/pachctl_finish/#pachctl-finish","text":"Finish a Pachyderm resource.","title":"pachctl finish"},{"location":"reference/pachctl/pachctl_finish/#synopsis","text":"Finish a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_finish/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_finish_commit/","text":"pachctl finish commit \u00b6 Finish a started commit. Synopsis \u00b6 Finish a started commit. Commit-id must be a writeable commit. pachctl finish commit <repo>@<branch-or-commit> Options \u00b6 --description string A description of this commit's contents (synonym for --message) -m, --message string A description of this commit's contents (overwrites any existing commit description) Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl finish commit"},{"location":"reference/pachctl/pachctl_finish_commit/#pachctl-finish-commit","text":"Finish a started commit.","title":"pachctl finish commit"},{"location":"reference/pachctl/pachctl_finish_commit/#synopsis","text":"Finish a started commit. Commit-id must be a writeable commit. pachctl finish commit <repo>@<branch-or-commit>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_finish_commit/#options","text":"--description string A description of this commit's contents (synonym for --message) -m, --message string A description of this commit's contents (overwrites any existing commit description)","title":"Options"},{"location":"reference/pachctl/pachctl_finish_commit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_finish_transaction/","text":"pachctl finish transaction \u00b6 Execute and clear the currently active transaction. Synopsis \u00b6 Execute and clear the currently active transaction. pachctl finish transaction [<transaction>] Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl finish transaction"},{"location":"reference/pachctl/pachctl_finish_transaction/#pachctl-finish-transaction","text":"Execute and clear the currently active transaction.","title":"pachctl finish transaction"},{"location":"reference/pachctl/pachctl_finish_transaction/#synopsis","text":"Execute and clear the currently active transaction. pachctl finish transaction [<transaction>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_finish_transaction/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_flush/","text":"pachctl flush \u00b6 Wait for the side-effects of a Pachyderm resource to propagate. Synopsis \u00b6 Wait for the side-effects of a Pachyderm resource to propagate. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl flush"},{"location":"reference/pachctl/pachctl_flush/#pachctl-flush","text":"Wait for the side-effects of a Pachyderm resource to propagate.","title":"pachctl flush"},{"location":"reference/pachctl/pachctl_flush/#synopsis","text":"Wait for the side-effects of a Pachyderm resource to propagate.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_flush/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_flush_commit/","text":"pachctl flush commit \u00b6 Wait for all commits caused by the specified commits to finish and return them. Synopsis \u00b6 Wait for all commits caused by the specified commits to finish and return them. pachctl flush commit <repo>@<branch-or-commit> ... Examples \u00b6 # return commits caused by foo@XXX and bar@YYY $ pachctl flush commit foo@XXX bar@YYY # return commits caused by foo@XXX leading to repos bar and baz $ pachctl flush commit foo@XXX -r bar -r baz Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json -r, --repos []string Wait only for commits leading to a specific set of repos (default []) Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl flush commit"},{"location":"reference/pachctl/pachctl_flush_commit/#pachctl-flush-commit","text":"Wait for all commits caused by the specified commits to finish and return them.","title":"pachctl flush commit"},{"location":"reference/pachctl/pachctl_flush_commit/#synopsis","text":"Wait for all commits caused by the specified commits to finish and return them. pachctl flush commit <repo>@<branch-or-commit> ...","title":"Synopsis"},{"location":"reference/pachctl/pachctl_flush_commit/#examples","text":"# return commits caused by foo@XXX and bar@YYY $ pachctl flush commit foo@XXX bar@YYY # return commits caused by foo@XXX leading to repos bar and baz $ pachctl flush commit foo@XXX -r bar -r baz","title":"Examples"},{"location":"reference/pachctl/pachctl_flush_commit/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json -r, --repos []string Wait only for commits leading to a specific set of repos (default [])","title":"Options"},{"location":"reference/pachctl/pachctl_flush_commit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_flush_job/","text":"pachctl flush job \u00b6 Wait for all jobs caused by the specified commits to finish and return them. Synopsis \u00b6 Wait for all jobs caused by the specified commits to finish and return them. pachctl flush job <repo>@<branch-or-commit> ... Examples \u00b6 # Return jobs caused by foo@XXX and bar@YYY. $ pachctl flush job foo@XXX bar@YYY # Return jobs caused by foo@XXX leading to pipelines bar and baz. $ pachctl flush job foo@XXX -p bar -p baz Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -p, --pipeline []string Wait only for jobs leading to a specific set of pipelines (default []) --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl flush job"},{"location":"reference/pachctl/pachctl_flush_job/#pachctl-flush-job","text":"Wait for all jobs caused by the specified commits to finish and return them.","title":"pachctl flush job"},{"location":"reference/pachctl/pachctl_flush_job/#synopsis","text":"Wait for all jobs caused by the specified commits to finish and return them. pachctl flush job <repo>@<branch-or-commit> ...","title":"Synopsis"},{"location":"reference/pachctl/pachctl_flush_job/#examples","text":"# Return jobs caused by foo@XXX and bar@YYY. $ pachctl flush job foo@XXX bar@YYY # Return jobs caused by foo@XXX leading to pipelines bar and baz. $ pachctl flush job foo@XXX -p bar -p baz","title":"Examples"},{"location":"reference/pachctl/pachctl_flush_job/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -p, --pipeline []string Wait only for jobs leading to a specific set of pipelines (default []) --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_flush_job/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_fsck/","text":"pachctl fsck \u00b6 Run a file system consistency check on pfs. Synopsis \u00b6 Run a file system consistency check on the pachyderm file system, ensuring the correct provenance relationships are satisfied. pachctl fsck Options \u00b6 -f, --fix Attempt to fix as many issues as possible. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl fsck"},{"location":"reference/pachctl/pachctl_fsck/#pachctl-fsck","text":"Run a file system consistency check on pfs.","title":"pachctl fsck"},{"location":"reference/pachctl/pachctl_fsck/#synopsis","text":"Run a file system consistency check on the pachyderm file system, ensuring the correct provenance relationships are satisfied. pachctl fsck","title":"Synopsis"},{"location":"reference/pachctl/pachctl_fsck/#options","text":"-f, --fix Attempt to fix as many issues as possible.","title":"Options"},{"location":"reference/pachctl/pachctl_fsck/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_garbage-collect/","text":"pachctl garbage-collect \u00b6 Garbage collect unused data. Synopsis \u00b6 Garbage collect unused data. When a file/commit/repo is deleted, the data is not immediately removed from the underlying storage system (e.g. S3) for performance and architectural reasons. This is similar to how when you delete a file on your computer, the file is not necessarily wiped from disk immediately. To actually remove the data, you will need to manually invoke garbage collection with \"pachctl garbage-collect\". Currently \"pachctl garbage-collect\" can only be started when there are no pipelines running. You also need to ensure that there's no ongoing \"put file\". Garbage collection puts the cluster into a readonly mode where no new jobs can be created and no data can be added. Pachyderm's garbage collection uses bloom filters to index live objects. This means that some dead objects may erronously not be deleted during garbage collection. The probability of this happening depends on how many objects you have; at around 10M objects it starts to become likely with the default values. To lower Pachyderm's error rate and make garbage-collection more comprehensive, you can increase the amount of memory used for the bloom filters with the --memory flag. The default value is 10MB. pachctl garbage-collect Options \u00b6 -m, --memory string The amount of memory to use during garbage collection. Default is 10MB. (default \"0\") Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl garbage collect"},{"location":"reference/pachctl/pachctl_garbage-collect/#pachctl-garbage-collect","text":"Garbage collect unused data.","title":"pachctl garbage-collect"},{"location":"reference/pachctl/pachctl_garbage-collect/#synopsis","text":"Garbage collect unused data. When a file/commit/repo is deleted, the data is not immediately removed from the underlying storage system (e.g. S3) for performance and architectural reasons. This is similar to how when you delete a file on your computer, the file is not necessarily wiped from disk immediately. To actually remove the data, you will need to manually invoke garbage collection with \"pachctl garbage-collect\". Currently \"pachctl garbage-collect\" can only be started when there are no pipelines running. You also need to ensure that there's no ongoing \"put file\". Garbage collection puts the cluster into a readonly mode where no new jobs can be created and no data can be added. Pachyderm's garbage collection uses bloom filters to index live objects. This means that some dead objects may erronously not be deleted during garbage collection. The probability of this happening depends on how many objects you have; at around 10M objects it starts to become likely with the default values. To lower Pachyderm's error rate and make garbage-collection more comprehensive, you can increase the amount of memory used for the bloom filters with the --memory flag. The default value is 10MB. pachctl garbage-collect","title":"Synopsis"},{"location":"reference/pachctl/pachctl_garbage-collect/#options","text":"-m, --memory string The amount of memory to use during garbage collection. Default is 10MB. (default \"0\")","title":"Options"},{"location":"reference/pachctl/pachctl_garbage-collect/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_get/","text":"pachctl get \u00b6 Get the raw data represented by a Pachyderm resource. Synopsis \u00b6 Get the raw data represented by a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl get"},{"location":"reference/pachctl/pachctl_get/#pachctl-get","text":"Get the raw data represented by a Pachyderm resource.","title":"pachctl get"},{"location":"reference/pachctl/pachctl_get/#synopsis","text":"Get the raw data represented by a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_get/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_get_file/","text":"pachctl get file \u00b6 Return the contents of a file. Synopsis \u00b6 Return the contents of a file. pachctl get file <repo>@<branch-or-commit>:<path/in/pfs> Examples \u00b6 # get file \"XXX\" on branch \"master\" in repo \"foo\" $ pachctl get file foo@master:XXX # get file \"XXX\" in the parent of the current head of branch \"master\" # in repo \"foo\" $ pachctl get file foo@master^:XXX # get file \"XXX\" in the grandparent of the current head of branch \"master\" # in repo \"foo\" $ pachctl get file foo@master^2:XXX Options \u00b6 -o, --output string The path where data will be downloaded. -p, --parallelism int The maximum number of files that can be downloaded in parallel (default 10) -r, --recursive Recursively download a directory. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl get file"},{"location":"reference/pachctl/pachctl_get_file/#pachctl-get-file","text":"Return the contents of a file.","title":"pachctl get file"},{"location":"reference/pachctl/pachctl_get_file/#synopsis","text":"Return the contents of a file. pachctl get file <repo>@<branch-or-commit>:<path/in/pfs>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_get_file/#examples","text":"# get file \"XXX\" on branch \"master\" in repo \"foo\" $ pachctl get file foo@master:XXX # get file \"XXX\" in the parent of the current head of branch \"master\" # in repo \"foo\" $ pachctl get file foo@master^:XXX # get file \"XXX\" in the grandparent of the current head of branch \"master\" # in repo \"foo\" $ pachctl get file foo@master^2:XXX","title":"Examples"},{"location":"reference/pachctl/pachctl_get_file/#options","text":"-o, --output string The path where data will be downloaded. -p, --parallelism int The maximum number of files that can be downloaded in parallel (default 10) -r, --recursive Recursively download a directory.","title":"Options"},{"location":"reference/pachctl/pachctl_get_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_get_object/","text":"pachctl get object \u00b6 Print the contents of an object. Synopsis \u00b6 Print the contents of an object. pachctl get object <hash> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl get object"},{"location":"reference/pachctl/pachctl_get_object/#pachctl-get-object","text":"Print the contents of an object.","title":"pachctl get object"},{"location":"reference/pachctl/pachctl_get_object/#synopsis","text":"Print the contents of an object. pachctl get object <hash>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_get_object/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_get_tag/","text":"pachctl get tag \u00b6 Print the contents of a tag. Synopsis \u00b6 Print the contents of a tag. pachctl get tag <tag> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl get tag"},{"location":"reference/pachctl/pachctl_get_tag/#pachctl-get-tag","text":"Print the contents of a tag.","title":"pachctl get tag"},{"location":"reference/pachctl/pachctl_get_tag/#synopsis","text":"Print the contents of a tag. pachctl get tag <tag>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_get_tag/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_glob/","text":"pachctl glob \u00b6 Print a list of Pachyderm resources matching a glob pattern. Synopsis \u00b6 Print a list of Pachyderm resources matching a glob pattern. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl glob"},{"location":"reference/pachctl/pachctl_glob/#pachctl-glob","text":"Print a list of Pachyderm resources matching a glob pattern.","title":"pachctl glob"},{"location":"reference/pachctl/pachctl_glob/#synopsis","text":"Print a list of Pachyderm resources matching a glob pattern.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_glob/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_glob_file/","text":"pachctl glob file \u00b6 Return files that match a glob pattern in a commit. Synopsis \u00b6 Return files that match a glob pattern in a commit (that is, match a glob pattern in a repo at the state represented by a commit). Glob patterns are documented here . pachctl glob file <repo>@<branch-or-commit>:<pattern> Examples \u00b6 # Return files in repo \"foo\" on branch \"master\" that start # with the character \"A\". Note how the double quotation marks around the # parameter are necessary because otherwise your shell might interpret the \"*\". $ pachctl glob file \"foo@master:A*\" # Return files in repo \"foo\" on branch \"master\" under directory \"data\". $ pachctl glob file \"foo@master:data/*\" Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl glob file"},{"location":"reference/pachctl/pachctl_glob_file/#pachctl-glob-file","text":"Return files that match a glob pattern in a commit.","title":"pachctl glob file"},{"location":"reference/pachctl/pachctl_glob_file/#synopsis","text":"Return files that match a glob pattern in a commit (that is, match a glob pattern in a repo at the state represented by a commit). Glob patterns are documented here . pachctl glob file <repo>@<branch-or-commit>:<pattern>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_glob_file/#examples","text":"# Return files in repo \"foo\" on branch \"master\" that start # with the character \"A\". Note how the double quotation marks around the # parameter are necessary because otherwise your shell might interpret the \"*\". $ pachctl glob file \"foo@master:A*\" # Return files in repo \"foo\" on branch \"master\" under directory \"data\". $ pachctl glob file \"foo@master:data/*\"","title":"Examples"},{"location":"reference/pachctl/pachctl_glob_file/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_glob_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect/","text":"pachctl inspect \u00b6 Show detailed information about a Pachyderm resource. Synopsis \u00b6 Show detailed information about a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect"},{"location":"reference/pachctl/pachctl_inspect/#pachctl-inspect","text":"Show detailed information about a Pachyderm resource.","title":"pachctl inspect"},{"location":"reference/pachctl/pachctl_inspect/#synopsis","text":"Show detailed information about a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_cluster/","text":"pachctl inspect cluster \u00b6 Returns info about the pachyderm cluster Synopsis \u00b6 Returns info about the pachyderm cluster pachctl inspect cluster Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect cluster"},{"location":"reference/pachctl/pachctl_inspect_cluster/#pachctl-inspect-cluster","text":"Returns info about the pachyderm cluster","title":"pachctl inspect cluster"},{"location":"reference/pachctl/pachctl_inspect_cluster/#synopsis","text":"Returns info about the pachyderm cluster pachctl inspect cluster","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_cluster/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_commit/","text":"pachctl inspect commit \u00b6 Return info about a commit. Synopsis \u00b6 Return info about a commit. pachctl inspect commit <repo>@<branch-or-commit> Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect commit"},{"location":"reference/pachctl/pachctl_inspect_commit/#pachctl-inspect-commit","text":"Return info about a commit.","title":"pachctl inspect commit"},{"location":"reference/pachctl/pachctl_inspect_commit/#synopsis","text":"Return info about a commit. pachctl inspect commit <repo>@<branch-or-commit>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_commit/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_inspect_commit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_datum/","text":"pachctl inspect datum \u00b6 Display detailed info about a single datum. Synopsis \u00b6 Display detailed info about a single datum. Requires the pipeline to have stats enabled. pachctl inspect datum <job> <datum> Options \u00b6 --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect datum"},{"location":"reference/pachctl/pachctl_inspect_datum/#pachctl-inspect-datum","text":"Display detailed info about a single datum.","title":"pachctl inspect datum"},{"location":"reference/pachctl/pachctl_inspect_datum/#synopsis","text":"Display detailed info about a single datum. Requires the pipeline to have stats enabled. pachctl inspect datum <job> <datum>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_datum/#options","text":"--raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_inspect_datum/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_file/","text":"pachctl inspect file \u00b6 Return info about a file. Synopsis \u00b6 Return info about a file. pachctl inspect file <repo>@<branch-or-commit>:<path/in/pfs> Options \u00b6 --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect file"},{"location":"reference/pachctl/pachctl_inspect_file/#pachctl-inspect-file","text":"Return info about a file.","title":"pachctl inspect file"},{"location":"reference/pachctl/pachctl_inspect_file/#synopsis","text":"Return info about a file. pachctl inspect file <repo>@<branch-or-commit>:<path/in/pfs>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_file/#options","text":"--raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_inspect_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_job/","text":"pachctl inspect job \u00b6 Return info about a job. Synopsis \u00b6 Return info about a job. pachctl inspect job <job> Options \u00b6 -b, --block block until the job has either succeeded or failed --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect job"},{"location":"reference/pachctl/pachctl_inspect_job/#pachctl-inspect-job","text":"Return info about a job.","title":"pachctl inspect job"},{"location":"reference/pachctl/pachctl_inspect_job/#synopsis","text":"Return info about a job. pachctl inspect job <job>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_job/#options","text":"-b, --block block until the job has either succeeded or failed --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_inspect_job/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_pipeline/","text":"pachctl inspect pipeline \u00b6 Return info about a pipeline. Synopsis \u00b6 Return info about a pipeline. pachctl inspect pipeline <pipeline> Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect pipeline"},{"location":"reference/pachctl/pachctl_inspect_pipeline/#pachctl-inspect-pipeline","text":"Return info about a pipeline.","title":"pachctl inspect pipeline"},{"location":"reference/pachctl/pachctl_inspect_pipeline/#synopsis","text":"Return info about a pipeline. pachctl inspect pipeline <pipeline>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_pipeline/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_inspect_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_repo/","text":"pachctl inspect repo \u00b6 Return info about a repo. Synopsis \u00b6 Return info about a repo. pachctl inspect repo <repo> Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect repo"},{"location":"reference/pachctl/pachctl_inspect_repo/#pachctl-inspect-repo","text":"Return info about a repo.","title":"pachctl inspect repo"},{"location":"reference/pachctl/pachctl_inspect_repo/#synopsis","text":"Return info about a repo. pachctl inspect repo <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_repo/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_inspect_repo/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_inspect_transaction/","text":"pachctl inspect transaction \u00b6 Print information about an open transaction. Synopsis \u00b6 Print information about an open transaction. pachctl inspect transaction [<transaction>] Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl inspect transaction"},{"location":"reference/pachctl/pachctl_inspect_transaction/#pachctl-inspect-transaction","text":"Print information about an open transaction.","title":"pachctl inspect transaction"},{"location":"reference/pachctl/pachctl_inspect_transaction/#synopsis","text":"Print information about an open transaction. pachctl inspect transaction [<transaction>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_inspect_transaction/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_inspect_transaction/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list/","text":"pachctl list \u00b6 Print a list of Pachyderm resources of a specific type. Synopsis \u00b6 Print a list of Pachyderm resources of a specific type. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list"},{"location":"reference/pachctl/pachctl_list/#pachctl-list","text":"Print a list of Pachyderm resources of a specific type.","title":"pachctl list"},{"location":"reference/pachctl/pachctl_list/#synopsis","text":"Print a list of Pachyderm resources of a specific type.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_branch/","text":"pachctl list branch \u00b6 Return all branches on a repo. Synopsis \u00b6 Return all branches on a repo. pachctl list branch <repo> Options \u00b6 --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list branch"},{"location":"reference/pachctl/pachctl_list_branch/#pachctl-list-branch","text":"Return all branches on a repo.","title":"pachctl list branch"},{"location":"reference/pachctl/pachctl_list_branch/#synopsis","text":"Return all branches on a repo. pachctl list branch <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_branch/#options","text":"--raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_list_branch/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_commit/","text":"pachctl list commit \u00b6 Return all commits on a repo. Synopsis \u00b6 Return all commits on a repo. pachctl list commit <repo>[@<branch>] Examples \u00b6 # return commits in repo \"foo\" $ pachctl list commit foo # return commits in repo \"foo\" on branch \"master\" $ pachctl list commit foo@master # return the last 20 commits in repo \"foo\" on branch \"master\" $ pachctl list commit foo@master -n 20 # return commits in repo \"foo\" since commit XXX $ pachctl list commit foo@master --from XXX Options \u00b6 -f, --from string list all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -n, --number int list only this many commits; if set to zero, list all commits --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list commit"},{"location":"reference/pachctl/pachctl_list_commit/#pachctl-list-commit","text":"Return all commits on a repo.","title":"pachctl list commit"},{"location":"reference/pachctl/pachctl_list_commit/#synopsis","text":"Return all commits on a repo. pachctl list commit <repo>[@<branch>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_commit/#examples","text":"# return commits in repo \"foo\" $ pachctl list commit foo # return commits in repo \"foo\" on branch \"master\" $ pachctl list commit foo@master # return the last 20 commits in repo \"foo\" on branch \"master\" $ pachctl list commit foo@master -n 20 # return commits in repo \"foo\" since commit XXX $ pachctl list commit foo@master --from XXX","title":"Examples"},{"location":"reference/pachctl/pachctl_list_commit/#options","text":"-f, --from string list all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). -n, --number int list only this many commits; if set to zero, list all commits --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_list_commit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_datum/","text":"pachctl list datum \u00b6 Return the datums in a job. Synopsis \u00b6 Return the datums in a job. pachctl list datum <job> Options \u00b6 --page int Specify the page of results to send --pageSize int Specify the number of results sent back in a single page --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list datum"},{"location":"reference/pachctl/pachctl_list_datum/#pachctl-list-datum","text":"Return the datums in a job.","title":"pachctl list datum"},{"location":"reference/pachctl/pachctl_list_datum/#synopsis","text":"Return the datums in a job. pachctl list datum <job>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_datum/#options","text":"--page int Specify the page of results to send --pageSize int Specify the number of results sent back in a single page --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_list_datum/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_file/","text":"pachctl list file \u00b6 Return the files in a directory. Synopsis \u00b6 Return the files in a directory. pachctl list file <repo>@<branch-or-commit>[:<path/in/pfs>] Examples \u00b6 # list top-level files on branch \"master\" in repo \"foo\" $ pachctl list file foo@master # list files under directory \"dir\" on branch \"master\" in repo \"foo\" $ pachctl list file foo@master:dir # list top-level files in the parent commit of the current head of \"master\" # in repo \"foo\" $ pachctl list file foo@master^ # list top-level files in the grandparent of the current head of \"master\" # in repo \"foo\" $ pachctl list file foo@master^2 # list the last n versions of top-level files on branch \"master\" in repo \"foo\" $ pachctl list file foo@master --history n # list all versions of top-level files on branch \"master\" in repo \"foo\" $ pachctl list file foo@master --history all Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --history string Return revision history for files. (default \"none\") --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list file"},{"location":"reference/pachctl/pachctl_list_file/#pachctl-list-file","text":"Return the files in a directory.","title":"pachctl list file"},{"location":"reference/pachctl/pachctl_list_file/#synopsis","text":"Return the files in a directory. pachctl list file <repo>@<branch-or-commit>[:<path/in/pfs>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_file/#examples","text":"# list top-level files on branch \"master\" in repo \"foo\" $ pachctl list file foo@master # list files under directory \"dir\" on branch \"master\" in repo \"foo\" $ pachctl list file foo@master:dir # list top-level files in the parent commit of the current head of \"master\" # in repo \"foo\" $ pachctl list file foo@master^ # list top-level files in the grandparent of the current head of \"master\" # in repo \"foo\" $ pachctl list file foo@master^2 # list the last n versions of top-level files on branch \"master\" in repo \"foo\" $ pachctl list file foo@master --history n # list all versions of top-level files on branch \"master\" in repo \"foo\" $ pachctl list file foo@master --history all","title":"Examples"},{"location":"reference/pachctl/pachctl_list_file/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --history string Return revision history for files. (default \"none\") --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_list_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_job/","text":"pachctl list job \u00b6 Return info about jobs. Synopsis \u00b6 Return info about jobs. pachctl list job Examples \u00b6 # Return all jobs $ pachctl list job # Return all jobs from the most recent version of pipeline \"foo\" $ pachctl list job -p foo # Return all jobs from all versions of pipeline \"foo\" $ pachctl list job -p foo --history all # Return all jobs whose input commits include foo@XXX and bar@YYY $ pachctl list job -i foo@XXX -i bar@YYY # Return all jobs in pipeline foo and whose input commits include bar@YYY $ pachctl list job -p foo -i bar@YYY Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --history string Return jobs from historical versions of pipelines. (default \"none\") -i, --input strings List jobs with a specific set of input commits. format: <repo>@<branch-or-commit> --no-pager Don't pipe output into a pager (i.e. less). -o, --output string List jobs with a specific output commit. format: <repo>@<branch-or-commit> -p, --pipeline string Limit to jobs made by pipeline. --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list job"},{"location":"reference/pachctl/pachctl_list_job/#pachctl-list-job","text":"Return info about jobs.","title":"pachctl list job"},{"location":"reference/pachctl/pachctl_list_job/#synopsis","text":"Return info about jobs. pachctl list job","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_job/#examples","text":"# Return all jobs $ pachctl list job # Return all jobs from the most recent version of pipeline \"foo\" $ pachctl list job -p foo # Return all jobs from all versions of pipeline \"foo\" $ pachctl list job -p foo --history all # Return all jobs whose input commits include foo@XXX and bar@YYY $ pachctl list job -i foo@XXX -i bar@YYY # Return all jobs in pipeline foo and whose input commits include bar@YYY $ pachctl list job -p foo -i bar@YYY","title":"Examples"},{"location":"reference/pachctl/pachctl_list_job/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --history string Return jobs from historical versions of pipelines. (default \"none\") -i, --input strings List jobs with a specific set of input commits. format: <repo>@<branch-or-commit> --no-pager Don't pipe output into a pager (i.e. less). -o, --output string List jobs with a specific output commit. format: <repo>@<branch-or-commit> -p, --pipeline string Limit to jobs made by pipeline. --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_list_job/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_pipeline/","text":"pachctl list pipeline \u00b6 Return info about all pipelines. Synopsis \u00b6 Return info about all pipelines. pachctl list pipeline [<pipeline>] Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --history string Return revision history for pipelines. (default \"none\") --raw disable pretty printing, print raw json -s, --spec Output 'create pipeline' compatibility specs. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list pipeline"},{"location":"reference/pachctl/pachctl_list_pipeline/#pachctl-list-pipeline","text":"Return info about all pipelines.","title":"pachctl list pipeline"},{"location":"reference/pachctl/pachctl_list_pipeline/#synopsis","text":"Return info about all pipelines. pachctl list pipeline [<pipeline>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_pipeline/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --history string Return revision history for pipelines. (default \"none\") --raw disable pretty printing, print raw json -s, --spec Output 'create pipeline' compatibility specs.","title":"Options"},{"location":"reference/pachctl/pachctl_list_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_repo/","text":"pachctl list repo \u00b6 Return all repos. Synopsis \u00b6 Return all repos. pachctl list repo Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list repo"},{"location":"reference/pachctl/pachctl_list_repo/#pachctl-list-repo","text":"Return all repos.","title":"pachctl list repo"},{"location":"reference/pachctl/pachctl_list_repo/#synopsis","text":"Return all repos. pachctl list repo","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_repo/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_list_repo/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_list_transaction/","text":"pachctl list transaction \u00b6 List transactions. Synopsis \u00b6 List transactions. pachctl list transaction Options \u00b6 --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl list transaction"},{"location":"reference/pachctl/pachctl_list_transaction/#pachctl-list-transaction","text":"List transactions.","title":"pachctl list transaction"},{"location":"reference/pachctl/pachctl_list_transaction/#synopsis","text":"List transactions. pachctl list transaction","title":"Synopsis"},{"location":"reference/pachctl/pachctl_list_transaction/#options","text":"--full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_list_transaction/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_logs/","text":"pachctl logs \u00b6 Return logs from a job. Synopsis \u00b6 Return logs from a job. pachctl logs [--pipeline=<pipeline>|--job=<job>] [--datum=<datum>] Examples \u00b6 # Return logs emitted by recent jobs in the \"filter\" pipeline $ pachctl logs --pipeline=filter # Return logs emitted by the job aedfa12aedf $ pachctl logs --job=aedfa12aedf # Return logs emitted by the pipeline \\\"filter\\\" while processing /apple.txt and a file with the hash 123aef $ pachctl logs --pipeline=filter --inputs=/apple.txt,123aef Options \u00b6 --datum string Filter for log lines for this datum (accepts datum ID) -f, --follow Follow logs as more are created. --inputs string Filter for log lines generated while processing these files (accepts PFS paths or file hashes) --job string Filter for log lines from this job (accepts job ID) --master Return log messages from the master process (pipeline must be set). -p, --pipeline string Filter the log for lines from this pipeline (accepts pipeline name) --raw Return log messages verbatim from server. -t, --tail int Lines of recent logs to display. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl logs"},{"location":"reference/pachctl/pachctl_logs/#pachctl-logs","text":"Return logs from a job.","title":"pachctl logs"},{"location":"reference/pachctl/pachctl_logs/#synopsis","text":"Return logs from a job. pachctl logs [--pipeline=<pipeline>|--job=<job>] [--datum=<datum>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_logs/#examples","text":"# Return logs emitted by recent jobs in the \"filter\" pipeline $ pachctl logs --pipeline=filter # Return logs emitted by the job aedfa12aedf $ pachctl logs --job=aedfa12aedf # Return logs emitted by the pipeline \\\"filter\\\" while processing /apple.txt and a file with the hash 123aef $ pachctl logs --pipeline=filter --inputs=/apple.txt,123aef","title":"Examples"},{"location":"reference/pachctl/pachctl_logs/#options","text":"--datum string Filter for log lines for this datum (accepts datum ID) -f, --follow Follow logs as more are created. --inputs string Filter for log lines generated while processing these files (accepts PFS paths or file hashes) --job string Filter for log lines from this job (accepts job ID) --master Return log messages from the master process (pipeline must be set). -p, --pipeline string Filter the log for lines from this pipeline (accepts pipeline name) --raw Return log messages verbatim from server. -t, --tail int Lines of recent logs to display.","title":"Options"},{"location":"reference/pachctl/pachctl_logs/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_mount/","text":"pachctl mount \u00b6 Mount pfs locally. This command blocks. Synopsis \u00b6 Mount pfs locally. This command blocks. pachctl mount <path/to/mount/point> Options \u00b6 -c, --commits []string Commits to mount for repos, arguments should be of the form \"repo@commit\" (default []) -d, --debug Turn on debug messages. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl mount"},{"location":"reference/pachctl/pachctl_mount/#pachctl-mount","text":"Mount pfs locally. This command blocks.","title":"pachctl mount"},{"location":"reference/pachctl/pachctl_mount/#synopsis","text":"Mount pfs locally. This command blocks. pachctl mount <path/to/mount/point>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_mount/#options","text":"-c, --commits []string Commits to mount for repos, arguments should be of the form \"repo@commit\" (default []) -d, --debug Turn on debug messages.","title":"Options"},{"location":"reference/pachctl/pachctl_mount/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_port-forward/","text":"pachctl port-forward \u00b6 Forward a port on the local machine to pachd. This command blocks. Synopsis \u00b6 Forward a port on the local machine to pachd. This command blocks. pachctl port-forward Options \u00b6 -f, --pfs-port uint16 The local port to bind PFS over HTTP to. (default 30652) -p, --port uint16 The local port to bind pachd to. (default 30650) -x, --proxy-port uint16 The local port to bind Pachyderm's dash proxy service to. (default 30081) --remote-port uint16 The remote port that pachd is bound to in the cluster. (default 650) -s, --s3gateway-port uint16 The local port to bind the s3gateway to. (default 30600) --saml-port uint16 The local port to bind pachd's SAML ACS to. (default 30654) -u, --ui-port uint16 The local port to bind Pachyderm's dash service to. (default 30080) Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl port forward"},{"location":"reference/pachctl/pachctl_port-forward/#pachctl-port-forward","text":"Forward a port on the local machine to pachd. This command blocks.","title":"pachctl port-forward"},{"location":"reference/pachctl/pachctl_port-forward/#synopsis","text":"Forward a port on the local machine to pachd. This command blocks. pachctl port-forward","title":"Synopsis"},{"location":"reference/pachctl/pachctl_port-forward/#options","text":"-f, --pfs-port uint16 The local port to bind PFS over HTTP to. (default 30652) -p, --port uint16 The local port to bind pachd to. (default 30650) -x, --proxy-port uint16 The local port to bind Pachyderm's dash proxy service to. (default 30081) --remote-port uint16 The remote port that pachd is bound to in the cluster. (default 650) -s, --s3gateway-port uint16 The local port to bind the s3gateway to. (default 30600) --saml-port uint16 The local port to bind pachd's SAML ACS to. (default 30654) -u, --ui-port uint16 The local port to bind Pachyderm's dash service to. (default 30080)","title":"Options"},{"location":"reference/pachctl/pachctl_port-forward/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_put/","text":"pachctl put \u00b6 Insert data into Pachyderm. Synopsis \u00b6 Insert data into Pachyderm. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl put"},{"location":"reference/pachctl/pachctl_put/#pachctl-put","text":"Insert data into Pachyderm.","title":"pachctl put"},{"location":"reference/pachctl/pachctl_put/#synopsis","text":"Insert data into Pachyderm.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_put/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_put_file/","text":"pachctl put file \u00b6 Put a file into the filesystem. Synopsis \u00b6 Put a file into the filesystem. This supports a number of ways to insert data into pfs. pachctl put file <repo>@<branch-or-commit>[:<path/in/pfs>] Examples \u00b6 # Put data from stdin as repo/branch/path: $ echo \"data\" | pachctl put file repo@branch:/path # Put data from stdin as repo/branch/path and start / finish a new commit on the branch. $ echo \"data\" | pachctl put file -c repo@branch:/path # Put a file from the local filesystem as repo/branch/path: $ pachctl put file repo@branch:/path -f file # Put a file from the local filesystem as repo/branch/file: $ pachctl put file repo@branch -f file # Put the contents of a directory as repo/branch/path/dir/file: $ pachctl put file -r repo@branch:/path -f dir # Put the contents of a directory as repo/branch/dir/file: $ pachctl put file -r repo@branch -f dir # Put the contents of a directory as repo/branch/file, i.e. put files at the top level: $ pachctl put file -r repo@branch:/ -f dir # Put the data from a URL as repo/branch/path: $ pachctl put file repo@branch:/path -f http://host/path # Put the data from a URL as repo/branch/path: $ pachctl put file repo@branch -f http://host/path # Put the data from an S3 bucket as repo/branch/s3_object: $ pachctl put file repo@branch -r -f s3://my_bucket # Put several files or URLs that are listed in file. # Files and URLs should be newline delimited. $ pachctl put file repo@branch -i file # Put several files or URLs that are listed at URL. # NOTE this URL can reference local files, so it could cause you to put sensitive # files into your Pachyderm cluster. $ pachctl put file repo@branch -i http://host/path Options \u00b6 -c, --commit DEPRECATED: Put file(s) in a new commit. -f, --file strings The file to be put, it can be a local file or a URL. (default [-]) --header-records uint the number of records that will be converted to a PFS 'header', and prepended to future retrievals of any subset of data from PFS; needs to be used with --split=(json|line|csv) -i, --input-file string Read filepaths or URLs from a file. If - is used, paths are read from the standard input. -o, --overwrite Overwrite the existing content of the file, either from previous commits or previous calls to 'put file' within this commit. -p, --parallelism int The maximum number of files that can be uploaded in parallel. (default 10) -r, --recursive Recursively put the files in a directory. --split line Split the input file into smaller files, subject to the constraints of --target-file-datums and --target-file-bytes. Permissible values are line, `json`, `sql` and `csv`. --target-file-bytes uint The target upper bound of the number of bytes that each file contains; needs to be used with --split. --target-file-datums uint The upper bound of the number of datums that each file contains, the last file will contain fewer if the datums don't divide evenly; needs to be used with --split. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl put file"},{"location":"reference/pachctl/pachctl_put_file/#pachctl-put-file","text":"Put a file into the filesystem.","title":"pachctl put file"},{"location":"reference/pachctl/pachctl_put_file/#synopsis","text":"Put a file into the filesystem. This supports a number of ways to insert data into pfs. pachctl put file <repo>@<branch-or-commit>[:<path/in/pfs>]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_put_file/#examples","text":"# Put data from stdin as repo/branch/path: $ echo \"data\" | pachctl put file repo@branch:/path # Put data from stdin as repo/branch/path and start / finish a new commit on the branch. $ echo \"data\" | pachctl put file -c repo@branch:/path # Put a file from the local filesystem as repo/branch/path: $ pachctl put file repo@branch:/path -f file # Put a file from the local filesystem as repo/branch/file: $ pachctl put file repo@branch -f file # Put the contents of a directory as repo/branch/path/dir/file: $ pachctl put file -r repo@branch:/path -f dir # Put the contents of a directory as repo/branch/dir/file: $ pachctl put file -r repo@branch -f dir # Put the contents of a directory as repo/branch/file, i.e. put files at the top level: $ pachctl put file -r repo@branch:/ -f dir # Put the data from a URL as repo/branch/path: $ pachctl put file repo@branch:/path -f http://host/path # Put the data from a URL as repo/branch/path: $ pachctl put file repo@branch -f http://host/path # Put the data from an S3 bucket as repo/branch/s3_object: $ pachctl put file repo@branch -r -f s3://my_bucket # Put several files or URLs that are listed in file. # Files and URLs should be newline delimited. $ pachctl put file repo@branch -i file # Put several files or URLs that are listed at URL. # NOTE this URL can reference local files, so it could cause you to put sensitive # files into your Pachyderm cluster. $ pachctl put file repo@branch -i http://host/path","title":"Examples"},{"location":"reference/pachctl/pachctl_put_file/#options","text":"-c, --commit DEPRECATED: Put file(s) in a new commit. -f, --file strings The file to be put, it can be a local file or a URL. (default [-]) --header-records uint the number of records that will be converted to a PFS 'header', and prepended to future retrievals of any subset of data from PFS; needs to be used with --split=(json|line|csv) -i, --input-file string Read filepaths or URLs from a file. If - is used, paths are read from the standard input. -o, --overwrite Overwrite the existing content of the file, either from previous commits or previous calls to 'put file' within this commit. -p, --parallelism int The maximum number of files that can be uploaded in parallel. (default 10) -r, --recursive Recursively put the files in a directory. --split line Split the input file into smaller files, subject to the constraints of --target-file-datums and --target-file-bytes. Permissible values are line, `json`, `sql` and `csv`. --target-file-bytes uint The target upper bound of the number of bytes that each file contains; needs to be used with --split. --target-file-datums uint The upper bound of the number of datums that each file contains, the last file will contain fewer if the datums don't divide evenly; needs to be used with --split.","title":"Options"},{"location":"reference/pachctl/pachctl_put_file/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_restart/","text":"pachctl restart \u00b6 Cancel and restart an ongoing task. Synopsis \u00b6 Cancel and restart an ongoing task. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl restart"},{"location":"reference/pachctl/pachctl_restart/#pachctl-restart","text":"Cancel and restart an ongoing task.","title":"pachctl restart"},{"location":"reference/pachctl/pachctl_restart/#synopsis","text":"Cancel and restart an ongoing task.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_restart/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_restart_datum/","text":"pachctl restart datum \u00b6 Restart a datum. Synopsis \u00b6 Restart a datum. pachctl restart datum <job> <datum-path1>,<datum-path2>,... Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl restart datum"},{"location":"reference/pachctl/pachctl_restart_datum/#pachctl-restart-datum","text":"Restart a datum.","title":"pachctl restart datum"},{"location":"reference/pachctl/pachctl_restart_datum/#synopsis","text":"Restart a datum. pachctl restart datum <job> <datum-path1>,<datum-path2>,...","title":"Synopsis"},{"location":"reference/pachctl/pachctl_restart_datum/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_restore/","text":"pachctl restore \u00b6 Restore Pachyderm state from stdin or an object store. Synopsis \u00b6 Restore Pachyderm state from stdin or an object store. pachctl restore Examples \u00b6 # Restore from a local file: $ pachctl restore < backup # Restore from s3: $ pachctl restore -u s3://bucket/backup Options \u00b6 -u, --url string An object storage url (i.e. s3://...) to restore from. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl restore"},{"location":"reference/pachctl/pachctl_restore/#pachctl-restore","text":"Restore Pachyderm state from stdin or an object store.","title":"pachctl restore"},{"location":"reference/pachctl/pachctl_restore/#synopsis","text":"Restore Pachyderm state from stdin or an object store. pachctl restore","title":"Synopsis"},{"location":"reference/pachctl/pachctl_restore/#examples","text":"# Restore from a local file: $ pachctl restore < backup # Restore from s3: $ pachctl restore -u s3://bucket/backup","title":"Examples"},{"location":"reference/pachctl/pachctl_restore/#options","text":"-u, --url string An object storage url (i.e. s3://...) to restore from.","title":"Options"},{"location":"reference/pachctl/pachctl_restore/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_resume/","text":"pachctl resume \u00b6 Resume a stopped task. Synopsis \u00b6 Resume a stopped task. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl resume"},{"location":"reference/pachctl/pachctl_resume/#pachctl-resume","text":"Resume a stopped task.","title":"pachctl resume"},{"location":"reference/pachctl/pachctl_resume/#synopsis","text":"Resume a stopped task.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_resume/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_resume_transaction/","text":"pachctl resume transaction \u00b6 Set an existing transaction as active. Synopsis \u00b6 Set an existing transaction as active. pachctl resume transaction <transaction> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl resume transaction"},{"location":"reference/pachctl/pachctl_resume_transaction/#pachctl-resume-transaction","text":"Set an existing transaction as active.","title":"pachctl resume transaction"},{"location":"reference/pachctl/pachctl_resume_transaction/#synopsis","text":"Set an existing transaction as active. pachctl resume transaction <transaction>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_resume_transaction/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_run/","text":"pachctl run \u00b6 Manually run a Pachyderm resource. Synopsis \u00b6 Manually run a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl run"},{"location":"reference/pachctl/pachctl_run/#pachctl-run","text":"Manually run a Pachyderm resource.","title":"pachctl run"},{"location":"reference/pachctl/pachctl_run/#synopsis","text":"Manually run a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_run/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_run_pipeline/","text":"pachctl run pipeline \u00b6 Run an existing Pachyderm pipeline on the specified commits or branches. Synopsis \u00b6 Run a Pachyderm pipeline on the datums from specific commits. Note: pipelines run automatically when data is committed to them. This command is for the case where you want to run the pipeline on a specific set of data, or if you want to rerun the pipeline. pachctl run pipeline <pipeline> [commits...] Examples \u00b6 # Rerun the latest job for the \"filter\" pipeline $ pachctl run pipeline filter # Reprocess the pipeline \"filter\" on the data from commits a23e4 and bf363 $ pachctl run pipeline filter a23e4 and bf363 # Run the pipeline \"filter\" on the data from the \"staging\" branch $ pachctl run pipeline filter staging Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl run pipeline"},{"location":"reference/pachctl/pachctl_run_pipeline/#pachctl-run-pipeline","text":"Run an existing Pachyderm pipeline on the specified commits or branches.","title":"pachctl run pipeline"},{"location":"reference/pachctl/pachctl_run_pipeline/#synopsis","text":"Run a Pachyderm pipeline on the datums from specific commits. Note: pipelines run automatically when data is committed to them. This command is for the case where you want to run the pipeline on a specific set of data, or if you want to rerun the pipeline. pachctl run pipeline <pipeline> [commits...]","title":"Synopsis"},{"location":"reference/pachctl/pachctl_run_pipeline/#examples","text":"# Rerun the latest job for the \"filter\" pipeline $ pachctl run pipeline filter # Reprocess the pipeline \"filter\" on the data from commits a23e4 and bf363 $ pachctl run pipeline filter a23e4 and bf363 # Run the pipeline \"filter\" on the data from the \"staging\" branch $ pachctl run pipeline filter staging","title":"Examples"},{"location":"reference/pachctl/pachctl_run_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_start/","text":"pachctl start \u00b6 Start a Pachyderm resource. Synopsis \u00b6 Start a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl start"},{"location":"reference/pachctl/pachctl_start/#pachctl-start","text":"Start a Pachyderm resource.","title":"pachctl start"},{"location":"reference/pachctl/pachctl_start/#synopsis","text":"Start a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_start/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_start_commit/","text":"pachctl start commit \u00b6 Start a new commit. Synopsis \u00b6 Start a new commit with parent-commit as the parent, or start a commit on the given branch; if the branch does not exist, it will be created. pachctl start commit <repo>@<branch-or-commit> Examples \u00b6 # Start a new commit in repo \"test\" that's not on any branch $ pachctl start commit test # Start a commit in repo \"test\" on branch \"master\" $ pachctl start commit test@master # Start a commit with \"master\" as the parent in repo \"test\", on a new branch \"patch\"; essentially a fork. $ pachctl start commit test@patch -p master # Start a commit with XXX as the parent in repo \"test\", not on any branch $ pachctl start commit test -p XXX Options \u00b6 --description string A description of this commit's contents (synonym for --message) -m, --message string A description of this commit's contents -p, --parent string The parent of the new commit, unneeded if branch is specified and you want to use the previous head of the branch as the parent. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl start commit"},{"location":"reference/pachctl/pachctl_start_commit/#pachctl-start-commit","text":"Start a new commit.","title":"pachctl start commit"},{"location":"reference/pachctl/pachctl_start_commit/#synopsis","text":"Start a new commit with parent-commit as the parent, or start a commit on the given branch; if the branch does not exist, it will be created. pachctl start commit <repo>@<branch-or-commit>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_start_commit/#examples","text":"# Start a new commit in repo \"test\" that's not on any branch $ pachctl start commit test # Start a commit in repo \"test\" on branch \"master\" $ pachctl start commit test@master # Start a commit with \"master\" as the parent in repo \"test\", on a new branch \"patch\"; essentially a fork. $ pachctl start commit test@patch -p master # Start a commit with XXX as the parent in repo \"test\", not on any branch $ pachctl start commit test -p XXX","title":"Examples"},{"location":"reference/pachctl/pachctl_start_commit/#options","text":"--description string A description of this commit's contents (synonym for --message) -m, --message string A description of this commit's contents -p, --parent string The parent of the new commit, unneeded if branch is specified and you want to use the previous head of the branch as the parent.","title":"Options"},{"location":"reference/pachctl/pachctl_start_commit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_start_pipeline/","text":"pachctl start pipeline \u00b6 Restart a stopped pipeline. Synopsis \u00b6 Restart a stopped pipeline. pachctl start pipeline <pipeline> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl start pipeline"},{"location":"reference/pachctl/pachctl_start_pipeline/#pachctl-start-pipeline","text":"Restart a stopped pipeline.","title":"pachctl start pipeline"},{"location":"reference/pachctl/pachctl_start_pipeline/#synopsis","text":"Restart a stopped pipeline. pachctl start pipeline <pipeline>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_start_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_start_transaction/","text":"pachctl start transaction \u00b6 Start a new transaction. Synopsis \u00b6 Start a new transaction. pachctl start transaction Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl start transaction"},{"location":"reference/pachctl/pachctl_start_transaction/#pachctl-start-transaction","text":"Start a new transaction.","title":"pachctl start transaction"},{"location":"reference/pachctl/pachctl_start_transaction/#synopsis","text":"Start a new transaction. pachctl start transaction","title":"Synopsis"},{"location":"reference/pachctl/pachctl_start_transaction/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_stop/","text":"pachctl stop \u00b6 Cancel an ongoing task. Synopsis \u00b6 Cancel an ongoing task. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl stop"},{"location":"reference/pachctl/pachctl_stop/#pachctl-stop","text":"Cancel an ongoing task.","title":"pachctl stop"},{"location":"reference/pachctl/pachctl_stop/#synopsis","text":"Cancel an ongoing task.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_stop/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_stop_job/","text":"pachctl stop job \u00b6 Stop a job. Synopsis \u00b6 Stop a job. The job will be stopped immediately. pachctl stop job <job> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl stop job"},{"location":"reference/pachctl/pachctl_stop_job/#pachctl-stop-job","text":"Stop a job.","title":"pachctl stop job"},{"location":"reference/pachctl/pachctl_stop_job/#synopsis","text":"Stop a job. The job will be stopped immediately. pachctl stop job <job>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_stop_job/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_stop_pipeline/","text":"pachctl stop pipeline \u00b6 Stop a running pipeline. Synopsis \u00b6 Stop a running pipeline. pachctl stop pipeline <pipeline> Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl stop pipeline"},{"location":"reference/pachctl/pachctl_stop_pipeline/#pachctl-stop-pipeline","text":"Stop a running pipeline.","title":"pachctl stop pipeline"},{"location":"reference/pachctl/pachctl_stop_pipeline/#synopsis","text":"Stop a running pipeline. pachctl stop pipeline <pipeline>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_stop_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_stop_transaction/","text":"pachctl stop transaction \u00b6 Stop modifying the current transaction. Synopsis \u00b6 Stop modifying the current transaction. pachctl stop transaction Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl stop transaction"},{"location":"reference/pachctl/pachctl_stop_transaction/#pachctl-stop-transaction","text":"Stop modifying the current transaction.","title":"pachctl stop transaction"},{"location":"reference/pachctl/pachctl_stop_transaction/#synopsis","text":"Stop modifying the current transaction. pachctl stop transaction","title":"Synopsis"},{"location":"reference/pachctl/pachctl_stop_transaction/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_subscribe/","text":"pachctl subscribe \u00b6 Wait for notifications of changes to a Pachyderm resource. Synopsis \u00b6 Wait for notifications of changes to a Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl subscribe"},{"location":"reference/pachctl/pachctl_subscribe/#pachctl-subscribe","text":"Wait for notifications of changes to a Pachyderm resource.","title":"pachctl subscribe"},{"location":"reference/pachctl/pachctl_subscribe/#synopsis","text":"Wait for notifications of changes to a Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_subscribe/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_subscribe_commit/","text":"pachctl subscribe commit \u00b6 Print commits as they are created (finished). Synopsis \u00b6 Print commits as they are created in the specified repo and branch. By default, all existing commits on the specified branch are returned first. A commit is only considered 'created' when it's been finished. pachctl subscribe commit <repo>@<branch> Examples \u00b6 # subscribe to commits in repo \"test\" on branch \"master\" $ pachctl subscribe commit test@master # subscribe to commits in repo \"test\" on branch \"master\", but only since commit XXX. $ pachctl subscribe commit test@master --from XXX # subscribe to commits in repo \"test\" on branch \"master\", but only for new commits created from now on. $ pachctl subscribe commit test@master --new Options \u00b6 --from string subscribe to all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --new subscribe to only new commits created from now on --raw disable pretty printing, print raw json Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl subscribe commit"},{"location":"reference/pachctl/pachctl_subscribe_commit/#pachctl-subscribe-commit","text":"Print commits as they are created (finished).","title":"pachctl subscribe commit"},{"location":"reference/pachctl/pachctl_subscribe_commit/#synopsis","text":"Print commits as they are created in the specified repo and branch. By default, all existing commits on the specified branch are returned first. A commit is only considered 'created' when it's been finished. pachctl subscribe commit <repo>@<branch>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_subscribe_commit/#examples","text":"# subscribe to commits in repo \"test\" on branch \"master\" $ pachctl subscribe commit test@master # subscribe to commits in repo \"test\" on branch \"master\", but only since commit XXX. $ pachctl subscribe commit test@master --from XXX # subscribe to commits in repo \"test\" on branch \"master\", but only for new commits created from now on. $ pachctl subscribe commit test@master --new","title":"Examples"},{"location":"reference/pachctl/pachctl_subscribe_commit/#options","text":"--from string subscribe to all commits since this commit --full-timestamps Return absolute timestamps (as opposed to the default, relative timestamps). --new subscribe to only new commits created from now on --raw disable pretty printing, print raw json","title":"Options"},{"location":"reference/pachctl/pachctl_subscribe_commit/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_undeploy/","text":"pachctl undeploy \u00b6 Tear down a deployed Pachyderm cluster. Synopsis \u00b6 Tear down a deployed Pachyderm cluster. pachctl undeploy Options \u00b6 -a, --all Delete everything, including the persistent volumes where metadata is stored. If your persistent volumes were dynamically provisioned (i.e. if you used the \"--dynamic-etcd-nodes\" flag), the underlying volumes will be removed, making metadata such repos, commits, pipelines, and jobs unrecoverable. If your persistent volume was manually provisioned (i.e. if you used the \"--static-etcd-volume\" flag), the underlying volume will not be removed. --namespace string Kubernetes namespace to undeploy Pachyderm from. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl undeploy"},{"location":"reference/pachctl/pachctl_undeploy/#pachctl-undeploy","text":"Tear down a deployed Pachyderm cluster.","title":"pachctl undeploy"},{"location":"reference/pachctl/pachctl_undeploy/#synopsis","text":"Tear down a deployed Pachyderm cluster. pachctl undeploy","title":"Synopsis"},{"location":"reference/pachctl/pachctl_undeploy/#options","text":"-a, --all Delete everything, including the persistent volumes where metadata is stored. If your persistent volumes were dynamically provisioned (i.e. if you used the \"--dynamic-etcd-nodes\" flag), the underlying volumes will be removed, making metadata such repos, commits, pipelines, and jobs unrecoverable. If your persistent volume was manually provisioned (i.e. if you used the \"--static-etcd-volume\" flag), the underlying volume will not be removed. --namespace string Kubernetes namespace to undeploy Pachyderm from.","title":"Options"},{"location":"reference/pachctl/pachctl_undeploy/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_unmount/","text":"pachctl unmount \u00b6 Unmount pfs. Synopsis \u00b6 Unmount pfs. pachctl unmount <path/to/mount/point> Options \u00b6 -a, --all unmount all pfs mounts Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl unmount"},{"location":"reference/pachctl/pachctl_unmount/#pachctl-unmount","text":"Unmount pfs.","title":"pachctl unmount"},{"location":"reference/pachctl/pachctl_unmount/#synopsis","text":"Unmount pfs. pachctl unmount <path/to/mount/point>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_unmount/#options","text":"-a, --all unmount all pfs mounts","title":"Options"},{"location":"reference/pachctl/pachctl_unmount/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_update-dash/","text":"pachctl update-dash \u00b6 Update and redeploy the Pachyderm Dashboard at the latest compatible version. Synopsis \u00b6 Update and redeploy the Pachyderm Dashboard at the latest compatible version. pachctl update-dash Options \u00b6 --dry-run Don't actually deploy Pachyderm Dash to Kubernetes, instead just print the manifest. -o, --output string Output format. One of: json|yaml (default \"json\") Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl update dash"},{"location":"reference/pachctl/pachctl_update-dash/#pachctl-update-dash","text":"Update and redeploy the Pachyderm Dashboard at the latest compatible version.","title":"pachctl update-dash"},{"location":"reference/pachctl/pachctl_update-dash/#synopsis","text":"Update and redeploy the Pachyderm Dashboard at the latest compatible version. pachctl update-dash","title":"Synopsis"},{"location":"reference/pachctl/pachctl_update-dash/#options","text":"--dry-run Don't actually deploy Pachyderm Dash to Kubernetes, instead just print the manifest. -o, --output string Output format. One of: json|yaml (default \"json\")","title":"Options"},{"location":"reference/pachctl/pachctl_update-dash/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_update/","text":"pachctl update \u00b6 Change the properties of an existing Pachyderm resource. Synopsis \u00b6 Change the properties of an existing Pachyderm resource. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl update"},{"location":"reference/pachctl/pachctl_update/#pachctl-update","text":"Change the properties of an existing Pachyderm resource.","title":"pachctl update"},{"location":"reference/pachctl/pachctl_update/#synopsis","text":"Change the properties of an existing Pachyderm resource.","title":"Synopsis"},{"location":"reference/pachctl/pachctl_update/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_update_pipeline/","text":"pachctl update pipeline \u00b6 Update an existing Pachyderm pipeline. Synopsis \u00b6 Update a Pachyderm pipeline with a new pipeline specification. For details on the format, see http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html . pachctl update pipeline Options \u00b6 -b, --build If true, build and push local docker images into the docker registry. -f, --file string The JSON file containing the pipeline, it can be a url or local file. - reads from stdin. (default \"-\") -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default \"docker.io\") --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. -u, --username string The username to push images as, defaults to your OS username. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl update pipeline"},{"location":"reference/pachctl/pachctl_update_pipeline/#pachctl-update-pipeline","text":"Update an existing Pachyderm pipeline.","title":"pachctl update pipeline"},{"location":"reference/pachctl/pachctl_update_pipeline/#synopsis","text":"Update a Pachyderm pipeline with a new pipeline specification. For details on the format, see http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html . pachctl update pipeline","title":"Synopsis"},{"location":"reference/pachctl/pachctl_update_pipeline/#options","text":"-b, --build If true, build and push local docker images into the docker registry. -f, --file string The JSON file containing the pipeline, it can be a url or local file. - reads from stdin. (default \"-\") -p, --push-images If true, push local docker images into the docker registry. -r, --registry string The registry to push images to. (default \"docker.io\") --reprocess If true, reprocess datums that were already processed by previous version of the pipeline. -u, --username string The username to push images as, defaults to your OS username.","title":"Options"},{"location":"reference/pachctl/pachctl_update_pipeline/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_update_repo/","text":"pachctl update repo \u00b6 Update a repo. Synopsis \u00b6 Update a repo. pachctl update repo <repo> Options \u00b6 -d, --description string A description of the repo. Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl update repo"},{"location":"reference/pachctl/pachctl_update_repo/#pachctl-update-repo","text":"Update a repo.","title":"pachctl update repo"},{"location":"reference/pachctl/pachctl_update_repo/#synopsis","text":"Update a repo. pachctl update repo <repo>","title":"Synopsis"},{"location":"reference/pachctl/pachctl_update_repo/#options","text":"-d, --description string A description of the repo.","title":"Options"},{"location":"reference/pachctl/pachctl_update_repo/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"reference/pachctl/pachctl_version/","text":"pachctl version \u00b6 Print Pachyderm version information. Synopsis \u00b6 Print Pachyderm version information. pachctl version Options \u00b6 --client-only If set, only print pachctl's version, but don't make any RPCs to pachd. Useful if pachd is unavailable --raw disable pretty printing, print raw json --timeout string If set, 'pachctl version' will timeout after the given duration (formatted as a golang time duration--a number followed by ns, us, ms, s, m, or h). If --client-only is set, this flag is ignored. If unset, pachctl will use a default timeout; if set to 0s, the call will never time out. (default \"default\") Options inherited from parent commands \u00b6 --no-color Turn off colors. -v, --verbose Output verbose logs","title":"Pachctl version"},{"location":"reference/pachctl/pachctl_version/#pachctl-version","text":"Print Pachyderm version information.","title":"pachctl version"},{"location":"reference/pachctl/pachctl_version/#synopsis","text":"Print Pachyderm version information. pachctl version","title":"Synopsis"},{"location":"reference/pachctl/pachctl_version/#options","text":"--client-only If set, only print pachctl's version, but don't make any RPCs to pachd. Useful if pachd is unavailable --raw disable pretty printing, print raw json --timeout string If set, 'pachctl version' will timeout after the given duration (formatted as a golang time duration--a number followed by ns, us, ms, s, m, or h). If --client-only is set, this flag is ignored. If unset, pachctl will use a default timeout; if set to 0s, the call will never time out. (default \"default\")","title":"Options"},{"location":"reference/pachctl/pachctl_version/#options-inherited-from-parent-commands","text":"--no-color Turn off colors. -v, --verbose Output verbose logs","title":"Options inherited from parent commands"},{"location":"troubleshooting/deploy_troubleshooting/","text":"Troubleshooting Deployments \u00b6 Here are some common issues by symptom related to certain deploys. General Pachyderm cluster deployment Environment-specific AWS Can't connect to the Pachyderm cluster after a rolling update The one shot deploy script, aws.sh , never completes VPC limit exceeded GPU node never appears General Pachyderm cluster deployment \u00b6 Pod stuck in CrashLoopBackoff Pod stuck in CrashLoopBackoff - with error attaching volume Pod stuck in CrashLoopBackoff \u00b6 Symptoms \u00b6 The pachd pod keeps crashing/restarting: $ kubectl get all NAME READY STATUS RESTARTS AGE po/etcd-281005231-qlkzw 1/1 Running 0 7m po/pachd-1333950811-0sm1p 0/1 CrashLoopBackOff 6 7m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/etcd 100.70.40.162 <nodes> 2379:30938/TCP 7m svc/kubernetes 100.64.0.1 <none> 443/TCP 9m svc/pachd 100.70.227.151 <nodes> 650:30650/TCP,651:30651/TCP 7m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/etcd 1 1 1 1 7m deploy/pachd 1 1 1 0 7m NAME DESIRED CURRENT READY AGE rs/etcd-281005231 1 1 1 7m rs/pachd-1333950811 1 1 0 7m Recourse \u00b6 First describe the pod: $ kubectl describe po/pachd-1333950811-0sm1p If you see an error including Error attaching EBS volume or similar, see the recourse for that error here under the corresponding section below. If you don't see that error, but do see something like: 1m 3s 9 {kubelet ip-172-20-48-123.us-west-2.compute.internal} Warning FailedSync Error syncing pod, skipping: failed to \"StartContainer\" for \"pachd\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=pachd pod=pachd-1333950811-0sm1p_default(a92b6665-506a-11e7-8e07-02e3d74c49ac)\" it means Kubernetes tried running pachd , but pachd generated an internal error. To see the specifics of this internal error, check the logs for the pachd pod: $kubectl logs po/pachd-1333950811-0sm1p Note : If you're using a log aggregator service (e.g. the default in GKE), you won't see any logs when using kubectl logs ... in this way. You will need to look at your logs UI (e.g. in GKE's case the stackdriver console). These logs will most likely reveal the issue directly, or at the very least, a good indicator as to what's causing the problem. For example, you might see, BucketRegionError: incorrect region, the bucket is not in 'us-west-2' region . In that case, your object store bucket in a different region than your pachyderm cluster and the fix would be to recreate the bucket in the same region as your pachydermm cluster. If the error / recourse isn't obvious from the error message, post the error as well as the pachd logs in our Slack channel , or open a GitHub Issue and provide the necessary details prompted by the issue template. Please do be sure provide these logs either way as it is extremely helpful in resolving the issue. Pod stuck in CrashLoopBackoff - with error attaching volume \u00b6 Symptoms \u00b6 A pod (could be the pachd pod or a worker pod) fails to startup, and is stuck in CrashLoopBackoff . If you execute kubectl describe po/pachd-xxxx , you'll see an error message like the following at the bottom of the output: 30s 30s 1 {attachdetach } Warning FailedMount Failed to attach volume \"etcd-volume\" on node \"ip-172-20-44-17.us-west-2.compute.internal\" with: Error attaching EBS volume \"vol-0c1d403ac05096dfe\" to instance \"i-0a12e00c0f3fb047d\": VolumeInUse: vol-0c1d403ac05096dfe is already attached to an instance This would indicate that the peristent volume claim is failing to get attached to the node in your kubernetes cluster. Recourse \u00b6 Your best bet is to manually detach the volume and restart the pod. For example, to resolve this issue when Pachyderm is deployed to AWS, pull up your AWS web console and look up the node mentioned in the error message (ip-172-20-44-17.us-west-2.compute.internal in our case). Then on the bottom pane for the attached volume. Follow the link to the attached volume, and detach the volume. You may need to \"Force Detach\" it. Once it's detached (and marked as available). Restart the pod by killing it, e.g: $kubectl delete po/pachd-xxx It will take a moment for a new pod to get scheduled. AWS Deployment \u00b6 Can't connect to the Pachyderm cluster after a rolling update \u00b6 Symptom \u00b6 After running kops rolling-update , kubectl (and/or pachctl ) all requests hang and you can't connect to the cluster. Recourse \u00b6 First get your cluster name. You can easily locate that information by running kops get clusters . If you used the one shot deployment]( http://docs.pachyderm.io/en/latest/deployment/amazon_web_services.html#one-shot-script ), you can also get this info in the deploy logs you created by aws.sh . Then you'll need to grab the new public IP address of your master node. The master node will be named something like master-us-west-2a.masters.somerandomstring.kubernetes.com Update the etc hosts entry in /etc/hosts such that the api endpoint reflects the new IP, e.g: 54.178.87.68 api.somerandomstring.kubernetes.com One shot script never completes \u00b6 Symptom \u00b6 The aws.sh one shot deploy script hangs on the line: Retrieving ec2 instance list to get k8s master domain name (may take a minute) If it's been more than 10 minutes, there's likely an error. Recourse \u00b6 Check the AWS web console / autoscale group / activity history. You have probably hit an instance limit. To confirm, open the AWS web console for EC2 and check to see if you have any instances with names like: master-us-west-2a.masters.tfgpu.kubernetes.com nodes.tfgpu.kubernetes.com If you don't see instances similar to the ones above the next thing to do is to navigate to \"Auto Scaling Groups\" in the left hand menu. Then find the ASG with your cluster name: master-us-west-2a.masters.tfgpu.kubernetes.com Look at the \"Activity History\" in the lower pane. More than likely, you'll see a \"Failed\" error message describing why it failed to provision the VM. You're probably run into an instance limit for your account for this region. If you're spinning up a GPU node, make sure that your region supports the instance type you're trying to spin up. A successful provisioning message looks like: Successful Launching a new EC2 instance: i-03422f3d32658e90c 2017 June 13 10:19:29 UTC-7 2017 June 13 10:20:33 UTC-7 Description:DescriptionLaunching a new EC2 instance: i-03422f3d32658e90c Cause:CauseAt 2017-06-13T17:19:15Z a user request created an AutoScalingGroup changing the desired capacity from 0 to 1. At 2017-06-13T17:19:28Z an instance was started in response to a difference between desired and actual capacity, increasing the capacity from 0 to 1. While a failed one looks like: Failed Launching a new EC2 instance 2017 June 12 13:21:49 UTC-7 2017 June 12 13:21:49 UTC-7 Description:DescriptionLaunching a new EC2 instance. Status Reason: You have requested more instances (1) than your current instance limit of 0 allows for the specified instance type. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit. Launching EC2 instance failed. Cause:CauseAt 2017-06-12T20:21:47Z an instance was started in response to a difference between desired and actual capacity, increasing the capacity from 0 to 1. VPC Limit Exceeded \u00b6 Symptom \u00b6 When running aws.sh or otherwise deploying with kops , you will see: W0426 17:28:10.435315 26463 executor.go:109] error running task \"VPC/5120cf0c-pachydermcluster.kubernetes.com\" (3s remaining to succeed): error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached. Recourse \u00b6 You'll need to increase your VPC limit or delete some existing VPCs that are not in use. On the AWS web console navigate to the VPC service. Make sure you're in the same region where you're attempting to deploy. It's not uncommon (depending on how you tear down clusters) for the VPCs not to be deleted. You'll see a list of VPCs here with cluster names, e.g. aee6b566-pachydermcluster.kubernetes.com . For clusters that you know are no longer in use, you can delete the VPC here. GPU Node Never Appears \u00b6 Symptom \u00b6 After running kops edit ig gpunodes and kops update (as outlined here ) the GPU node never appears, which can be confirmed via the AWS web console. Recourse \u00b6 It's likely you have hit an instance limit for the GPU instance type you're using, or it's possible that AWS doesn't support that instance type in the current region. Follow these instructions to check for and update Instance Limits . If this region doesn't support your instance type, you'll see an error message like: Failed Launching a new EC2 instance 2017 June 12 13:21:49 UTC-7 2017 June 12 13:21:49 UTC-7 Description:DescriptionLaunching a new EC2 instance. Status Reason: You have requested more instances (1) than your current instance limit of 0 allows for the specified instance type. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit. Launching EC2 instance failed. Cause:CauseAt 2017-06-12T20:21:47Z an instance was started in response to a difference between desired and actual capacity, increasing the capacity from 0 to 1.","title":"Deployment Troubleshootin"},{"location":"troubleshooting/deploy_troubleshooting/#troubleshooting-deployments","text":"Here are some common issues by symptom related to certain deploys. General Pachyderm cluster deployment Environment-specific AWS Can't connect to the Pachyderm cluster after a rolling update The one shot deploy script, aws.sh , never completes VPC limit exceeded GPU node never appears","title":"Troubleshooting Deployments"},{"location":"troubleshooting/deploy_troubleshooting/#general-pachyderm-cluster-deployment","text":"Pod stuck in CrashLoopBackoff Pod stuck in CrashLoopBackoff - with error attaching volume","title":"General Pachyderm cluster deployment"},{"location":"troubleshooting/deploy_troubleshooting/#pod-stuck-in-crashloopbackoff","text":"","title":"Pod stuck in CrashLoopBackoff"},{"location":"troubleshooting/deploy_troubleshooting/#symptoms","text":"The pachd pod keeps crashing/restarting: $ kubectl get all NAME READY STATUS RESTARTS AGE po/etcd-281005231-qlkzw 1/1 Running 0 7m po/pachd-1333950811-0sm1p 0/1 CrashLoopBackOff 6 7m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/etcd 100.70.40.162 <nodes> 2379:30938/TCP 7m svc/kubernetes 100.64.0.1 <none> 443/TCP 9m svc/pachd 100.70.227.151 <nodes> 650:30650/TCP,651:30651/TCP 7m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/etcd 1 1 1 1 7m deploy/pachd 1 1 1 0 7m NAME DESIRED CURRENT READY AGE rs/etcd-281005231 1 1 1 7m rs/pachd-1333950811 1 1 0 7m","title":"Symptoms"},{"location":"troubleshooting/deploy_troubleshooting/#recourse","text":"First describe the pod: $ kubectl describe po/pachd-1333950811-0sm1p If you see an error including Error attaching EBS volume or similar, see the recourse for that error here under the corresponding section below. If you don't see that error, but do see something like: 1m 3s 9 {kubelet ip-172-20-48-123.us-west-2.compute.internal} Warning FailedSync Error syncing pod, skipping: failed to \"StartContainer\" for \"pachd\" with CrashLoopBackOff: \"Back-off 2m40s restarting failed container=pachd pod=pachd-1333950811-0sm1p_default(a92b6665-506a-11e7-8e07-02e3d74c49ac)\" it means Kubernetes tried running pachd , but pachd generated an internal error. To see the specifics of this internal error, check the logs for the pachd pod: $kubectl logs po/pachd-1333950811-0sm1p Note : If you're using a log aggregator service (e.g. the default in GKE), you won't see any logs when using kubectl logs ... in this way. You will need to look at your logs UI (e.g. in GKE's case the stackdriver console). These logs will most likely reveal the issue directly, or at the very least, a good indicator as to what's causing the problem. For example, you might see, BucketRegionError: incorrect region, the bucket is not in 'us-west-2' region . In that case, your object store bucket in a different region than your pachyderm cluster and the fix would be to recreate the bucket in the same region as your pachydermm cluster. If the error / recourse isn't obvious from the error message, post the error as well as the pachd logs in our Slack channel , or open a GitHub Issue and provide the necessary details prompted by the issue template. Please do be sure provide these logs either way as it is extremely helpful in resolving the issue.","title":"Recourse"},{"location":"troubleshooting/deploy_troubleshooting/#pod-stuck-in-crashloopbackoff-with-error-attaching-volume","text":"","title":"Pod stuck in CrashLoopBackoff - with error attaching volume"},{"location":"troubleshooting/deploy_troubleshooting/#symptoms_1","text":"A pod (could be the pachd pod or a worker pod) fails to startup, and is stuck in CrashLoopBackoff . If you execute kubectl describe po/pachd-xxxx , you'll see an error message like the following at the bottom of the output: 30s 30s 1 {attachdetach } Warning FailedMount Failed to attach volume \"etcd-volume\" on node \"ip-172-20-44-17.us-west-2.compute.internal\" with: Error attaching EBS volume \"vol-0c1d403ac05096dfe\" to instance \"i-0a12e00c0f3fb047d\": VolumeInUse: vol-0c1d403ac05096dfe is already attached to an instance This would indicate that the peristent volume claim is failing to get attached to the node in your kubernetes cluster.","title":"Symptoms"},{"location":"troubleshooting/deploy_troubleshooting/#recourse_1","text":"Your best bet is to manually detach the volume and restart the pod. For example, to resolve this issue when Pachyderm is deployed to AWS, pull up your AWS web console and look up the node mentioned in the error message (ip-172-20-44-17.us-west-2.compute.internal in our case). Then on the bottom pane for the attached volume. Follow the link to the attached volume, and detach the volume. You may need to \"Force Detach\" it. Once it's detached (and marked as available). Restart the pod by killing it, e.g: $kubectl delete po/pachd-xxx It will take a moment for a new pod to get scheduled.","title":"Recourse"},{"location":"troubleshooting/deploy_troubleshooting/#aws-deployment","text":"","title":"AWS Deployment"},{"location":"troubleshooting/deploy_troubleshooting/#cant-connect-to-the-pachyderm-cluster-after-a-rolling-update","text":"","title":"Can't connect to the Pachyderm cluster after a rolling update"},{"location":"troubleshooting/deploy_troubleshooting/#symptom","text":"After running kops rolling-update , kubectl (and/or pachctl ) all requests hang and you can't connect to the cluster.","title":"Symptom"},{"location":"troubleshooting/deploy_troubleshooting/#recourse_2","text":"First get your cluster name. You can easily locate that information by running kops get clusters . If you used the one shot deployment]( http://docs.pachyderm.io/en/latest/deployment/amazon_web_services.html#one-shot-script ), you can also get this info in the deploy logs you created by aws.sh . Then you'll need to grab the new public IP address of your master node. The master node will be named something like master-us-west-2a.masters.somerandomstring.kubernetes.com Update the etc hosts entry in /etc/hosts such that the api endpoint reflects the new IP, e.g: 54.178.87.68 api.somerandomstring.kubernetes.com","title":"Recourse"},{"location":"troubleshooting/deploy_troubleshooting/#one-shot-script-never-completes","text":"","title":"One shot script never completes"},{"location":"troubleshooting/deploy_troubleshooting/#symptom_1","text":"The aws.sh one shot deploy script hangs on the line: Retrieving ec2 instance list to get k8s master domain name (may take a minute) If it's been more than 10 minutes, there's likely an error.","title":"Symptom"},{"location":"troubleshooting/deploy_troubleshooting/#recourse_3","text":"Check the AWS web console / autoscale group / activity history. You have probably hit an instance limit. To confirm, open the AWS web console for EC2 and check to see if you have any instances with names like: master-us-west-2a.masters.tfgpu.kubernetes.com nodes.tfgpu.kubernetes.com If you don't see instances similar to the ones above the next thing to do is to navigate to \"Auto Scaling Groups\" in the left hand menu. Then find the ASG with your cluster name: master-us-west-2a.masters.tfgpu.kubernetes.com Look at the \"Activity History\" in the lower pane. More than likely, you'll see a \"Failed\" error message describing why it failed to provision the VM. You're probably run into an instance limit for your account for this region. If you're spinning up a GPU node, make sure that your region supports the instance type you're trying to spin up. A successful provisioning message looks like: Successful Launching a new EC2 instance: i-03422f3d32658e90c 2017 June 13 10:19:29 UTC-7 2017 June 13 10:20:33 UTC-7 Description:DescriptionLaunching a new EC2 instance: i-03422f3d32658e90c Cause:CauseAt 2017-06-13T17:19:15Z a user request created an AutoScalingGroup changing the desired capacity from 0 to 1. At 2017-06-13T17:19:28Z an instance was started in response to a difference between desired and actual capacity, increasing the capacity from 0 to 1. While a failed one looks like: Failed Launching a new EC2 instance 2017 June 12 13:21:49 UTC-7 2017 June 12 13:21:49 UTC-7 Description:DescriptionLaunching a new EC2 instance. Status Reason: You have requested more instances (1) than your current instance limit of 0 allows for the specified instance type. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit. Launching EC2 instance failed. Cause:CauseAt 2017-06-12T20:21:47Z an instance was started in response to a difference between desired and actual capacity, increasing the capacity from 0 to 1.","title":"Recourse"},{"location":"troubleshooting/deploy_troubleshooting/#vpc-limit-exceeded","text":"","title":"VPC Limit Exceeded"},{"location":"troubleshooting/deploy_troubleshooting/#symptom_2","text":"When running aws.sh or otherwise deploying with kops , you will see: W0426 17:28:10.435315 26463 executor.go:109] error running task \"VPC/5120cf0c-pachydermcluster.kubernetes.com\" (3s remaining to succeed): error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached.","title":"Symptom"},{"location":"troubleshooting/deploy_troubleshooting/#recourse_4","text":"You'll need to increase your VPC limit or delete some existing VPCs that are not in use. On the AWS web console navigate to the VPC service. Make sure you're in the same region where you're attempting to deploy. It's not uncommon (depending on how you tear down clusters) for the VPCs not to be deleted. You'll see a list of VPCs here with cluster names, e.g. aee6b566-pachydermcluster.kubernetes.com . For clusters that you know are no longer in use, you can delete the VPC here.","title":"Recourse"},{"location":"troubleshooting/deploy_troubleshooting/#gpu-node-never-appears","text":"","title":"GPU Node Never Appears"},{"location":"troubleshooting/deploy_troubleshooting/#symptom_3","text":"After running kops edit ig gpunodes and kops update (as outlined here ) the GPU node never appears, which can be confirmed via the AWS web console.","title":"Symptom"},{"location":"troubleshooting/deploy_troubleshooting/#recourse_5","text":"It's likely you have hit an instance limit for the GPU instance type you're using, or it's possible that AWS doesn't support that instance type in the current region. Follow these instructions to check for and update Instance Limits . If this region doesn't support your instance type, you'll see an error message like: Failed Launching a new EC2 instance 2017 June 12 13:21:49 UTC-7 2017 June 12 13:21:49 UTC-7 Description:DescriptionLaunching a new EC2 instance. Status Reason: You have requested more instances (1) than your current instance limit of 0 allows for the specified instance type. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit. Launching EC2 instance failed. Cause:CauseAt 2017-06-12T20:21:47Z an instance was started in response to a difference between desired and actual capacity, increasing the capacity from 0 to 1.","title":"Recourse"},{"location":"troubleshooting/general_troubleshooting/","text":"General Troubleshooting \u00b6 Here are some common issues by symptom along with steps to resolve them. Connecting to a Pachyderm cluster Cannot connect via pachctl - context deadline exceeded Certificate error when using kubectl Uploads/downloads are slow Connecting to a Pachyderm Cluster \u00b6 Cannot connect via pachctl - context deadline exceeded \u00b6 Symptom \u00b6 You may be using the pachd address config value or environment variable to specify how pachctl talks to your Pachyderm cluster, or you may be forwarding the pachyderm port. In any event, you might see something similar to: $ pachctl version COMPONENT VERSION pachctl 1.9.5 context deadline exceeded Also, you might get this message if pachd is not running. Recourse \u00b6 It's possible that the connection is just taking a while. Occasionally this can happen if your cluster is far away (deployed in a region across the country). Check your internet connection. It's also possible that you haven't poked a hole in the firewall to access the node on this port. Usually to do that you adjust a security rule (in AWS parlance a security group). For example, on AWS, if you find your node in the web console and click on it, you should see a link to the associated security group. Inspect that group. There should be a way to \"add a rule\" to the group. You'll want to enable TCP access (ingress) on port 30650. You'll usually be asked which incoming IPs should be whitelisted. You can choose to use your own, or enable it for everyone (0.0.0.0/0). Certificate Error When Using Kubectl \u00b6 Symptom \u00b6 This can happen on any request using kubectl (e.g. kubectl get all ). In particular you'll see: $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.4\", GitCommit:\"d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae\", GitTreeState:\"clean\", BuildDate:\"2017-05-19T20:41:24Z\", GoVersion:\"go1.8.1\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Unable to connect to the server: x509: certificate signed by unknown authority Recourse \u00b6 Check if you're on any sort of VPN or other egress proxy that would break SSL. Also, there is a possibility that your credentials have expired. In the case where you're using GKE and gcloud, renew your credentials via: $ kubectl get all Unable to connect to the server: x509: certificate signed by unknown authority $ gcloud container clusters get-credentials my-cluster-name-dev Fetching cluster endpoint and auth data. kubeconfig entry generated for my-cluster-name-dev. $ kubectl config current-context gke_my-org_us-east1-b_my-cluster-name-dev Uploads/Downloads are Slow \u00b6 Symptom \u00b6 Any pachctl put file or pachctl get file commands are slow. Recourse \u00b6 If you do not explicitly set the pachd address config value, pachctl will default to using port forwarding, which throttles traffic to ~1MB/s. If you need to do large downloads/uploads you should consider using pachd address config value. You'll also want to make sure you've allowed ingress access through any firewalls to your k8s cluster.","title":"General Troubleshooting"},{"location":"troubleshooting/general_troubleshooting/#general-troubleshooting","text":"Here are some common issues by symptom along with steps to resolve them. Connecting to a Pachyderm cluster Cannot connect via pachctl - context deadline exceeded Certificate error when using kubectl Uploads/downloads are slow","title":"General Troubleshooting"},{"location":"troubleshooting/general_troubleshooting/#connecting-to-a-pachyderm-cluster","text":"","title":"Connecting to a Pachyderm Cluster"},{"location":"troubleshooting/general_troubleshooting/#cannot-connect-via-pachctl-context-deadline-exceeded","text":"","title":"Cannot connect via pachctl - context deadline exceeded"},{"location":"troubleshooting/general_troubleshooting/#symptom","text":"You may be using the pachd address config value or environment variable to specify how pachctl talks to your Pachyderm cluster, or you may be forwarding the pachyderm port. In any event, you might see something similar to: $ pachctl version COMPONENT VERSION pachctl 1.9.5 context deadline exceeded Also, you might get this message if pachd is not running.","title":"Symptom"},{"location":"troubleshooting/general_troubleshooting/#recourse","text":"It's possible that the connection is just taking a while. Occasionally this can happen if your cluster is far away (deployed in a region across the country). Check your internet connection. It's also possible that you haven't poked a hole in the firewall to access the node on this port. Usually to do that you adjust a security rule (in AWS parlance a security group). For example, on AWS, if you find your node in the web console and click on it, you should see a link to the associated security group. Inspect that group. There should be a way to \"add a rule\" to the group. You'll want to enable TCP access (ingress) on port 30650. You'll usually be asked which incoming IPs should be whitelisted. You can choose to use your own, or enable it for everyone (0.0.0.0/0).","title":"Recourse"},{"location":"troubleshooting/general_troubleshooting/#certificate-error-when-using-kubectl","text":"","title":"Certificate Error When Using Kubectl"},{"location":"troubleshooting/general_troubleshooting/#symptom_1","text":"This can happen on any request using kubectl (e.g. kubectl get all ). In particular you'll see: $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.4\", GitCommit:\"d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae\", GitTreeState:\"clean\", BuildDate:\"2017-05-19T20:41:24Z\", GoVersion:\"go1.8.1\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Unable to connect to the server: x509: certificate signed by unknown authority","title":"Symptom"},{"location":"troubleshooting/general_troubleshooting/#recourse_1","text":"Check if you're on any sort of VPN or other egress proxy that would break SSL. Also, there is a possibility that your credentials have expired. In the case where you're using GKE and gcloud, renew your credentials via: $ kubectl get all Unable to connect to the server: x509: certificate signed by unknown authority $ gcloud container clusters get-credentials my-cluster-name-dev Fetching cluster endpoint and auth data. kubeconfig entry generated for my-cluster-name-dev. $ kubectl config current-context gke_my-org_us-east1-b_my-cluster-name-dev","title":"Recourse"},{"location":"troubleshooting/general_troubleshooting/#uploadsdownloads-are-slow","text":"","title":"Uploads/Downloads are Slow"},{"location":"troubleshooting/general_troubleshooting/#symptom_2","text":"Any pachctl put file or pachctl get file commands are slow.","title":"Symptom"},{"location":"troubleshooting/general_troubleshooting/#recourse_2","text":"If you do not explicitly set the pachd address config value, pachctl will default to using port forwarding, which throttles traffic to ~1MB/s. If you need to do large downloads/uploads you should consider using pachd address config value. You'll also want to make sure you've allowed ingress access through any firewalls to your k8s cluster.","title":"Recourse"},{"location":"troubleshooting/pipeline_troubleshooting/","text":"Troubleshooting Pipelines \u00b6 Introduction \u00b6 Job failures can occur for a variety of reasons, but they generally categorize into 3 failure types: User-code-related : An error in the user code running inside the container or the json pipeline config. Data-related : A problem with the input data such as incorrect file type or file name. System- or infrastructure-related : An error in Pachyderm or Kubernetes such as missing credentials, transient network errors, or resource constraints (for example, out-of-memory--OOM--killed). In this document, we'll show you the tools for determining what kind of failure it is. For each of the failure modes, we\u2019ll describe Pachyderm\u2019s and Kubernetes\u2019s specific retry and error-reporting behaviors as well as typical user triaging methodologies. Failed jobs in a pipeline will propagate information to downstream pipelines with empty commits to preserve provenance and make tracing the failed job easier. A failed job is no longer running. In this document, we'll describe what you'll see, how Pachyderm will respond, and techniques for triaging each of those three categories of failure. At the bottom of the document, we'll provide specific troubleshooting steps for specific scenarios . Pipeline exists but never runs All your pods / jobs get evicted Determining the kind of failure \u00b6 First off, you can see the status of Pachyderm's jobs with pachctl list job , which will show you the status of all jobs. For a failed job, use pachctl inspect job <job-id> to find out more about the failure. The different categories of failures are addressed below. User Code Failures \u00b6 When there\u2019s an error in user code, the typical error message you\u2019ll see is failed to process datum <UUID> with error: <user code error> This means pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code. If any datum in a pipeline fails, the entire job will be marked as failed, but datums that did not fail will not need to be reprocessed on future jobs. You can use pachctl inspect datum <job-id> <datum-id> or pachctl logs with the --pipeline , --job or --datum flags to get more details. There are some cases where users may want mark a datum as successful even for a non-zero error code by setting the transform.accept_return_code field in the pipeline config . Retries \u00b6 Pachyderm will automatically retry user code three (3) times before marking the datum as failed. This mitigates datums failing for transient connection reasons. Triage \u00b6 pachctl logs --job=<job_ID> or pachctl logs --pipeline=<pipeline_name> will print out any logs from your user code to help you triage the issue. Kubernetes will rotate logs occasionally so if nothing is being returned, you\u2019ll need to make sure that you have a persistent log collection tool running in your cluster. If you set enable_stats:true in your pachyderm pipeline, pachyderm will persist the user logs for you. In cases where user code is failing, changes first need to be made to the code and followed by updating the pachyderm pipeline. This involves building a new docker container with the corrected code, modifying the pachyderm pipeline config to use the new image, and then calling pachctl update pipeline -f updated_pipeline_config.json . Depending on the issue/error, user may or may not want to also include the --reprocess flag with update pipeline . Data Failures \u00b6 When there\u2019s an error in the data, this will typically manifest in a user code error such as failed to process datum <UUID> with error: <user code error> This means pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code, usually due to being unable to find a file or a path, a misformatted file, or incorrect fields/data within a file. If any datum in a pipeline fails, the entire job will be marked as failed. Datums that did not fail will not need to be reprocessed on future jobs. Retries \u00b6 Just like with user code failures, Pachyderm will automatically retry running a datum 3 times before marking the datum as failed. This mitigates datums failing for transient connection reasons. Triage \u00b6 Data failures can be triaged in a few different way depending on the nature of the failure and design of the pipeline. In some cases, where malformed datums are expected to happen occasionally, they can be \u201cswallowed\u201d (e.g. marked as successful using transform.accept_return_codes or written out to a \u201cfailed_datums\u201d directory and handled within user code). This would simply require the necessary updates to the user code and pipeline config as described above. For cases where your code detects bad input data, a \"dead letter queue\" design pattern may be needed. Many pachyderm developers use a special directory in each output repo for \"bad data\" and pipelines with globs for detecting bad data direct that data for automated and manual intervention. Pachyderm's engineering team is working on changes to the Pachyderm Pipeline System in a future release that may make implementation of design patterns like this easier. Take a look at the pipeline design changes for pachyderm 1.9 If a few files as part of the input commit are causing the failure, they can simply be removed from the HEAD commit with start commit , delete file , finish commit . The files can also be corrected in this manner as well. This method is similar to a revert in Git -- the \u201cbad\u201d data will still live in the older commits in Pachyderm, but will not be part of the HEAD commit and therefore not processed by the pipeline. If the entire commit is bad and you just want to remove it forever as if it never happened, delete commit will both remove that commit and all downstream commits and jobs that were created as downstream effects of that input data. System-level Failures \u00b6 System-level failures are the most varied and often hardest to debug. We\u2019ll outline a few common patterns and triage steps. Generally, you\u2019ll need to look at deeper logs to find these errors using pachctl logs --pipeline=<pipeline_name> --raw and/or --master and kubectl logs pod <pod_name> . Here are some of the most common system-level failures: Malformed or missing credentials such that a pipeline cannot connect to object storage, registry, or other external service. In the best case, you\u2019ll see permission denied errors, but in some cases you\u2019ll only see \u201cdoes not exist\u201d errors (this is common reading from object stores) Out-of-memory (OOM) killed or other resource constraint issues such as not being able to schedule pods on available cluster resources. Network issues trying to connect Pachd, etcd, or other internal or external resources Failure to find or pull a docker image from the registry Retries \u00b6 For system-level failures, Pachyderm or Kubernetes will generally continually retry the operation with exponential backoff. If a job is stuck in a given state (e.g. starting, merging) or a pod is in CrashLoopBackoff , those are common signs of a system-level failure mode. Triage \u00b6 Triaging system failures varies as widely as the issues do themselves. Here are options for the common issues mentioned previously. - Credentials: check your secrets in k8s, make sure they\u2019re added correctly to the pipeline config, and double check your roles/perms within the cluster - OOM: Increase the memory limit/request or node size for your pipeline. If you are very resource constrained, making your datums smaller to require less resources may be necessary. - Network: Check to make sure etcd and pachd are up and running, that k8s DNS is correctly configured for pods to resolve each other and outside resources, firewalls and other networking configurations allow k8s components to reach each other, and ingress controllers are configured correctly - Check your container image name in the pipeline config and image_pull_secret. Specific scenarios \u00b6 All your pods / jobs get evicted \u00b6 Symptom \u00b6 Running: $ kubectl get all shows a bunch of pods that are marked Evicted . If you kubectl describe ... one of those evicted pods, you see an error saying that it was evicted due to disk pressure. Recourse \u00b6 Your nodes are not configured with a big enough root volume size. You need to make sure that each node's root volume is big enough to store the biggest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum. Let's say you have a repo with 100 folders. You have a single pipeline with this repo as an input, and the glob pattern is /* . That means each folder will be processed as a single datum. If the biggest folder is 50GB and your pipeline's output is about 3 times as big, then your root volume size needs to be bigger than: 50 GB (to accommodate the input) + 50 GB x 3 (to accommodate the output) = 200GB In this case we would recommend 250GB to be safe. If your root volume size is less than 50GB (many defaults are 20GB), this pipeline will fail when downloading the input. The pod may get evicted and rescheduled to a different node, where the same thing will happen. Pipeline exists but never runs \u00b6 Symptom \u00b6 You can see the pipeline via: $ pachctl list pipeline But if you look at the job via: $ pachctl list job It's marked as running with 0/0 datums having been processed. If you inspect the job via: $ pachctl inspect job You don't see any worker set. E.g: Worker Status: WORKER JOB DATUM STARTED ... If you do kubectl get pod you see the worker pod for your pipeline, e.g: po/pipeline-foo-5-v1-273zc But it's state is Pending or CrashLoopBackoff . Recourse \u00b6 First make sure that there is no parent job still running. Do pachctl list job | grep yourPipelineName to see if there are pending jobs on this pipeline that were kicked off prior to your job. A parent job is the job that corresponds to the parent output commit of this pipeline. A job will block until all parent jobs complete. If there are no parent jobs that are still running, then continue debugging: Describe the pod via: $kubectl describe po/pipeline-foo-5-v1-273zc If the state is CrashLoopBackoff , you're looking for a descriptive error message. One such cause for this behavior might be if you specified an image for your pipeline that does not exist. If the state is Pending it's likely the cluster doesn't have enough resources. In this case, you'll see a could not schedule type of error message which should describe which resource you're low on. This is more likely to happen if you've set resource requests (cpu/mem/gpu) for your pipelines. In this case, you'll just need to scale up your resources. If you deployed using kops , you'll want to do edit the instance group, e.g. kops edit ig nodes ... and up the number of nodes. If you didn't use kops to deploy, you can use your cloud provider's auto scaling groups to increase the size of your instance group. Either way, it can take up to 10 minutes for the changes to go into effect. For more information, see Autoscale Your Cluster .","title":"Pipeline Troubleshooting"},{"location":"troubleshooting/pipeline_troubleshooting/#troubleshooting-pipelines","text":"","title":"Troubleshooting Pipelines"},{"location":"troubleshooting/pipeline_troubleshooting/#introduction","text":"Job failures can occur for a variety of reasons, but they generally categorize into 3 failure types: User-code-related : An error in the user code running inside the container or the json pipeline config. Data-related : A problem with the input data such as incorrect file type or file name. System- or infrastructure-related : An error in Pachyderm or Kubernetes such as missing credentials, transient network errors, or resource constraints (for example, out-of-memory--OOM--killed). In this document, we'll show you the tools for determining what kind of failure it is. For each of the failure modes, we\u2019ll describe Pachyderm\u2019s and Kubernetes\u2019s specific retry and error-reporting behaviors as well as typical user triaging methodologies. Failed jobs in a pipeline will propagate information to downstream pipelines with empty commits to preserve provenance and make tracing the failed job easier. A failed job is no longer running. In this document, we'll describe what you'll see, how Pachyderm will respond, and techniques for triaging each of those three categories of failure. At the bottom of the document, we'll provide specific troubleshooting steps for specific scenarios . Pipeline exists but never runs All your pods / jobs get evicted","title":"Introduction"},{"location":"troubleshooting/pipeline_troubleshooting/#determining-the-kind-of-failure","text":"First off, you can see the status of Pachyderm's jobs with pachctl list job , which will show you the status of all jobs. For a failed job, use pachctl inspect job <job-id> to find out more about the failure. The different categories of failures are addressed below.","title":"Determining the kind of failure"},{"location":"troubleshooting/pipeline_troubleshooting/#user-code-failures","text":"When there\u2019s an error in user code, the typical error message you\u2019ll see is failed to process datum <UUID> with error: <user code error> This means pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code. If any datum in a pipeline fails, the entire job will be marked as failed, but datums that did not fail will not need to be reprocessed on future jobs. You can use pachctl inspect datum <job-id> <datum-id> or pachctl logs with the --pipeline , --job or --datum flags to get more details. There are some cases where users may want mark a datum as successful even for a non-zero error code by setting the transform.accept_return_code field in the pipeline config .","title":"User Code Failures"},{"location":"troubleshooting/pipeline_troubleshooting/#retries","text":"Pachyderm will automatically retry user code three (3) times before marking the datum as failed. This mitigates datums failing for transient connection reasons.","title":"Retries"},{"location":"troubleshooting/pipeline_troubleshooting/#triage","text":"pachctl logs --job=<job_ID> or pachctl logs --pipeline=<pipeline_name> will print out any logs from your user code to help you triage the issue. Kubernetes will rotate logs occasionally so if nothing is being returned, you\u2019ll need to make sure that you have a persistent log collection tool running in your cluster. If you set enable_stats:true in your pachyderm pipeline, pachyderm will persist the user logs for you. In cases where user code is failing, changes first need to be made to the code and followed by updating the pachyderm pipeline. This involves building a new docker container with the corrected code, modifying the pachyderm pipeline config to use the new image, and then calling pachctl update pipeline -f updated_pipeline_config.json . Depending on the issue/error, user may or may not want to also include the --reprocess flag with update pipeline .","title":"Triage"},{"location":"troubleshooting/pipeline_troubleshooting/#data-failures","text":"When there\u2019s an error in the data, this will typically manifest in a user code error such as failed to process datum <UUID> with error: <user code error> This means pachyderm successfully got to the point where it was running user code, but that code exited with a non-zero error code, usually due to being unable to find a file or a path, a misformatted file, or incorrect fields/data within a file. If any datum in a pipeline fails, the entire job will be marked as failed. Datums that did not fail will not need to be reprocessed on future jobs.","title":"Data Failures"},{"location":"troubleshooting/pipeline_troubleshooting/#retries_1","text":"Just like with user code failures, Pachyderm will automatically retry running a datum 3 times before marking the datum as failed. This mitigates datums failing for transient connection reasons.","title":"Retries"},{"location":"troubleshooting/pipeline_troubleshooting/#triage_1","text":"Data failures can be triaged in a few different way depending on the nature of the failure and design of the pipeline. In some cases, where malformed datums are expected to happen occasionally, they can be \u201cswallowed\u201d (e.g. marked as successful using transform.accept_return_codes or written out to a \u201cfailed_datums\u201d directory and handled within user code). This would simply require the necessary updates to the user code and pipeline config as described above. For cases where your code detects bad input data, a \"dead letter queue\" design pattern may be needed. Many pachyderm developers use a special directory in each output repo for \"bad data\" and pipelines with globs for detecting bad data direct that data for automated and manual intervention. Pachyderm's engineering team is working on changes to the Pachyderm Pipeline System in a future release that may make implementation of design patterns like this easier. Take a look at the pipeline design changes for pachyderm 1.9 If a few files as part of the input commit are causing the failure, they can simply be removed from the HEAD commit with start commit , delete file , finish commit . The files can also be corrected in this manner as well. This method is similar to a revert in Git -- the \u201cbad\u201d data will still live in the older commits in Pachyderm, but will not be part of the HEAD commit and therefore not processed by the pipeline. If the entire commit is bad and you just want to remove it forever as if it never happened, delete commit will both remove that commit and all downstream commits and jobs that were created as downstream effects of that input data.","title":"Triage"},{"location":"troubleshooting/pipeline_troubleshooting/#system-level-failures","text":"System-level failures are the most varied and often hardest to debug. We\u2019ll outline a few common patterns and triage steps. Generally, you\u2019ll need to look at deeper logs to find these errors using pachctl logs --pipeline=<pipeline_name> --raw and/or --master and kubectl logs pod <pod_name> . Here are some of the most common system-level failures: Malformed or missing credentials such that a pipeline cannot connect to object storage, registry, or other external service. In the best case, you\u2019ll see permission denied errors, but in some cases you\u2019ll only see \u201cdoes not exist\u201d errors (this is common reading from object stores) Out-of-memory (OOM) killed or other resource constraint issues such as not being able to schedule pods on available cluster resources. Network issues trying to connect Pachd, etcd, or other internal or external resources Failure to find or pull a docker image from the registry","title":"System-level Failures"},{"location":"troubleshooting/pipeline_troubleshooting/#retries_2","text":"For system-level failures, Pachyderm or Kubernetes will generally continually retry the operation with exponential backoff. If a job is stuck in a given state (e.g. starting, merging) or a pod is in CrashLoopBackoff , those are common signs of a system-level failure mode.","title":"Retries"},{"location":"troubleshooting/pipeline_troubleshooting/#triage_2","text":"Triaging system failures varies as widely as the issues do themselves. Here are options for the common issues mentioned previously. - Credentials: check your secrets in k8s, make sure they\u2019re added correctly to the pipeline config, and double check your roles/perms within the cluster - OOM: Increase the memory limit/request or node size for your pipeline. If you are very resource constrained, making your datums smaller to require less resources may be necessary. - Network: Check to make sure etcd and pachd are up and running, that k8s DNS is correctly configured for pods to resolve each other and outside resources, firewalls and other networking configurations allow k8s components to reach each other, and ingress controllers are configured correctly - Check your container image name in the pipeline config and image_pull_secret.","title":"Triage"},{"location":"troubleshooting/pipeline_troubleshooting/#specific-scenarios","text":"","title":"Specific scenarios"},{"location":"troubleshooting/pipeline_troubleshooting/#all-your-pods-jobs-get-evicted","text":"","title":"All your pods / jobs get evicted"},{"location":"troubleshooting/pipeline_troubleshooting/#symptom","text":"Running: $ kubectl get all shows a bunch of pods that are marked Evicted . If you kubectl describe ... one of those evicted pods, you see an error saying that it was evicted due to disk pressure.","title":"Symptom"},{"location":"troubleshooting/pipeline_troubleshooting/#recourse","text":"Your nodes are not configured with a big enough root volume size. You need to make sure that each node's root volume is big enough to store the biggest datum you expect to process anywhere on your DAG plus the size of the output files that will be written for that datum. Let's say you have a repo with 100 folders. You have a single pipeline with this repo as an input, and the glob pattern is /* . That means each folder will be processed as a single datum. If the biggest folder is 50GB and your pipeline's output is about 3 times as big, then your root volume size needs to be bigger than: 50 GB (to accommodate the input) + 50 GB x 3 (to accommodate the output) = 200GB In this case we would recommend 250GB to be safe. If your root volume size is less than 50GB (many defaults are 20GB), this pipeline will fail when downloading the input. The pod may get evicted and rescheduled to a different node, where the same thing will happen.","title":"Recourse"},{"location":"troubleshooting/pipeline_troubleshooting/#pipeline-exists-but-never-runs","text":"","title":"Pipeline exists but never runs"},{"location":"troubleshooting/pipeline_troubleshooting/#symptom_1","text":"You can see the pipeline via: $ pachctl list pipeline But if you look at the job via: $ pachctl list job It's marked as running with 0/0 datums having been processed. If you inspect the job via: $ pachctl inspect job You don't see any worker set. E.g: Worker Status: WORKER JOB DATUM STARTED ... If you do kubectl get pod you see the worker pod for your pipeline, e.g: po/pipeline-foo-5-v1-273zc But it's state is Pending or CrashLoopBackoff .","title":"Symptom"},{"location":"troubleshooting/pipeline_troubleshooting/#recourse_1","text":"First make sure that there is no parent job still running. Do pachctl list job | grep yourPipelineName to see if there are pending jobs on this pipeline that were kicked off prior to your job. A parent job is the job that corresponds to the parent output commit of this pipeline. A job will block until all parent jobs complete. If there are no parent jobs that are still running, then continue debugging: Describe the pod via: $kubectl describe po/pipeline-foo-5-v1-273zc If the state is CrashLoopBackoff , you're looking for a descriptive error message. One such cause for this behavior might be if you specified an image for your pipeline that does not exist. If the state is Pending it's likely the cluster doesn't have enough resources. In this case, you'll see a could not schedule type of error message which should describe which resource you're low on. This is more likely to happen if you've set resource requests (cpu/mem/gpu) for your pipelines. In this case, you'll just need to scale up your resources. If you deployed using kops , you'll want to do edit the instance group, e.g. kops edit ig nodes ... and up the number of nodes. If you didn't use kops to deploy, you can use your cloud provider's auto scaling groups to increase the size of your instance group. Either way, it can take up to 10 minutes for the changes to go into effect. For more information, see Autoscale Your Cluster .","title":"Recourse"}]}